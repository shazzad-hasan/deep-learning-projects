{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/seq_to_seq/char_level_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CITpOagOBv3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "95e63350-537e-466b-d9f5-d8f53c51b8bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14150ac5-9d26-4db6-b324-6fa0f576bbd1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-14150ac5-9d26-4db6-b324-6fa0f576bbd1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"shazzadraihan\",\"key\":\"da63bbe0f8dcb3bd7fb35034046ca758\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# upload kaggle API key from your local machine\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a kaggle dir, copy the API key to it\n",
        "# and make sure the file in only readable by yourself (chmod 600)\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IajRinOUFcEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f98e64b-08d5-49da-d814-27f9c3e9011c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use API command to download the dataset\n",
        "!kaggle datasets download -d wanderdust/anna-karenina-book"
      ],
      "metadata": {
        "id": "BGlxw58uGJf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d5f792-6dc5-4d0e-ee90-2df032fe86a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna-karenina-book.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the dataset\n",
        "!unzip -qq anna-karenina-book.zip"
      ],
      "metadata": {
        "id": "QfxEg189G_4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10644d5c-6f28-43ec-900a-ee359bef19b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace anna.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open text file and read in some data as text\n",
        "with open(\"/content/anna.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "metadata": {
        "id": "ruMs8t0lGOwW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bef2ea6e-75c1-4363-ddf2-6ef54ba8079d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M7s-hXEgM4tN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print(\"CUDA is not available\")\n",
        "else:\n",
        "  print(\"CUDA is available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "hmDInJ5mcHI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970b15b3-7ddb-47b1-f217-4982f19f6908"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the dataset"
      ],
      "metadata": {
        "id": "1XeTje0xHgu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "chars = tuple(set(text))\n",
        "# map each int to char\n",
        "int_to_char = dict(enumerate(chars))\n",
        "# map each char to int\n",
        "char_to_int = {ch:idx for idx, ch in int_to_char.items()}\n",
        "\n",
        "# encode \n",
        "encoded = np.array([char_to_int[ch] for ch in text])\n",
        "encoded[:100]"
      ],
      "metadata": {
        "id": "AuOH3bqVHRpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd027a96-939b-4173-e30a-c77451deb792"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 67,  6, 78, 37, 11, 19, 43, 27, 20, 20, 20, 47,  6, 78, 78, 39,\n",
              "       43, 70,  6, 10, 77,  1, 77, 11, 12, 43,  6, 19, 11, 43,  6,  1,  1,\n",
              "       43,  6,  1, 77, 80, 11, 21, 43, 11, 18, 11, 19, 39, 43,  8, 29, 67,\n",
              "        6, 78, 78, 39, 43, 70,  6, 10, 77,  1, 39, 43, 77, 12, 43,  8, 29,\n",
              "       67,  6, 78, 78, 39, 43, 77, 29, 43, 77, 37, 12, 43, 30, 25, 29, 20,\n",
              "       25,  6, 39, 50, 20, 20, 16, 18, 11, 19, 39, 37, 67, 77, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "valid_size = 0.1\n",
        "\n",
        "valid_idx = int(len(encoded)*(1-valid_size))\n",
        "train_data, valid_data = encoded[:valid_idx], encoded[valid_idx:]"
      ],
      "metadata": {
        "id": "hZxzVyi6hpNA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # initialize the the encoded array with zeros\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # fill with ones where appropriate\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # reshape to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "Z_18UrpVQAjm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    total_batch_size = batch_size * seq_length\n",
        "    # total number of batches\n",
        "    n_batches = len(arr)//total_batch_size\n",
        "    \n",
        "    # keep enough characters to make full batches\n",
        "    arr = arr[:n_batches * total_batch_size]\n",
        "    # reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "  "
      ],
      "metadata": {
        "id": "XxElsQitRNph"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "vCVb7_7fcL7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden, n_layers, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.n_layers = n_layers \n",
        "    self.n_hidden = n_hidden \n",
        "    self.drop_prob = drop_prob\n",
        "    self.lr = lr \n",
        "\n",
        "    # create character dictionaries\n",
        "    self.chars = tokens \n",
        "    self.int_to_char = dict(enumerate(self.chars))\n",
        "    self.char_to_int = {ch:idx for idx, ch in self.int_to_char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out, hidden\n",
        "\n",
        "  def initialize_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    # initialize hidden state and cell state of LSTM with zeros (n_layers * batch_size * n_hidden)\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "metadata": {
        "id": "jx8zpIx8Re2E"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 512 \n",
        "n_layers = 2 \n",
        "drop_prob=0.5\n",
        "lr=0.001\n",
        "\n",
        "model = RNN(chars, n_hidden, n_layers, drop_prob, lr)\n",
        "print(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "rxQgMBKE50NJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f56d46-b4ff-4dcb-e36e-b3594921bfe2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "8cgoevWpoyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every=10):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(model.chars)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # initialize the hidden state\n",
        "    h = model.initialize_hidden(batch_size)\n",
        "\n",
        "    for inputs, targets in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1 \n",
        "      # one-hot encode the data\n",
        "      inputs = one_hot_encode(inputs, n_chars)\n",
        "      # make torch tensor\n",
        "      inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "      # move the tensors to the right device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      # create new variable for the hidden state to avoid backprop through the \n",
        "      # entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      # clear the gradients of all optimized variables\n",
        "      model.zero_grad()\n",
        "      # forward pass\n",
        "      output, h = model(inputs, h)\n",
        "      # calculate the loss\n",
        "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "      # backprob\n",
        "      loss.backward()\n",
        "      # prevent exploding gradients problem in rnn/lstm\n",
        "      nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # ------------ validate the model -----------------\n",
        "      if counter % print_every == 0:\n",
        "        # initialize the hidden state\n",
        "        valid_h = model.initialize_hidden(batch_size)\n",
        "\n",
        "        valid_losses = []\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for inputs, targets in get_batches(valid_data, batch_size, seq_length):\n",
        "          # one-hot encode the inputs\n",
        "          inputs = one_hot_encode(inputs, n_chars)\n",
        "          # make torch tensor\n",
        "          inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "          # create new variable for the hidden state to avoid backprop through the \n",
        "          # entire training history \n",
        "          valid_h = tuple([each for each in valid_h])\n",
        "          # move the tensor to the right device\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          # forward pass\n",
        "          output, valid_h = model(inputs, valid_h)\n",
        "          # calculate the batch loss\n",
        "          valid_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "          valid_losses.append(valid_loss.item())\n",
        "\n",
        "        # reset to train mode\n",
        "        model.train()\n",
        "\n",
        "        print(\"Epochs: {} \\tStep: {} \\tTraining loss: {:.6f} \\tValidation loss: {:.6f}\".format(epoch+1, \n",
        "                                                                                               counter, \n",
        "                                                                                               loss.item(), \n",
        "                                                                                               np.mean(valid_losses)))"
      ],
      "metadata": {
        "id": "IsnPrdprRe5H"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "lr=0.001\n",
        "clip = 5\n",
        "print_every=10\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every=10)"
      ],
      "metadata": {
        "id": "SjF7eho750QE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cf23ee6-c67c-48ea-d83b-28f50da7ef36"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-5a209efc95a1>:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), clip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 \tStep: 10 \tTraining loss: 3.016371 \tValidation loss: 2.998936\n",
            "Epochs: 1 \tStep: 20 \tTraining loss: 2.923572 \tValidation loss: 2.909138\n",
            "Epochs: 1 \tStep: 30 \tTraining loss: 2.786562 \tValidation loss: 2.814736\n",
            "Epochs: 1 \tStep: 40 \tTraining loss: 2.674449 \tValidation loss: 2.668220\n",
            "Epochs: 1 \tStep: 50 \tTraining loss: 2.590210 \tValidation loss: 2.552837\n",
            "Epochs: 1 \tStep: 60 \tTraining loss: 2.527857 \tValidation loss: 2.503482\n",
            "Epochs: 1 \tStep: 70 \tTraining loss: 2.477844 \tValidation loss: 2.445897\n",
            "Epochs: 1 \tStep: 80 \tTraining loss: 2.444426 \tValidation loss: 2.410971\n",
            "Epochs: 1 \tStep: 90 \tTraining loss: 2.416042 \tValidation loss: 2.370056\n",
            "Epochs: 1 \tStep: 100 \tTraining loss: 2.361307 \tValidation loss: 2.329679\n",
            "Epochs: 1 \tStep: 110 \tTraining loss: 2.338189 \tValidation loss: 2.293090\n",
            "Epochs: 1 \tStep: 120 \tTraining loss: 2.309092 \tValidation loss: 2.260346\n",
            "Epochs: 1 \tStep: 130 \tTraining loss: 2.272703 \tValidation loss: 2.244631\n",
            "Epochs: 1 \tStep: 140 \tTraining loss: 2.262676 \tValidation loss: 2.206252\n",
            "Epochs: 1 \tStep: 150 \tTraining loss: 2.221212 \tValidation loss: 2.182786\n",
            "Epochs: 2 \tStep: 160 \tTraining loss: 2.187258 \tValidation loss: 2.157280\n",
            "Epochs: 2 \tStep: 170 \tTraining loss: 2.160297 \tValidation loss: 2.131328\n",
            "Epochs: 2 \tStep: 180 \tTraining loss: 2.138675 \tValidation loss: 2.105278\n",
            "Epochs: 2 \tStep: 190 \tTraining loss: 2.125119 \tValidation loss: 2.084969\n",
            "Epochs: 2 \tStep: 200 \tTraining loss: 2.090819 \tValidation loss: 2.056802\n",
            "Epochs: 2 \tStep: 210 \tTraining loss: 2.063466 \tValidation loss: 2.039560\n",
            "Epochs: 2 \tStep: 220 \tTraining loss: 2.047600 \tValidation loss: 2.019820\n",
            "Epochs: 2 \tStep: 230 \tTraining loss: 2.039400 \tValidation loss: 1.999812\n",
            "Epochs: 2 \tStep: 240 \tTraining loss: 2.006268 \tValidation loss: 1.982761\n",
            "Epochs: 2 \tStep: 250 \tTraining loss: 2.040069 \tValidation loss: 1.959403\n",
            "Epochs: 2 \tStep: 260 \tTraining loss: 1.966975 \tValidation loss: 1.943008\n",
            "Epochs: 2 \tStep: 270 \tTraining loss: 1.956186 \tValidation loss: 1.920430\n",
            "Epochs: 2 \tStep: 280 \tTraining loss: 1.945949 \tValidation loss: 1.902158\n",
            "Epochs: 2 \tStep: 290 \tTraining loss: 1.950511 \tValidation loss: 1.885717\n",
            "Epochs: 2 \tStep: 300 \tTraining loss: 1.944580 \tValidation loss: 1.864419\n",
            "Epochs: 2 \tStep: 310 \tTraining loss: 1.908761 \tValidation loss: 1.852312\n",
            "Epochs: 3 \tStep: 320 \tTraining loss: 1.879394 \tValidation loss: 1.837301\n",
            "Epochs: 3 \tStep: 330 \tTraining loss: 1.854907 \tValidation loss: 1.825635\n",
            "Epochs: 3 \tStep: 340 \tTraining loss: 1.879532 \tValidation loss: 1.808092\n",
            "Epochs: 3 \tStep: 350 \tTraining loss: 1.848016 \tValidation loss: 1.794345\n",
            "Epochs: 3 \tStep: 360 \tTraining loss: 1.855525 \tValidation loss: 1.780389\n",
            "Epochs: 3 \tStep: 370 \tTraining loss: 1.806623 \tValidation loss: 1.767022\n",
            "Epochs: 3 \tStep: 380 \tTraining loss: 1.851388 \tValidation loss: 1.757731\n",
            "Epochs: 3 \tStep: 390 \tTraining loss: 1.804838 \tValidation loss: 1.744576\n",
            "Epochs: 3 \tStep: 400 \tTraining loss: 1.792985 \tValidation loss: 1.731149\n",
            "Epochs: 3 \tStep: 410 \tTraining loss: 1.798403 \tValidation loss: 1.722541\n",
            "Epochs: 3 \tStep: 420 \tTraining loss: 1.806451 \tValidation loss: 1.708038\n",
            "Epochs: 3 \tStep: 430 \tTraining loss: 1.769637 \tValidation loss: 1.696007\n",
            "Epochs: 3 \tStep: 440 \tTraining loss: 1.752373 \tValidation loss: 1.687301\n",
            "Epochs: 3 \tStep: 450 \tTraining loss: 1.777947 \tValidation loss: 1.680720\n",
            "Epochs: 3 \tStep: 460 \tTraining loss: 1.759344 \tValidation loss: 1.669412\n",
            "Epochs: 4 \tStep: 470 \tTraining loss: 1.718813 \tValidation loss: 1.662702\n",
            "Epochs: 4 \tStep: 480 \tTraining loss: 1.699098 \tValidation loss: 1.650593\n",
            "Epochs: 4 \tStep: 490 \tTraining loss: 1.703503 \tValidation loss: 1.642708\n",
            "Epochs: 4 \tStep: 500 \tTraining loss: 1.715363 \tValidation loss: 1.633087\n",
            "Epochs: 4 \tStep: 510 \tTraining loss: 1.703044 \tValidation loss: 1.626350\n",
            "Epochs: 4 \tStep: 520 \tTraining loss: 1.678313 \tValidation loss: 1.620393\n",
            "Epochs: 4 \tStep: 530 \tTraining loss: 1.685188 \tValidation loss: 1.610963\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-bdcadf7e20b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-5a209efc95a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m# clear the gradients of all optimized variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \"\"\"\n\u001b[0;32m-> 1706\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1733\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1679\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, char, h=None, top_k=None):\n",
        "\n",
        "  x = np.array([[model.char_to_int[char]]])\n",
        "  x = one_hot_encode(x, len(model.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  p = F.softmax(out, dim=1).data \n",
        "\n",
        "  if train_on_gpu:\n",
        "    p = p.cpu()\n",
        "\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(model.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p = p/p.sum())\n",
        "  return model.int_to_char[char], h"
      ],
      "metadata": {
        "id": "ZDPcIBLy50WH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime=\"The\", top_k=None):\n",
        "  model.to(device)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  chars = [ch for ch in prime]\n",
        "  h = model.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "  return \"\".join(chars)"
      ],
      "metadata": {
        "id": "K0cSQ5KL50Y5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 2000, prime=\"Anna\", top_k=5))"
      ],
      "metadata": {
        "id": "oUBVQmOYO6is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6b0e8f-7d8b-4e7e-b985-9104f64708a9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna,\n",
            "how it was the same teacher, was that he must think about her own\n",
            "sort. And it was so successful. This in an old peinters would not\n",
            "high and corserve himself that, and with an exasperated partners.\n",
            "\n",
            "The liver was still he could not be asking an old particular wither\n",
            "of an interest in her acquaintances, and that there was nothing so like\n",
            "a shirt, to be disappointed. As he had such an answer. She set the\n",
            "hall was gown away, and as she would have come to stop it,\" said Anna, smiling,\n",
            "with a smile. \"It's a meaning face, as it was better than all the same.\"\n",
            "\n",
            "\"Yes, it was not tired.\"\n",
            "\n",
            "\"It's so,\" she thought, \"that I don't stretch her in the hand of it.\n",
            "That is to get in,\" she added, so it was still more angry at the\n",
            "simple and election, and went out.\n",
            "\n",
            "And the sick man he had been continually staying with his father,\n",
            "who had not spent, and with the happiest consciousness\n",
            "of her sisters, and this was, so he was seeing him to be\n",
            "clearly, and that he had a continual gaily cheek in a stort.\n",
            "\n",
            "\"All right. I'll talk about you,\" answered Levin, and when he saw the\n",
            "ball, her face and tigalery and went to the study to a significance of\n",
            "the discovery. A chief state of husband, and was not all the same\n",
            "that it was satisfied with this condition. The merchant was angry,\n",
            "while she could have brought a corner of a look of stern, and\n",
            "the prayers, and said something in the same place of his sister's. The\n",
            "same shaking hands, who had speak of answering water, with\n",
            "a sout, the black price, and the peasants about too.\n",
            "\n",
            "After an instant of his conversation with her work. He saw that\n",
            "the marshal of the position had brought such a sense of her artist,\n",
            "tried to see him, and would have layed to go to him, she could not\n",
            "believe it. As soon as the most sin in his hands had already saw nothing\n",
            "that all at the porter, who, serious eyes, and at the same\n",
            "strange to be as so sorry for her son. But at the same time this with\n",
            "her son's soul was still so strange in his heart and set an one\n",
            "of the sensations \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"char_rnn.model\"\n",
        "\n",
        "checkpoint = {\"n_hidden\": model.n_hidden,\n",
        "              \"n_layers\": model.n_layers,\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"tokens\": model.chars}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "1kHHTVxN50TF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"char_rnn.model\", \"rb\") as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint[\"n_hidden\"], n_layers=checkpoint[\"n_layers\"])\n",
        "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
        "print(sample(loaded, size = 2000, prime=\"And Levin said\", top_k=5))"
      ],
      "metadata": {
        "id": "z4zZJCN3PFHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2c914a-e7c3-42c4-aed0-dc1229e1e76e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And Levin said:\n",
            "\n",
            "\"You must give him a girl and would bucht on the beginning!\"\n",
            "\n",
            "\"Why do you want?\"\n",
            "\n",
            "\"I haven't the same,\" said Levin.\n",
            "\n",
            "\"Oh, this woman has told me a step defect on all the particular of\n",
            "his position and so much affected by a sort of men of a treather, and\n",
            "to manage to answer the professor.\"\n",
            "\n",
            "\"Why is it you had a chesce to the problem? What do you say, I describe\n",
            "myself on that story.\"\n",
            "\n",
            "\"What a sense of teachers think of it. He doesn't been in this\n",
            "position in which, and I drive up with her into the wedding\n",
            "set to see her. They don't but have any, but he's so delighted.\"\n",
            "\n",
            "Alexey Alexandrovitch got up, when he was all open was despising in her arms\n",
            "in his corners, and shook her face, and settling at the same time in\n",
            "all with which he had already sent the politicial committee on his\n",
            "son, who had not must be dull say, and a signs of conversation with which\n",
            "he came the hall--who should say that he was at the same time. Anna with\n",
            "his handsome, station in his handsome waters, silent, had the\n",
            "cross, walked into his elbow to his brother, he went to the street and\n",
            "walked off, the sort of manner, without stopping. Tit showed them,\n",
            "and the most pride was stroking somewhere when, when they saw a\n",
            "shadow on the colonel over his hand with a smile. She was tried to look\n",
            "at the countess with a memories of the princess and Varenka,\n",
            "the sight of their happiness, and with a bad clouds he said all\n",
            "these three men in the woman that had seemed to her in a stripp of\n",
            "correct man, and was stupid. They detailed a little.\n",
            "\n",
            "\"At one mind. Would you say all this moral? I'm glad to see you,\" said Anna.\n",
            "\n",
            "\"What do you say?\" she said, smiling as it were happy, and stood\n",
            "still, she began talking about it.\n",
            "\n",
            "They went into the drawing room to see her. But he was so much to do anything\n",
            "talked to her and had a sound for, and a little girl, and the sense\n",
            "first they had been all over again and went to her. They did not shrive\n",
            "and show herself down in the carriage. At her end was still hope.\n",
            "\n",
            "\"You know her me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwbIx2D6lLRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}