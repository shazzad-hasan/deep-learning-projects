{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/seq_to_seq/char_level_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CITpOagOBv3Z",
        "outputId": "40687300-8bc9-49c7-d140-90f959871e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca87dada-3d04-4a44-a9b6-70cf8bb2aac9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca87dada-3d04-4a44-a9b6-70cf8bb2aac9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"shazzadraihan\",\"key\":\"da63bbe0f8dcb3bd7fb35034046ca758\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# upload kaggle API key from your local machine\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a kaggle dir, copy the API key to it\n",
        "# and make sure the file in only readable by yourself (chmod 600)\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IajRinOUFcEm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use API command to download the dataset\n",
        "!kaggle datasets download -d wanderdust/anna-karenina-book"
      ],
      "metadata": {
        "id": "BGlxw58uGJf1",
        "outputId": "738249e0-4dd8-4ab1-92b0-a0b8d8e5d54f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading anna-karenina-book.zip to /content\n",
            "100% 739k/739k [00:00<00:00, 898kB/s]\n",
            "100% 739k/739k [00:00<00:00, 897kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the dataset\n",
        "!unzip -qq anna-karenina-book.zip"
      ],
      "metadata": {
        "id": "QfxEg189G_4J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M7s-hXEgM4tN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print(\"CUDA is not available\")\n",
        "else:\n",
        "  print(\"CUDA is available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "hmDInJ5mcHI0",
        "outputId": "7abd3e60-b664-454c-b64f-e4a7100e91b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open text file and read dataset\n",
        "with open(\"/content/anna.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "metadata": {
        "id": "ruMs8t0lGOwW",
        "outputId": "c5ea156b-efc8-4cbb-daed-8ea7edc83a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the dataset"
      ],
      "metadata": {
        "id": "1XeTje0xHgu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "chars = tuple(set(text))\n",
        "# map each int to char\n",
        "int_to_char = dict(enumerate(chars))\n",
        "# map each char to int\n",
        "char_to_int = {ch:idx for idx, ch in int_to_char.items()}\n",
        "\n",
        "# encode \n",
        "encoded = np.array([char_to_int[ch] for ch in text])\n",
        "\n",
        "encoded[:100]"
      ],
      "metadata": {
        "id": "AuOH3bqVHRpP",
        "outputId": "f1ccbccc-11e9-4286-8777-70b4283bf81f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([62, 52, 47, 75, 81,  3, 70, 55, 61, 22, 22, 22, 76, 47, 75, 75, 68,\n",
              "       55,  1, 47,  4, 71,  7, 71,  3, 51, 55, 47, 70,  3, 55, 47,  7,  7,\n",
              "       55, 47,  7, 71, 53,  3, 28, 55,  3, 10,  3, 70, 68, 55, 72, 30, 52,\n",
              "       47, 75, 75, 68, 55,  1, 47,  4, 71,  7, 68, 55, 71, 51, 55, 72, 30,\n",
              "       52, 47, 75, 75, 68, 55, 71, 30, 55, 71, 81, 51, 55, 34, 13, 30, 22,\n",
              "       13, 47, 68, 12, 22, 22, 16, 10,  3, 70, 68, 81, 52, 71, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  # initialize the encoded array with zeros\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  # fill with ones where appropriate\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  # reshape to get back to the original array\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  \n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "Z_18UrpVQAjm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  \"\"\"returns batches of size batch_size * seq_length\"\"\"\n",
        "  total_batch_size = batch_size * seq_length\n",
        "  # total number of batches\n",
        "  n_batches = len(arr)//total_batch_size\n",
        "  \n",
        "  # keep enough characters to make full batches\n",
        "  arr = arr[:n_batches * total_batch_size]\n",
        "  # reshape into batch_size rows\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  \n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "      # features\n",
        "      x = arr[:, n:n+seq_length]\n",
        "      # targets, shifted by one\n",
        "      y = np.zeros_like(x)\n",
        "      try:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "      except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "      yield x, y\n"
      ],
      "metadata": {
        "id": "XxElsQitRNph"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "valid_size = 0.1\n",
        "\n",
        "valid_idx = int(len(encoded)*(1-valid_size))\n",
        "train_data, valid_data = encoded[:valid_idx], encoded[valid_idx:]"
      ],
      "metadata": {
        "id": "hZxzVyi6hpNA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "vCVb7_7fcL7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden, n_layers, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.n_layers = n_layers \n",
        "    self.n_hidden = n_hidden \n",
        "    self.drop_prob = drop_prob\n",
        "    self.lr = lr \n",
        "\n",
        "    # create character dictionaries\n",
        "    self.chars = tokens \n",
        "    self.int_to_char = dict(enumerate(self.chars))\n",
        "    self.char_to_int = {ch:idx for idx, ch in self.int_to_char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out, hidden\n",
        "\n",
        "  def initialize_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    # initialize hidden state and cell state of LSTM with zeros (n_layers * batch_size * n_hidden)\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "metadata": {
        "id": "jx8zpIx8Re2E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 512 \n",
        "n_layers = 2 \n",
        "drop_prob=0.5\n",
        "lr=0.001\n",
        "\n",
        "model = RNN(chars, n_hidden, n_layers, drop_prob, lr)\n",
        "print(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "rxQgMBKE50NJ",
        "outputId": "f9f04f46-0c17-497e-f1bb-a7f11fdf63a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "8cgoevWpoyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every=10):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(model.chars)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # initialize the hidden state\n",
        "    h = model.initialize_hidden(batch_size)\n",
        "\n",
        "    for inputs, targets in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1 \n",
        "      # one-hot encode the data\n",
        "      inputs = one_hot_encode(inputs, n_chars)\n",
        "      # make torch tensor\n",
        "      inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "      # move the tensors to the right device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      # create new variable for the hidden state to avoid backprop through the \n",
        "      # entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      # clear the gradients of all optimized variables\n",
        "      model.zero_grad()\n",
        "      # forward pass\n",
        "      output, h = model(inputs, h)\n",
        "      # calculate the loss\n",
        "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "      # backprob\n",
        "      loss.backward()\n",
        "      # prevent exploding gradients problem in rnn/lstm\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # ------------ validate the model -----------------\n",
        "      if counter % print_every == 0:\n",
        "        # initialize the hidden state\n",
        "        valid_h = model.initialize_hidden(batch_size)\n",
        "\n",
        "        valid_losses = []\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for inputs, targets in get_batches(valid_data, batch_size, seq_length):\n",
        "          # one-hot encode the inputs\n",
        "          inputs = one_hot_encode(inputs, n_chars)\n",
        "          # make torch tensor\n",
        "          inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "          # create new variable for the hidden state to avoid backprop through the \n",
        "          # entire training history \n",
        "          valid_h = tuple([each for each in valid_h])\n",
        "          # move the tensor to the right device\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          # forward pass\n",
        "          output, valid_h = model(inputs, valid_h)\n",
        "          # calculate the batch loss\n",
        "          valid_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "          valid_losses.append(valid_loss.item())\n",
        "\n",
        "        # reset to train mode\n",
        "        model.train()\n",
        "\n",
        "        print(\"Epochs: {} \\tStep: {} \\tTraining loss: {:.6f} \\tValidation loss: {:.6f}\".format(epoch+1, \n",
        "                                                                                               counter, \n",
        "                                                                                               loss.item(), \n",
        "                                                                                               np.mean(valid_losses)))"
      ],
      "metadata": {
        "id": "IsnPrdprRe5H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "seq_length = 200\n",
        "lr=0.001\n",
        "clip = 5\n",
        "print_every=10\n",
        "\n",
        "# define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "# define a loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every)"
      ],
      "metadata": {
        "id": "SjF7eho750QE",
        "outputId": "456fb2ad-7295-489b-f63c-3d130c13e139",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 \tStep: 10 \tTraining loss: 3.253586 \tValidation loss: 3.184217\n",
            "Epochs: 1 \tStep: 20 \tTraining loss: 3.171566 \tValidation loss: 3.114517\n",
            "Epochs: 1 \tStep: 30 \tTraining loss: 3.134019 \tValidation loss: 3.100145\n",
            "Epochs: 1 \tStep: 40 \tTraining loss: 3.137975 \tValidation loss: 3.096791\n",
            "Epochs: 1 \tStep: 50 \tTraining loss: 3.101921 \tValidation loss: 3.092978\n",
            "Epochs: 1 \tStep: 60 \tTraining loss: 3.133132 \tValidation loss: 3.092322\n",
            "Epochs: 1 \tStep: 70 \tTraining loss: 3.123014 \tValidation loss: 3.089268\n",
            "Epochs: 2 \tStep: 80 \tTraining loss: 3.101340 \tValidation loss: 3.083387\n",
            "Epochs: 2 \tStep: 90 \tTraining loss: 3.089215 \tValidation loss: 3.071867\n",
            "Epochs: 2 \tStep: 100 \tTraining loss: 3.067941 \tValidation loss: 3.037249\n",
            "Epochs: 2 \tStep: 110 \tTraining loss: 3.011845 \tValidation loss: 2.970101\n",
            "Epochs: 2 \tStep: 120 \tTraining loss: 2.910742 \tValidation loss: 2.847421\n",
            "Epochs: 2 \tStep: 130 \tTraining loss: 2.810991 \tValidation loss: 2.745389\n",
            "Epochs: 2 \tStep: 140 \tTraining loss: 2.709513 \tValidation loss: 2.635672\n",
            "Epochs: 2 \tStep: 150 \tTraining loss: 2.618359 \tValidation loss: 2.547693\n",
            "Epochs: 3 \tStep: 160 \tTraining loss: 2.554372 \tValidation loss: 2.459487\n",
            "Epochs: 3 \tStep: 170 \tTraining loss: 2.483126 \tValidation loss: 2.412549\n",
            "Epochs: 3 \tStep: 180 \tTraining loss: 2.451112 \tValidation loss: 2.365139\n",
            "Epochs: 3 \tStep: 190 \tTraining loss: 2.405185 \tValidation loss: 2.327397\n",
            "Epochs: 3 \tStep: 200 \tTraining loss: 2.360353 \tValidation loss: 2.310744\n",
            "Epochs: 3 \tStep: 210 \tTraining loss: 2.334660 \tValidation loss: 2.274752\n",
            "Epochs: 3 \tStep: 220 \tTraining loss: 2.318538 \tValidation loss: 2.245363\n",
            "Epochs: 3 \tStep: 230 \tTraining loss: 2.287179 \tValidation loss: 2.212005\n",
            "Epochs: 4 \tStep: 240 \tTraining loss: 2.265656 \tValidation loss: 2.180674\n",
            "Epochs: 4 \tStep: 250 \tTraining loss: 2.235425 \tValidation loss: 2.150691\n",
            "Epochs: 4 \tStep: 260 \tTraining loss: 2.220802 \tValidation loss: 2.125372\n",
            "Epochs: 4 \tStep: 270 \tTraining loss: 2.185433 \tValidation loss: 2.095349\n",
            "Epochs: 4 \tStep: 280 \tTraining loss: 2.160530 \tValidation loss: 2.072408\n",
            "Epochs: 4 \tStep: 290 \tTraining loss: 2.128755 \tValidation loss: 2.046279\n",
            "Epochs: 4 \tStep: 300 \tTraining loss: 2.104125 \tValidation loss: 2.023832\n",
            "Epochs: 5 \tStep: 310 \tTraining loss: 2.084571 \tValidation loss: 2.000888\n",
            "Epochs: 5 \tStep: 320 \tTraining loss: 2.088803 \tValidation loss: 1.980334\n",
            "Epochs: 5 \tStep: 330 \tTraining loss: 2.036621 \tValidation loss: 1.961816\n",
            "Epochs: 5 \tStep: 340 \tTraining loss: 2.039758 \tValidation loss: 1.938400\n",
            "Epochs: 5 \tStep: 350 \tTraining loss: 2.001055 \tValidation loss: 1.921035\n",
            "Epochs: 5 \tStep: 360 \tTraining loss: 2.002208 \tValidation loss: 1.903865\n",
            "Epochs: 5 \tStep: 370 \tTraining loss: 1.980483 \tValidation loss: 1.886072\n",
            "Epochs: 5 \tStep: 380 \tTraining loss: 1.959139 \tValidation loss: 1.865482\n",
            "Epochs: 6 \tStep: 390 \tTraining loss: 1.955232 \tValidation loss: 1.849169\n",
            "Epochs: 6 \tStep: 400 \tTraining loss: 1.934964 \tValidation loss: 1.833958\n",
            "Epochs: 6 \tStep: 410 \tTraining loss: 1.954236 \tValidation loss: 1.817611\n",
            "Epochs: 6 \tStep: 420 \tTraining loss: 1.920082 \tValidation loss: 1.803863\n",
            "Epochs: 6 \tStep: 430 \tTraining loss: 1.889293 \tValidation loss: 1.787923\n",
            "Epochs: 6 \tStep: 440 \tTraining loss: 1.872587 \tValidation loss: 1.775487\n",
            "Epochs: 6 \tStep: 450 \tTraining loss: 1.890172 \tValidation loss: 1.762574\n",
            "Epochs: 6 \tStep: 460 \tTraining loss: 1.855209 \tValidation loss: 1.749045\n",
            "Epochs: 7 \tStep: 470 \tTraining loss: 1.834543 \tValidation loss: 1.738884\n",
            "Epochs: 7 \tStep: 480 \tTraining loss: 1.845291 \tValidation loss: 1.723871\n",
            "Epochs: 7 \tStep: 490 \tTraining loss: 1.824275 \tValidation loss: 1.708740\n",
            "Epochs: 7 \tStep: 500 \tTraining loss: 1.801651 \tValidation loss: 1.699911\n",
            "Epochs: 7 \tStep: 510 \tTraining loss: 1.817095 \tValidation loss: 1.688222\n",
            "Epochs: 7 \tStep: 520 \tTraining loss: 1.760835 \tValidation loss: 1.674684\n",
            "Epochs: 7 \tStep: 530 \tTraining loss: 1.760531 \tValidation loss: 1.665481\n",
            "Epochs: 8 \tStep: 540 \tTraining loss: 1.783022 \tValidation loss: 1.656940\n",
            "Epochs: 8 \tStep: 550 \tTraining loss: 1.787350 \tValidation loss: 1.645121\n",
            "Epochs: 8 \tStep: 560 \tTraining loss: 1.730011 \tValidation loss: 1.637918\n",
            "Epochs: 8 \tStep: 570 \tTraining loss: 1.763789 \tValidation loss: 1.624748\n",
            "Epochs: 8 \tStep: 580 \tTraining loss: 1.712630 \tValidation loss: 1.617822\n",
            "Epochs: 8 \tStep: 590 \tTraining loss: 1.716087 \tValidation loss: 1.609696\n",
            "Epochs: 8 \tStep: 600 \tTraining loss: 1.705072 \tValidation loss: 1.601226\n",
            "Epochs: 8 \tStep: 610 \tTraining loss: 1.700722 \tValidation loss: 1.593571\n",
            "Epochs: 9 \tStep: 620 \tTraining loss: 1.689212 \tValidation loss: 1.585290\n",
            "Epochs: 9 \tStep: 630 \tTraining loss: 1.704807 \tValidation loss: 1.577485\n",
            "Epochs: 9 \tStep: 640 \tTraining loss: 1.712027 \tValidation loss: 1.570160\n",
            "Epochs: 9 \tStep: 650 \tTraining loss: 1.709130 \tValidation loss: 1.558568\n",
            "Epochs: 9 \tStep: 660 \tTraining loss: 1.656451 \tValidation loss: 1.553312\n",
            "Epochs: 9 \tStep: 670 \tTraining loss: 1.667236 \tValidation loss: 1.546922\n",
            "Epochs: 9 \tStep: 680 \tTraining loss: 1.639829 \tValidation loss: 1.536018\n",
            "Epochs: 9 \tStep: 690 \tTraining loss: 1.652836 \tValidation loss: 1.530821\n",
            "Epochs: 10 \tStep: 700 \tTraining loss: 1.655183 \tValidation loss: 1.528637\n",
            "Epochs: 10 \tStep: 710 \tTraining loss: 1.617092 \tValidation loss: 1.519737\n",
            "Epochs: 10 \tStep: 720 \tTraining loss: 1.630311 \tValidation loss: 1.510967\n",
            "Epochs: 10 \tStep: 730 \tTraining loss: 1.647389 \tValidation loss: 1.503246\n",
            "Epochs: 10 \tStep: 740 \tTraining loss: 1.605633 \tValidation loss: 1.500416\n",
            "Epochs: 10 \tStep: 750 \tTraining loss: 1.584329 \tValidation loss: 1.493119\n",
            "Epochs: 10 \tStep: 760 \tTraining loss: 1.583738 \tValidation loss: 1.487134\n",
            "Epochs: 10 \tStep: 770 \tTraining loss: 1.616235 \tValidation loss: 1.481574\n",
            "Epochs: 11 \tStep: 780 \tTraining loss: 1.610336 \tValidation loss: 1.479723\n",
            "Epochs: 11 \tStep: 790 \tTraining loss: 1.569449 \tValidation loss: 1.471363\n",
            "Epochs: 11 \tStep: 800 \tTraining loss: 1.556960 \tValidation loss: 1.462694\n",
            "Epochs: 11 \tStep: 810 \tTraining loss: 1.593047 \tValidation loss: 1.457745\n",
            "Epochs: 11 \tStep: 820 \tTraining loss: 1.555163 \tValidation loss: 1.455279\n",
            "Epochs: 11 \tStep: 830 \tTraining loss: 1.576203 \tValidation loss: 1.446923\n",
            "Epochs: 11 \tStep: 840 \tTraining loss: 1.562129 \tValidation loss: 1.442497\n",
            "Epochs: 12 \tStep: 850 \tTraining loss: 1.522602 \tValidation loss: 1.444251\n",
            "Epochs: 12 \tStep: 860 \tTraining loss: 1.539093 \tValidation loss: 1.435108\n",
            "Epochs: 12 \tStep: 870 \tTraining loss: 1.543809 \tValidation loss: 1.429784\n",
            "Epochs: 12 \tStep: 880 \tTraining loss: 1.547625 \tValidation loss: 1.422952\n",
            "Epochs: 12 \tStep: 890 \tTraining loss: 1.523621 \tValidation loss: 1.420854\n",
            "Epochs: 12 \tStep: 900 \tTraining loss: 1.524475 \tValidation loss: 1.417515\n",
            "Epochs: 12 \tStep: 910 \tTraining loss: 1.527599 \tValidation loss: 1.411962\n",
            "Epochs: 12 \tStep: 920 \tTraining loss: 1.536671 \tValidation loss: 1.405219\n",
            "Epochs: 13 \tStep: 930 \tTraining loss: 1.528465 \tValidation loss: 1.409273\n",
            "Epochs: 13 \tStep: 940 \tTraining loss: 1.523943 \tValidation loss: 1.401132\n",
            "Epochs: 13 \tStep: 950 \tTraining loss: 1.550163 \tValidation loss: 1.397765\n",
            "Epochs: 13 \tStep: 960 \tTraining loss: 1.511480 \tValidation loss: 1.391377\n",
            "Epochs: 13 \tStep: 970 \tTraining loss: 1.494286 \tValidation loss: 1.387196\n",
            "Epochs: 13 \tStep: 980 \tTraining loss: 1.488047 \tValidation loss: 1.383652\n",
            "Epochs: 13 \tStep: 990 \tTraining loss: 1.497877 \tValidation loss: 1.378614\n",
            "Epochs: 13 \tStep: 1000 \tTraining loss: 1.483445 \tValidation loss: 1.375181\n",
            "Epochs: 14 \tStep: 1010 \tTraining loss: 1.511458 \tValidation loss: 1.373423\n",
            "Epochs: 14 \tStep: 1020 \tTraining loss: 1.460106 \tValidation loss: 1.367724\n",
            "Epochs: 14 \tStep: 1030 \tTraining loss: 1.487878 \tValidation loss: 1.361974\n",
            "Epochs: 14 \tStep: 1040 \tTraining loss: 1.482381 \tValidation loss: 1.359134\n",
            "Epochs: 14 \tStep: 1050 \tTraining loss: 1.464774 \tValidation loss: 1.357222\n",
            "Epochs: 14 \tStep: 1060 \tTraining loss: 1.463299 \tValidation loss: 1.353896\n",
            "Epochs: 14 \tStep: 1070 \tTraining loss: 1.459304 \tValidation loss: 1.350006\n",
            "Epochs: 15 \tStep: 1080 \tTraining loss: 1.451007 \tValidation loss: 1.348408\n",
            "Epochs: 15 \tStep: 1090 \tTraining loss: 1.471464 \tValidation loss: 1.344425\n",
            "Epochs: 15 \tStep: 1100 \tTraining loss: 1.432967 \tValidation loss: 1.342354\n",
            "Epochs: 15 \tStep: 1110 \tTraining loss: 1.460685 \tValidation loss: 1.338758\n",
            "Epochs: 15 \tStep: 1120 \tTraining loss: 1.424361 \tValidation loss: 1.333786\n",
            "Epochs: 15 \tStep: 1130 \tTraining loss: 1.450853 \tValidation loss: 1.331465\n",
            "Epochs: 15 \tStep: 1140 \tTraining loss: 1.428296 \tValidation loss: 1.327631\n",
            "Epochs: 15 \tStep: 1150 \tTraining loss: 1.427437 \tValidation loss: 1.324876\n",
            "Epochs: 16 \tStep: 1160 \tTraining loss: 1.442176 \tValidation loss: 1.326111\n",
            "Epochs: 16 \tStep: 1170 \tTraining loss: 1.431960 \tValidation loss: 1.319503\n",
            "Epochs: 16 \tStep: 1180 \tTraining loss: 1.467935 \tValidation loss: 1.317489\n",
            "Epochs: 16 \tStep: 1190 \tTraining loss: 1.444942 \tValidation loss: 1.312683\n",
            "Epochs: 16 \tStep: 1200 \tTraining loss: 1.435251 \tValidation loss: 1.311390\n",
            "Epochs: 16 \tStep: 1210 \tTraining loss: 1.401619 \tValidation loss: 1.308646\n",
            "Epochs: 16 \tStep: 1220 \tTraining loss: 1.433154 \tValidation loss: 1.302943\n",
            "Epochs: 16 \tStep: 1230 \tTraining loss: 1.405388 \tValidation loss: 1.305088\n",
            "Epochs: 17 \tStep: 1240 \tTraining loss: 1.417519 \tValidation loss: 1.302904\n",
            "Epochs: 17 \tStep: 1250 \tTraining loss: 1.423415 \tValidation loss: 1.301264\n",
            "Epochs: 17 \tStep: 1260 \tTraining loss: 1.415868 \tValidation loss: 1.295519\n",
            "Epochs: 17 \tStep: 1270 \tTraining loss: 1.405007 \tValidation loss: 1.293121\n",
            "Epochs: 17 \tStep: 1280 \tTraining loss: 1.417536 \tValidation loss: 1.291142\n",
            "Epochs: 17 \tStep: 1290 \tTraining loss: 1.371142 \tValidation loss: 1.288515\n",
            "Epochs: 17 \tStep: 1300 \tTraining loss: 1.384901 \tValidation loss: 1.284496\n",
            "Epochs: 18 \tStep: 1310 \tTraining loss: 1.421013 \tValidation loss: 1.282999\n",
            "Epochs: 18 \tStep: 1320 \tTraining loss: 1.411641 \tValidation loss: 1.281791\n",
            "Epochs: 18 \tStep: 1330 \tTraining loss: 1.363273 \tValidation loss: 1.278673\n",
            "Epochs: 18 \tStep: 1340 \tTraining loss: 1.418542 \tValidation loss: 1.274915\n",
            "Epochs: 18 \tStep: 1350 \tTraining loss: 1.370871 \tValidation loss: 1.272136\n",
            "Epochs: 18 \tStep: 1360 \tTraining loss: 1.371327 \tValidation loss: 1.271625\n",
            "Epochs: 18 \tStep: 1370 \tTraining loss: 1.369784 \tValidation loss: 1.266959\n",
            "Epochs: 18 \tStep: 1380 \tTraining loss: 1.368272 \tValidation loss: 1.265587\n",
            "Epochs: 19 \tStep: 1390 \tTraining loss: 1.371976 \tValidation loss: 1.264424\n",
            "Epochs: 19 \tStep: 1400 \tTraining loss: 1.375231 \tValidation loss: 1.264191\n",
            "Epochs: 19 \tStep: 1410 \tTraining loss: 1.384025 \tValidation loss: 1.261938\n",
            "Epochs: 19 \tStep: 1420 \tTraining loss: 1.398439 \tValidation loss: 1.257472\n",
            "Epochs: 19 \tStep: 1430 \tTraining loss: 1.361183 \tValidation loss: 1.257156\n",
            "Epochs: 19 \tStep: 1440 \tTraining loss: 1.359894 \tValidation loss: 1.255801\n",
            "Epochs: 19 \tStep: 1450 \tTraining loss: 1.345590 \tValidation loss: 1.252089\n",
            "Epochs: 19 \tStep: 1460 \tTraining loss: 1.363968 \tValidation loss: 1.251134\n",
            "Epochs: 20 \tStep: 1470 \tTraining loss: 1.375244 \tValidation loss: 1.252356\n",
            "Epochs: 20 \tStep: 1480 \tTraining loss: 1.342745 \tValidation loss: 1.246412\n",
            "Epochs: 20 \tStep: 1490 \tTraining loss: 1.354420 \tValidation loss: 1.245053\n",
            "Epochs: 20 \tStep: 1500 \tTraining loss: 1.372078 \tValidation loss: 1.242686\n",
            "Epochs: 20 \tStep: 1510 \tTraining loss: 1.344153 \tValidation loss: 1.241172\n",
            "Epochs: 20 \tStep: 1520 \tTraining loss: 1.327392 \tValidation loss: 1.239920\n",
            "Epochs: 20 \tStep: 1530 \tTraining loss: 1.327976 \tValidation loss: 1.236875\n",
            "Epochs: 20 \tStep: 1540 \tTraining loss: 1.363863 \tValidation loss: 1.236644\n",
            "Epochs: 21 \tStep: 1550 \tTraining loss: 1.361839 \tValidation loss: 1.236382\n",
            "Epochs: 21 \tStep: 1560 \tTraining loss: 1.332836 \tValidation loss: 1.235437\n",
            "Epochs: 21 \tStep: 1570 \tTraining loss: 1.320510 \tValidation loss: 1.229518\n",
            "Epochs: 21 \tStep: 1580 \tTraining loss: 1.355878 \tValidation loss: 1.225856\n",
            "Epochs: 21 \tStep: 1590 \tTraining loss: 1.315195 \tValidation loss: 1.228689\n",
            "Epochs: 21 \tStep: 1600 \tTraining loss: 1.340998 \tValidation loss: 1.224167\n",
            "Epochs: 21 \tStep: 1610 \tTraining loss: 1.337427 \tValidation loss: 1.223896\n",
            "Epochs: 22 \tStep: 1620 \tTraining loss: 1.299039 \tValidation loss: 1.224131\n",
            "Epochs: 22 \tStep: 1630 \tTraining loss: 1.314404 \tValidation loss: 1.221874\n",
            "Epochs: 22 \tStep: 1640 \tTraining loss: 1.317867 \tValidation loss: 1.219627\n",
            "Epochs: 22 \tStep: 1650 \tTraining loss: 1.321165 \tValidation loss: 1.219114\n",
            "Epochs: 22 \tStep: 1660 \tTraining loss: 1.311626 \tValidation loss: 1.219422\n",
            "Epochs: 22 \tStep: 1670 \tTraining loss: 1.316110 \tValidation loss: 1.214904\n",
            "Epochs: 22 \tStep: 1680 \tTraining loss: 1.320876 \tValidation loss: 1.211760\n",
            "Epochs: 22 \tStep: 1690 \tTraining loss: 1.327126 \tValidation loss: 1.210071\n",
            "Epochs: 23 \tStep: 1700 \tTraining loss: 1.332120 \tValidation loss: 1.211655\n",
            "Epochs: 23 \tStep: 1710 \tTraining loss: 1.315499 \tValidation loss: 1.207804\n",
            "Epochs: 23 \tStep: 1720 \tTraining loss: 1.347621 \tValidation loss: 1.207567\n",
            "Epochs: 23 \tStep: 1730 \tTraining loss: 1.316926 \tValidation loss: 1.205288\n",
            "Epochs: 23 \tStep: 1740 \tTraining loss: 1.311073 \tValidation loss: 1.203779\n",
            "Epochs: 23 \tStep: 1750 \tTraining loss: 1.307849 \tValidation loss: 1.203624\n",
            "Epochs: 23 \tStep: 1760 \tTraining loss: 1.312434 \tValidation loss: 1.202752\n",
            "Epochs: 23 \tStep: 1770 \tTraining loss: 1.298464 \tValidation loss: 1.201073\n",
            "Epochs: 24 \tStep: 1780 \tTraining loss: 1.331121 \tValidation loss: 1.201052\n",
            "Epochs: 24 \tStep: 1790 \tTraining loss: 1.276616 \tValidation loss: 1.196827\n",
            "Epochs: 24 \tStep: 1800 \tTraining loss: 1.307387 \tValidation loss: 1.193892\n",
            "Epochs: 24 \tStep: 1810 \tTraining loss: 1.307846 \tValidation loss: 1.193568\n",
            "Epochs: 24 \tStep: 1820 \tTraining loss: 1.295590 \tValidation loss: 1.192244\n",
            "Epochs: 24 \tStep: 1830 \tTraining loss: 1.288617 \tValidation loss: 1.191990\n",
            "Epochs: 24 \tStep: 1840 \tTraining loss: 1.294699 \tValidation loss: 1.189938\n",
            "Epochs: 25 \tStep: 1850 \tTraining loss: 1.289658 \tValidation loss: 1.187830\n",
            "Epochs: 25 \tStep: 1860 \tTraining loss: 1.313764 \tValidation loss: 1.186634\n",
            "Epochs: 25 \tStep: 1870 \tTraining loss: 1.269246 \tValidation loss: 1.184206\n",
            "Epochs: 25 \tStep: 1880 \tTraining loss: 1.306373 \tValidation loss: 1.184484\n",
            "Epochs: 25 \tStep: 1890 \tTraining loss: 1.270963 \tValidation loss: 1.181408\n",
            "Epochs: 25 \tStep: 1900 \tTraining loss: 1.293329 \tValidation loss: 1.183728\n",
            "Epochs: 25 \tStep: 1910 \tTraining loss: 1.266813 \tValidation loss: 1.178430\n",
            "Epochs: 25 \tStep: 1920 \tTraining loss: 1.281451 \tValidation loss: 1.178567\n",
            "Epochs: 26 \tStep: 1930 \tTraining loss: 1.289332 \tValidation loss: 1.184615\n",
            "Epochs: 26 \tStep: 1940 \tTraining loss: 1.282069 \tValidation loss: 1.179007\n",
            "Epochs: 26 \tStep: 1950 \tTraining loss: 1.309090 \tValidation loss: 1.177379\n",
            "Epochs: 26 \tStep: 1960 \tTraining loss: 1.304858 \tValidation loss: 1.174152\n",
            "Epochs: 26 \tStep: 1970 \tTraining loss: 1.288562 \tValidation loss: 1.176280\n",
            "Epochs: 26 \tStep: 1980 \tTraining loss: 1.265394 \tValidation loss: 1.172563\n",
            "Epochs: 26 \tStep: 1990 \tTraining loss: 1.289104 \tValidation loss: 1.169455\n",
            "Epochs: 26 \tStep: 2000 \tTraining loss: 1.252353 \tValidation loss: 1.171897\n",
            "Epochs: 27 \tStep: 2010 \tTraining loss: 1.275129 \tValidation loss: 1.172277\n",
            "Epochs: 27 \tStep: 2020 \tTraining loss: 1.277712 \tValidation loss: 1.167740\n",
            "Epochs: 27 \tStep: 2030 \tTraining loss: 1.268984 \tValidation loss: 1.167744\n",
            "Epochs: 27 \tStep: 2040 \tTraining loss: 1.273506 \tValidation loss: 1.165825\n",
            "Epochs: 27 \tStep: 2050 \tTraining loss: 1.285187 \tValidation loss: 1.163374\n",
            "Epochs: 27 \tStep: 2060 \tTraining loss: 1.236785 \tValidation loss: 1.162872\n",
            "Epochs: 27 \tStep: 2070 \tTraining loss: 1.248545 \tValidation loss: 1.162346\n",
            "Epochs: 28 \tStep: 2080 \tTraining loss: 1.292542 \tValidation loss: 1.162493\n",
            "Epochs: 28 \tStep: 2090 \tTraining loss: 1.269641 \tValidation loss: 1.161857\n",
            "Epochs: 28 \tStep: 2100 \tTraining loss: 1.233654 \tValidation loss: 1.158812\n",
            "Epochs: 28 \tStep: 2110 \tTraining loss: 1.290175 \tValidation loss: 1.156787\n",
            "Epochs: 28 \tStep: 2120 \tTraining loss: 1.248709 \tValidation loss: 1.155666\n",
            "Epochs: 28 \tStep: 2130 \tTraining loss: 1.241984 \tValidation loss: 1.157778\n",
            "Epochs: 28 \tStep: 2140 \tTraining loss: 1.245121 \tValidation loss: 1.155411\n",
            "Epochs: 28 \tStep: 2150 \tTraining loss: 1.246063 \tValidation loss: 1.155333\n",
            "Epochs: 29 \tStep: 2160 \tTraining loss: 1.255286 \tValidation loss: 1.154115\n",
            "Epochs: 29 \tStep: 2170 \tTraining loss: 1.262687 \tValidation loss: 1.152777\n",
            "Epochs: 29 \tStep: 2180 \tTraining loss: 1.264352 \tValidation loss: 1.151303\n",
            "Epochs: 29 \tStep: 2190 \tTraining loss: 1.280261 \tValidation loss: 1.150284\n",
            "Epochs: 29 \tStep: 2200 \tTraining loss: 1.237634 \tValidation loss: 1.148620\n",
            "Epochs: 29 \tStep: 2210 \tTraining loss: 1.243119 \tValidation loss: 1.147741\n",
            "Epochs: 29 \tStep: 2220 \tTraining loss: 1.230950 \tValidation loss: 1.145591\n",
            "Epochs: 29 \tStep: 2230 \tTraining loss: 1.239972 \tValidation loss: 1.144674\n",
            "Epochs: 30 \tStep: 2240 \tTraining loss: 1.259267 \tValidation loss: 1.147095\n",
            "Epochs: 30 \tStep: 2250 \tTraining loss: 1.228094 \tValidation loss: 1.142588\n",
            "Epochs: 30 \tStep: 2260 \tTraining loss: 1.244075 \tValidation loss: 1.144229\n",
            "Epochs: 30 \tStep: 2270 \tTraining loss: 1.254002 \tValidation loss: 1.142156\n",
            "Epochs: 30 \tStep: 2280 \tTraining loss: 1.234871 \tValidation loss: 1.139023\n",
            "Epochs: 30 \tStep: 2290 \tTraining loss: 1.215659 \tValidation loss: 1.137662\n",
            "Epochs: 30 \tStep: 2300 \tTraining loss: 1.215441 \tValidation loss: 1.138100\n",
            "Epochs: 30 \tStep: 2310 \tTraining loss: 1.252699 \tValidation loss: 1.137802\n",
            "Epochs: 31 \tStep: 2320 \tTraining loss: 1.253841 \tValidation loss: 1.138279\n",
            "Epochs: 31 \tStep: 2330 \tTraining loss: 1.229849 \tValidation loss: 1.137014\n",
            "Epochs: 31 \tStep: 2340 \tTraining loss: 1.213460 \tValidation loss: 1.133993\n",
            "Epochs: 31 \tStep: 2350 \tTraining loss: 1.256070 \tValidation loss: 1.136183\n",
            "Epochs: 31 \tStep: 2360 \tTraining loss: 1.214843 \tValidation loss: 1.132276\n",
            "Epochs: 31 \tStep: 2370 \tTraining loss: 1.239845 \tValidation loss: 1.133605\n",
            "Epochs: 31 \tStep: 2380 \tTraining loss: 1.233987 \tValidation loss: 1.130455\n",
            "Epochs: 32 \tStep: 2390 \tTraining loss: 1.202279 \tValidation loss: 1.132744\n",
            "Epochs: 32 \tStep: 2400 \tTraining loss: 1.215768 \tValidation loss: 1.131410\n",
            "Epochs: 32 \tStep: 2410 \tTraining loss: 1.223338 \tValidation loss: 1.129468\n",
            "Epochs: 32 \tStep: 2420 \tTraining loss: 1.230460 \tValidation loss: 1.128990\n",
            "Epochs: 32 \tStep: 2430 \tTraining loss: 1.211833 \tValidation loss: 1.125448\n",
            "Epochs: 32 \tStep: 2440 \tTraining loss: 1.218390 \tValidation loss: 1.130317\n",
            "Epochs: 32 \tStep: 2450 \tTraining loss: 1.221755 \tValidation loss: 1.122992\n",
            "Epochs: 32 \tStep: 2460 \tTraining loss: 1.228602 \tValidation loss: 1.123355\n",
            "Epochs: 33 \tStep: 2470 \tTraining loss: 1.226365 \tValidation loss: 1.122279\n",
            "Epochs: 33 \tStep: 2480 \tTraining loss: 1.219647 \tValidation loss: 1.122993\n",
            "Epochs: 33 \tStep: 2490 \tTraining loss: 1.258029 \tValidation loss: 1.125415\n",
            "Epochs: 33 \tStep: 2500 \tTraining loss: 1.230918 \tValidation loss: 1.120769\n",
            "Epochs: 33 \tStep: 2510 \tTraining loss: 1.217699 \tValidation loss: 1.121316\n",
            "Epochs: 33 \tStep: 2520 \tTraining loss: 1.218926 \tValidation loss: 1.117954\n",
            "Epochs: 33 \tStep: 2530 \tTraining loss: 1.217277 \tValidation loss: 1.115707\n",
            "Epochs: 33 \tStep: 2540 \tTraining loss: 1.204568 \tValidation loss: 1.116364\n",
            "Epochs: 34 \tStep: 2550 \tTraining loss: 1.239339 \tValidation loss: 1.116408\n",
            "Epochs: 34 \tStep: 2560 \tTraining loss: 1.194629 \tValidation loss: 1.114978\n",
            "Epochs: 34 \tStep: 2570 \tTraining loss: 1.214405 \tValidation loss: 1.114792\n",
            "Epochs: 34 \tStep: 2580 \tTraining loss: 1.218480 \tValidation loss: 1.115006\n",
            "Epochs: 34 \tStep: 2590 \tTraining loss: 1.208616 \tValidation loss: 1.113937\n",
            "Epochs: 34 \tStep: 2600 \tTraining loss: 1.198259 \tValidation loss: 1.111258\n",
            "Epochs: 34 \tStep: 2610 \tTraining loss: 1.206998 \tValidation loss: 1.110273\n",
            "Epochs: 35 \tStep: 2620 \tTraining loss: 1.203502 \tValidation loss: 1.111502\n",
            "Epochs: 35 \tStep: 2630 \tTraining loss: 1.227359 \tValidation loss: 1.108785\n",
            "Epochs: 35 \tStep: 2640 \tTraining loss: 1.187878 \tValidation loss: 1.107925\n",
            "Epochs: 35 \tStep: 2650 \tTraining loss: 1.215420 \tValidation loss: 1.105245\n",
            "Epochs: 35 \tStep: 2660 \tTraining loss: 1.192065 \tValidation loss: 1.104433\n",
            "Epochs: 35 \tStep: 2670 \tTraining loss: 1.209699 \tValidation loss: 1.105231\n",
            "Epochs: 35 \tStep: 2680 \tTraining loss: 1.188849 \tValidation loss: 1.105811\n",
            "Epochs: 35 \tStep: 2690 \tTraining loss: 1.190142 \tValidation loss: 1.105602\n",
            "Epochs: 36 \tStep: 2700 \tTraining loss: 1.202397 \tValidation loss: 1.103746\n",
            "Epochs: 36 \tStep: 2710 \tTraining loss: 1.203744 \tValidation loss: 1.102032\n",
            "Epochs: 36 \tStep: 2720 \tTraining loss: 1.234000 \tValidation loss: 1.101635\n",
            "Epochs: 36 \tStep: 2730 \tTraining loss: 1.218777 \tValidation loss: 1.100596\n",
            "Epochs: 36 \tStep: 2740 \tTraining loss: 1.203256 \tValidation loss: 1.098631\n",
            "Epochs: 36 \tStep: 2750 \tTraining loss: 1.179418 \tValidation loss: 1.098670\n",
            "Epochs: 36 \tStep: 2760 \tTraining loss: 1.207633 \tValidation loss: 1.096428\n",
            "Epochs: 36 \tStep: 2770 \tTraining loss: 1.174715 \tValidation loss: 1.095911\n",
            "Epochs: 37 \tStep: 2780 \tTraining loss: 1.196304 \tValidation loss: 1.095801\n",
            "Epochs: 37 \tStep: 2790 \tTraining loss: 1.201852 \tValidation loss: 1.095740\n",
            "Epochs: 37 \tStep: 2800 \tTraining loss: 1.197077 \tValidation loss: 1.097562\n",
            "Epochs: 37 \tStep: 2810 \tTraining loss: 1.192689 \tValidation loss: 1.094447\n",
            "Epochs: 37 \tStep: 2820 \tTraining loss: 1.210056 \tValidation loss: 1.092663\n",
            "Epochs: 37 \tStep: 2830 \tTraining loss: 1.165316 \tValidation loss: 1.091607\n",
            "Epochs: 37 \tStep: 2840 \tTraining loss: 1.172008 \tValidation loss: 1.091603\n",
            "Epochs: 38 \tStep: 2850 \tTraining loss: 1.214120 \tValidation loss: 1.091697\n",
            "Epochs: 38 \tStep: 2860 \tTraining loss: 1.198447 \tValidation loss: 1.091956\n",
            "Epochs: 38 \tStep: 2870 \tTraining loss: 1.164194 \tValidation loss: 1.090067\n",
            "Epochs: 38 \tStep: 2880 \tTraining loss: 1.219281 \tValidation loss: 1.087319\n",
            "Epochs: 38 \tStep: 2890 \tTraining loss: 1.178384 \tValidation loss: 1.090631\n",
            "Epochs: 38 \tStep: 2900 \tTraining loss: 1.171260 \tValidation loss: 1.088280\n",
            "Epochs: 38 \tStep: 2910 \tTraining loss: 1.176722 \tValidation loss: 1.088690\n",
            "Epochs: 38 \tStep: 2920 \tTraining loss: 1.177537 \tValidation loss: 1.086435\n",
            "Epochs: 39 \tStep: 2930 \tTraining loss: 1.177914 \tValidation loss: 1.088062\n",
            "Epochs: 39 \tStep: 2940 \tTraining loss: 1.186340 \tValidation loss: 1.085940\n",
            "Epochs: 39 \tStep: 2950 \tTraining loss: 1.190716 \tValidation loss: 1.084378\n",
            "Epochs: 39 \tStep: 2960 \tTraining loss: 1.208869 \tValidation loss: 1.083175\n",
            "Epochs: 39 \tStep: 2970 \tTraining loss: 1.164960 \tValidation loss: 1.081218\n",
            "Epochs: 39 \tStep: 2980 \tTraining loss: 1.178068 \tValidation loss: 1.085159\n",
            "Epochs: 39 \tStep: 2990 \tTraining loss: 1.170645 \tValidation loss: 1.081942\n",
            "Epochs: 39 \tStep: 3000 \tTraining loss: 1.180423 \tValidation loss: 1.082057\n",
            "Epochs: 40 \tStep: 3010 \tTraining loss: 1.192394 \tValidation loss: 1.080294\n",
            "Epochs: 40 \tStep: 3020 \tTraining loss: 1.161377 \tValidation loss: 1.080810\n",
            "Epochs: 40 \tStep: 3030 \tTraining loss: 1.179913 \tValidation loss: 1.081164\n",
            "Epochs: 40 \tStep: 3040 \tTraining loss: 1.190428 \tValidation loss: 1.079594\n",
            "Epochs: 40 \tStep: 3050 \tTraining loss: 1.175276 \tValidation loss: 1.077234\n",
            "Epochs: 40 \tStep: 3060 \tTraining loss: 1.153508 \tValidation loss: 1.077320\n",
            "Epochs: 40 \tStep: 3070 \tTraining loss: 1.153845 \tValidation loss: 1.076419\n",
            "Epochs: 40 \tStep: 3080 \tTraining loss: 1.197293 \tValidation loss: 1.075552\n",
            "Epochs: 41 \tStep: 3090 \tTraining loss: 1.191562 \tValidation loss: 1.075998\n",
            "Epochs: 41 \tStep: 3100 \tTraining loss: 1.162548 \tValidation loss: 1.073043\n",
            "Epochs: 41 \tStep: 3110 \tTraining loss: 1.153261 \tValidation loss: 1.076000\n",
            "Epochs: 41 \tStep: 3120 \tTraining loss: 1.183854 \tValidation loss: 1.072693\n",
            "Epochs: 41 \tStep: 3130 \tTraining loss: 1.150426 \tValidation loss: 1.072517\n",
            "Epochs: 41 \tStep: 3140 \tTraining loss: 1.170638 \tValidation loss: 1.071153\n",
            "Epochs: 41 \tStep: 3150 \tTraining loss: 1.180273 \tValidation loss: 1.069796\n",
            "Epochs: 42 \tStep: 3160 \tTraining loss: 1.142380 \tValidation loss: 1.073905\n",
            "Epochs: 42 \tStep: 3170 \tTraining loss: 1.155035 \tValidation loss: 1.070788\n",
            "Epochs: 42 \tStep: 3180 \tTraining loss: 1.155295 \tValidation loss: 1.068725\n",
            "Epochs: 42 \tStep: 3190 \tTraining loss: 1.162542 \tValidation loss: 1.065610\n",
            "Epochs: 42 \tStep: 3200 \tTraining loss: 1.152811 \tValidation loss: 1.068270\n",
            "Epochs: 42 \tStep: 3210 \tTraining loss: 1.160829 \tValidation loss: 1.068085\n",
            "Epochs: 42 \tStep: 3220 \tTraining loss: 1.161312 \tValidation loss: 1.067294\n",
            "Epochs: 42 \tStep: 3230 \tTraining loss: 1.166428 \tValidation loss: 1.065124\n",
            "Epochs: 43 \tStep: 3240 \tTraining loss: 1.169429 \tValidation loss: 1.068974\n",
            "Epochs: 43 \tStep: 3250 \tTraining loss: 1.160998 \tValidation loss: 1.066270\n",
            "Epochs: 43 \tStep: 3260 \tTraining loss: 1.188874 \tValidation loss: 1.065930\n",
            "Epochs: 43 \tStep: 3270 \tTraining loss: 1.171527 \tValidation loss: 1.062740\n",
            "Epochs: 43 \tStep: 3280 \tTraining loss: 1.167536 \tValidation loss: 1.064097\n",
            "Epochs: 43 \tStep: 3290 \tTraining loss: 1.152453 \tValidation loss: 1.063231\n",
            "Epochs: 43 \tStep: 3300 \tTraining loss: 1.162875 \tValidation loss: 1.062572\n",
            "Epochs: 43 \tStep: 3310 \tTraining loss: 1.149291 \tValidation loss: 1.061713\n",
            "Epochs: 44 \tStep: 3320 \tTraining loss: 1.184456 \tValidation loss: 1.061666\n",
            "Epochs: 44 \tStep: 3330 \tTraining loss: 1.135529 \tValidation loss: 1.061544\n",
            "Epochs: 44 \tStep: 3340 \tTraining loss: 1.163973 \tValidation loss: 1.061680\n",
            "Epochs: 44 \tStep: 3350 \tTraining loss: 1.164871 \tValidation loss: 1.060452\n",
            "Epochs: 44 \tStep: 3360 \tTraining loss: 1.152550 \tValidation loss: 1.058771\n",
            "Epochs: 44 \tStep: 3370 \tTraining loss: 1.150873 \tValidation loss: 1.058948\n",
            "Epochs: 44 \tStep: 3380 \tTraining loss: 1.148807 \tValidation loss: 1.056482\n",
            "Epochs: 45 \tStep: 3390 \tTraining loss: 1.151374 \tValidation loss: 1.058813\n",
            "Epochs: 45 \tStep: 3400 \tTraining loss: 1.179010 \tValidation loss: 1.058969\n",
            "Epochs: 45 \tStep: 3410 \tTraining loss: 1.133988 \tValidation loss: 1.053276\n",
            "Epochs: 45 \tStep: 3420 \tTraining loss: 1.168250 \tValidation loss: 1.052464\n",
            "Epochs: 45 \tStep: 3430 \tTraining loss: 1.135041 \tValidation loss: 1.053855\n",
            "Epochs: 45 \tStep: 3440 \tTraining loss: 1.160929 \tValidation loss: 1.054010\n",
            "Epochs: 45 \tStep: 3450 \tTraining loss: 1.133001 \tValidation loss: 1.053504\n",
            "Epochs: 45 \tStep: 3460 \tTraining loss: 1.145280 \tValidation loss: 1.051746\n",
            "Epochs: 46 \tStep: 3470 \tTraining loss: 1.148393 \tValidation loss: 1.054280\n",
            "Epochs: 46 \tStep: 3480 \tTraining loss: 1.152733 \tValidation loss: 1.054039\n",
            "Epochs: 46 \tStep: 3490 \tTraining loss: 1.176179 \tValidation loss: 1.049891\n",
            "Epochs: 46 \tStep: 3500 \tTraining loss: 1.171689 \tValidation loss: 1.048021\n",
            "Epochs: 46 \tStep: 3510 \tTraining loss: 1.152717 \tValidation loss: 1.047552\n",
            "Epochs: 46 \tStep: 3520 \tTraining loss: 1.130704 \tValidation loss: 1.049448\n",
            "Epochs: 46 \tStep: 3530 \tTraining loss: 1.158119 \tValidation loss: 1.048399\n",
            "Epochs: 46 \tStep: 3540 \tTraining loss: 1.125552 \tValidation loss: 1.048339\n",
            "Epochs: 47 \tStep: 3550 \tTraining loss: 1.139179 \tValidation loss: 1.047956\n",
            "Epochs: 47 \tStep: 3560 \tTraining loss: 1.148442 \tValidation loss: 1.050123\n",
            "Epochs: 47 \tStep: 3570 \tTraining loss: 1.151495 \tValidation loss: 1.050137\n",
            "Epochs: 47 \tStep: 3580 \tTraining loss: 1.143067 \tValidation loss: 1.044088\n",
            "Epochs: 47 \tStep: 3590 \tTraining loss: 1.158154 \tValidation loss: 1.043537\n",
            "Epochs: 47 \tStep: 3600 \tTraining loss: 1.111757 \tValidation loss: 1.042506\n",
            "Epochs: 47 \tStep: 3610 \tTraining loss: 1.119211 \tValidation loss: 1.043478\n",
            "Epochs: 48 \tStep: 3620 \tTraining loss: 1.168755 \tValidation loss: 1.045275\n",
            "Epochs: 48 \tStep: 3630 \tTraining loss: 1.150083 \tValidation loss: 1.045595\n",
            "Epochs: 48 \tStep: 3640 \tTraining loss: 1.118269 \tValidation loss: 1.043453\n",
            "Epochs: 48 \tStep: 3650 \tTraining loss: 1.169485 \tValidation loss: 1.039938\n",
            "Epochs: 48 \tStep: 3660 \tTraining loss: 1.129206 \tValidation loss: 1.039177\n",
            "Epochs: 48 \tStep: 3670 \tTraining loss: 1.118186 \tValidation loss: 1.039822\n",
            "Epochs: 48 \tStep: 3680 \tTraining loss: 1.133317 \tValidation loss: 1.038275\n",
            "Epochs: 48 \tStep: 3690 \tTraining loss: 1.126984 \tValidation loss: 1.037637\n",
            "Epochs: 49 \tStep: 3700 \tTraining loss: 1.131544 \tValidation loss: 1.040304\n",
            "Epochs: 49 \tStep: 3710 \tTraining loss: 1.139786 \tValidation loss: 1.037371\n",
            "Epochs: 49 \tStep: 3720 \tTraining loss: 1.139666 \tValidation loss: 1.033861\n",
            "Epochs: 49 \tStep: 3730 \tTraining loss: 1.155815 \tValidation loss: 1.032923\n",
            "Epochs: 49 \tStep: 3740 \tTraining loss: 1.119900 \tValidation loss: 1.034056\n",
            "Epochs: 49 \tStep: 3750 \tTraining loss: 1.121388 \tValidation loss: 1.036235\n",
            "Epochs: 49 \tStep: 3760 \tTraining loss: 1.128120 \tValidation loss: 1.035295\n",
            "Epochs: 49 \tStep: 3770 \tTraining loss: 1.124880 \tValidation loss: 1.034861\n",
            "Epochs: 50 \tStep: 3780 \tTraining loss: 1.141616 \tValidation loss: 1.036044\n",
            "Epochs: 50 \tStep: 3790 \tTraining loss: 1.114793 \tValidation loss: 1.035175\n",
            "Epochs: 50 \tStep: 3800 \tTraining loss: 1.132569 \tValidation loss: 1.034200\n",
            "Epochs: 50 \tStep: 3810 \tTraining loss: 1.141108 \tValidation loss: 1.032323\n",
            "Epochs: 50 \tStep: 3820 \tTraining loss: 1.118985 \tValidation loss: 1.030447\n",
            "Epochs: 50 \tStep: 3830 \tTraining loss: 1.106770 \tValidation loss: 1.030589\n",
            "Epochs: 50 \tStep: 3840 \tTraining loss: 1.109939 \tValidation loss: 1.029723\n",
            "Epochs: 50 \tStep: 3850 \tTraining loss: 1.155222 \tValidation loss: 1.031240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "YTGqB0O7gGho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "  \"\"\"Given an input character, returns the predicted next character and hidden state\"\"\"\n",
        "\n",
        "  x = np.array([[model.char_to_int[char]]])\n",
        "  x = one_hot_encode(x, len(model.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "  # output of the model\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  # character probabilities\n",
        "  p = F.softmax(out, dim=1).data \n",
        "\n",
        "  if train_on_gpu: p = p.cpu()\n",
        "\n",
        "  # get top charactors\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(model.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  # randomly select the probable next characters\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p = p/p.sum())\n",
        "\n",
        "  return model.int_to_char[char], h"
      ],
      "metadata": {
        "id": "ZDPcIBLy50WH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime=\"The\", top_k=None):\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  model.eval()\n",
        "  # run through the prime characters\n",
        "  chars = [ch for ch in prime]\n",
        "  h = model.initialize_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  for i in range(size):\n",
        "    char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "  return \"\".join(chars)"
      ],
      "metadata": {
        "id": "K0cSQ5KL50Y5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 2000, prime=\"Anna\", top_k=10))"
      ],
      "metadata": {
        "id": "oUBVQmOYO6is",
        "outputId": "0f55187a-73ef-4b65-8cdc-3cc213b23b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna had\n",
            "been still done; he wlated himself and going out of his\n",
            "heart.\n",
            "\n",
            "About his\n",
            "place in haste to take his head his caresses and smalleways, and the only\n",
            "mere more dividing his face\n",
            "with shooting,\n",
            "later of the same terms with it\n",
            "almost in the little principle to the pausen, who degined, for that with his land of tasting complete letter of the\n",
            "men after the lip was the probanch from half home and had\n",
            "been put a leg when one in which she was not\n",
            "sincere, and that it would be in his minute, and was in annoyons in the same\n",
            "warms mean to\n",
            "be seen that it was he did not asked off the considerity\n",
            "of his character, all the principles of the direction of which the while was all\n",
            "time to come about him, though he would be subject that she were coming again, but\n",
            "they lift. Sergey Ivanovitch had long been\n",
            "brought for the dishanges when there\n",
            "were painting\n",
            "for\n",
            "what he was so saying. So that if they was sevired from the\n",
            "peolle of the prince.... To act at the better of her son and his mace at the fact that his full heart, had both been burning\n",
            "and her hand, and hrew.\n",
            "The other work of the ladies in the sense of the wheels of the recalling something at land, she conscious she was for her, and she felt he looked at the ball, but the were answers, horestless of papers. The principle of which a\n",
            "little short have packed upon\n",
            "him, that she would never forgive sight for him.\n",
            "\n",
            "\"But hor hatne for,\" the princess asked\n",
            "spainly. He felt his proposibility who had had first feeling. He ceased to the mown he had sent what she was considered what he sat shooting and but he was already. She reassed to hinder that it was\n",
            "so inventering those\n",
            "definiteless, had been so there their\n",
            "mind, and he did not hope that the coan too. Levin was caused to came bit into her eliced by which he\n",
            "knew all the saudtes of some statedy room, and standing off the barghors. The day\n",
            "of the\n",
            "carriage had come no strive, but, then he crawsed with this careful. \"There's some\n",
            "piccust fine surpose,\" but an inquiry west in the conte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "model_name = \"char_rnn.model\"\n",
        "\n",
        "checkpoint = {\"n_hidden\": model.n_hidden,\n",
        "              \"n_layers\": model.n_layers,\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"tokens\": model.chars}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "1kHHTVxN50TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved model\n",
        "with open(\"char_rnn.model\", \"rb\") as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "# sample using trained model\n",
        "loaded = RNN(checkpoint['tokens'], n_hidden=checkpoint[\"n_hidden\"], n_layers=checkpoint[\"n_layers\"])\n",
        "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
        "print(sample(loaded, size = 2000, prime=\"And Levin said\", top_k=10))"
      ],
      "metadata": {
        "id": "z4zZJCN3PFHL",
        "outputId": "2b407f4e-e908-4f55-b391-b8c9e1832068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And Levin said this.\n",
            "\n",
            "\"Well, we'll\n",
            "dean of it.\"\n",
            "\n",
            "Alexey Alexandrovitch\n",
            "had not mentioned. At suches for some corridig he, and considering what was he did some to have been fancied for this at the child\n",
            "and in the whole\n",
            "crowd would say to herself his capital education on love of love. When the child before he began helpatilly, with a letter she had the rut that he had been hair that wished the sound of\n",
            "his capable, she sent to the\n",
            "detail at the sight of the bed had been fif all\n",
            "circles, and standing and coath up their starss on the\n",
            "big great children. And while\n",
            "it she was definitely, he saw that he thought it since the party\n",
            "thought that he had been to talk about, on the picture till\n",
            "he had not been in love with the matter as no day,\n",
            "and\n",
            "felt\n",
            "that the country\n",
            "means of all her hossess, of coldness. She heard\n",
            "what the men were stupid, the shriek of\n",
            "changal in the supporitions that\n",
            "always did to say anything a tring, woman she had chores. To definite her harry with words) of what she should be all his sense.\n",
            "\n",
            "Stepan Arkadyevitch was never seen everything too, and if was at home. While the princess are, and they were\n",
            "terrible to her sitting, and\n",
            "his son in the sick man showed to him that he could hear the lower, to announce that he could not carried her.\n",
            "\n",
            "\"Yes, I've\n",
            "forgotten. I'll think\n",
            "mitute.\n",
            "What did\n",
            "you begin to me so?, and I won't ask you. I know that.\"\n",
            "\n",
            "\"And about such perfection?\" he said, looking into\n",
            "a face of having getting out his hand, with the\n",
            "whole same ago the fingers in\n",
            "offucion he knew that he had annoyed the morning.\n",
            "\n",
            "\"How was she this position in which they'll be afraid to be mistressed. But a spokt of it. At the peasant had been doing which I don't care to cast her wife. He has gone with him,.... How are you talking you so?\"\n",
            "\n",
            "\"When a little more was one of\n",
            "her.\"\n",
            "\n",
            "\"Worse!\" she repeated to her\n",
            "as taking\n",
            "his hand, clanced his face in the feeling. \"If you heard her!\n",
            "What, sit down.\"\n",
            "\n",
            "\"What do his meading the fitningiy? If what half you have the painfesine!\" Levin lived\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwbIx2D6lLRF"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}