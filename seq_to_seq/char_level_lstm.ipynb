{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/seq_to_seq/char_level_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CITpOagOBv3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7dd3a6e5-d166-408e-b6ef-164a5bc563e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-183696e7-f6c8-4ef0-ab5d-4be1b79cc020\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-183696e7-f6c8-4ef0-ab5d-4be1b79cc020\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"shazzadraihan\",\"key\":\"da63bbe0f8dcb3bd7fb35034046ca758\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# upload kaggle API key from your local machine\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a kaggle dir, copy the API key to it\n",
        "# and make sure the file in only readable by yourself (chmod 600)\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IajRinOUFcEm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use API command to download the dataset\n",
        "!kaggle datasets download -d wanderdust/anna-karenina-book"
      ],
      "metadata": {
        "id": "BGlxw58uGJf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39b6d99-4e08-4738-eca9-af7b0e0bd343"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading anna-karenina-book.zip to /content\n",
            "\r  0% 0.00/739k [00:00<?, ?B/s]\n",
            "\r100% 739k/739k [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the dataset\n",
        "!unzip -qq anna-karenina-book.zip"
      ],
      "metadata": {
        "id": "QfxEg189G_4J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open text file and read in some data as text\n",
        "with open(\"/content/anna.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "metadata": {
        "id": "ruMs8t0lGOwW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d430f3b3-d3f1-4754-8e36-e22f77f5d115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M7s-hXEgM4tN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print(\"CUDA is not available\")\n",
        "else:\n",
        "  print(\"CUDA is available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "hmDInJ5mcHI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3641ecaf-6877-49ee-bb6b-26c4c0b52867"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the dataset"
      ],
      "metadata": {
        "id": "1XeTje0xHgu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "chars = tuple(set(text))\n",
        "# map each int to char\n",
        "int_to_char = dict(enumerate(chars))\n",
        "# map each char to int\n",
        "char_to_int = {ch:idx for idx, ch in int_to_char.items()}\n",
        "\n",
        "# encode \n",
        "encoded = np.array([char_to_int[ch] for ch in text])\n",
        "encoded[:100]"
      ],
      "metadata": {
        "id": "AuOH3bqVHRpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d8b955-ff91-458c-a856-4de17c0e729c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 67,  3, 10, 25, 37, 21, 82,  7, 20, 20, 20, 11,  3, 10, 10, 73,\n",
              "       82, 33,  3, 18, 50, 13, 50, 37, 51, 82,  3, 21, 37, 82,  3, 13, 13,\n",
              "       82,  3, 13, 50, 63, 37, 49, 82, 37, 52, 37, 21, 73, 82, 40, 60, 67,\n",
              "        3, 10, 10, 73, 82, 33,  3, 18, 50, 13, 73, 82, 50, 51, 82, 40, 60,\n",
              "       67,  3, 10, 10, 73, 82, 50, 60, 82, 50, 25, 51, 82, 31, 54, 60, 20,\n",
              "       54,  3, 73, 35, 20, 20,  8, 52, 37, 21, 73, 25, 67, 50, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "valid_size = 0.1\n",
        "\n",
        "valid_idx = int(len(encoded)*(1-valid_size))\n",
        "train_data, valid_data = encoded[:valid_idx], encoded[valid_idx:]"
      ],
      "metadata": {
        "id": "hZxzVyi6hpNA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # initialize the the encoded array with zeros\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # fill with ones where appropriate\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # reshape to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "Z_18UrpVQAjm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    total_batch_size = batch_size * seq_length\n",
        "    # total number of batches\n",
        "    n_batches = len(arr)//total_batch_size\n",
        "    \n",
        "    # keep enough characters to make full batches\n",
        "    arr = arr[:n_batches * total_batch_size]\n",
        "    # reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "  "
      ],
      "metadata": {
        "id": "XxElsQitRNph"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "vCVb7_7fcL7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden, n_layers, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.n_layers = n_layers \n",
        "    self.n_hidden = n_hidden \n",
        "    self.drop_prob = drop_prob\n",
        "    self.lr = lr \n",
        "\n",
        "    # create character dictionaries\n",
        "    self.chars = tokens \n",
        "    self.int_to_char = dict(enumerate(self.chars))\n",
        "    self.char_to_int = {ch:idx for idx, ch in self.int_to_char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out, hidden\n",
        "\n",
        "  def initialize_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    # initialize hidden state and cell state of LSTM with zeros (n_layers * batch_size * n_hidden)\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "metadata": {
        "id": "jx8zpIx8Re2E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 512 \n",
        "n_layers = 2 \n",
        "drop_prob=0.5\n",
        "lr=0.001\n",
        "\n",
        "model = RNN(chars, n_hidden, n_layers, drop_prob, lr)\n",
        "print(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "rxQgMBKE50NJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f974cf06-b68a-4c08-c14f-679009903228"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "8cgoevWpoyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every=10):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(model.chars)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # initialize the hidden state\n",
        "    h = model.initialize_hidden(batch_size)\n",
        "\n",
        "    for inputs, targets in get_batches(data, batch_size, seq_length):\n",
        "      counter += 1 \n",
        "      # one-hot encode the data\n",
        "      inputs = one_hot_encode(inputs, n_chars)\n",
        "      # make torch tensor\n",
        "      inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "      # move the tensors to the right device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      # create new variable for the hidden state to avoid backprop through the \n",
        "      # entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      # clear the gradients of all optimized variables\n",
        "      model.zero_grad()\n",
        "      # forward pass\n",
        "      output, h = model(inputs, h)\n",
        "      # calculate the loss\n",
        "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "      # backprob\n",
        "      loss.backward()\n",
        "      # prevent exploding gradients problem in rnn/lstm\n",
        "      nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # ------------ validate the model -----------------\n",
        "      if counter % print_every == 0:\n",
        "        # initialize the hidden state\n",
        "        valid_h = model.initialize_hidden(batch_size)\n",
        "\n",
        "        valid_losses = []\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for inputs, targets in get_batches(valid_data, batch_size, seq_length):\n",
        "          # one-hot encode the inputs\n",
        "          inputs = one_hot_encode(inputs, n_chars)\n",
        "          # make torch tensor\n",
        "          inputs, targets = torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "          # create new variable for the hidden state to avoid backprop through the \n",
        "          # entire training history \n",
        "          valid_h = tuple([each for each in valid_h])\n",
        "          # move the tensor to the right device\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          # forward pass\n",
        "          output, valid_h = model(inputs, valid_h)\n",
        "          # calculate the batch loss\n",
        "          valid_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "          valid_losses.append(valid_loss.item())\n",
        "\n",
        "        # reset to train mode\n",
        "        model.train()\n",
        "\n",
        "        print(\"Epochs: {} \\tStep: {} \\tTraining loss: {:.6f} \\tValidation loss: {:.6f}\".format(epoch+1, \n",
        "                                                                                               counter, \n",
        "                                                                                               loss.item(), \n",
        "                                                                                               np.mean(valid_losses)))"
      ],
      "metadata": {
        "id": "IsnPrdprRe5H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "lr=0.001\n",
        "clip = 5\n",
        "print_every=10\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, epochs, batch_size, seq_length, criterion, optimizer, lr, clip, print_every=10)"
      ],
      "metadata": {
        "id": "SjF7eho750QE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d114a2-6357-4679-fe60-f813a7b8206a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e911c20ef1d7>:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), clip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 \tStep: 10 \tTraining loss: 3.287554 \tValidation loss: 3.229743\n",
            "Epochs: 1 \tStep: 20 \tTraining loss: 3.173907 \tValidation loss: 3.140636\n",
            "Epochs: 1 \tStep: 30 \tTraining loss: 3.138572 \tValidation loss: 3.126788\n",
            "Epochs: 1 \tStep: 40 \tTraining loss: 3.125086 \tValidation loss: 3.118804\n",
            "Epochs: 1 \tStep: 50 \tTraining loss: 3.118336 \tValidation loss: 3.118044\n",
            "Epochs: 1 \tStep: 60 \tTraining loss: 3.109410 \tValidation loss: 3.114468\n",
            "Epochs: 1 \tStep: 70 \tTraining loss: 3.108942 \tValidation loss: 3.111491\n",
            "Epochs: 1 \tStep: 80 \tTraining loss: 3.127609 \tValidation loss: 3.102601\n",
            "Epochs: 1 \tStep: 90 \tTraining loss: 3.120541 \tValidation loss: 3.085549\n",
            "Epochs: 1 \tStep: 100 \tTraining loss: 3.059779 \tValidation loss: 3.044197\n",
            "Epochs: 1 \tStep: 110 \tTraining loss: 2.977266 \tValidation loss: 2.953002\n",
            "Epochs: 1 \tStep: 120 \tTraining loss: 2.881481 \tValidation loss: 2.843322\n",
            "Epochs: 1 \tStep: 130 \tTraining loss: 2.779883 \tValidation loss: 2.786189\n",
            "Epochs: 1 \tStep: 140 \tTraining loss: 2.687161 \tValidation loss: 2.643127\n",
            "Epochs: 1 \tStep: 150 \tTraining loss: 2.598953 \tValidation loss: 2.544995\n",
            "Epochs: 2 \tStep: 160 \tTraining loss: 2.530260 \tValidation loss: 2.490193\n",
            "Epochs: 2 \tStep: 170 \tTraining loss: 2.470708 \tValidation loss: 2.442862\n",
            "Epochs: 2 \tStep: 180 \tTraining loss: 2.436802 \tValidation loss: 2.403971\n",
            "Epochs: 2 \tStep: 190 \tTraining loss: 2.402543 \tValidation loss: 2.377790\n",
            "Epochs: 2 \tStep: 200 \tTraining loss: 2.377016 \tValidation loss: 2.347917\n",
            "Epochs: 2 \tStep: 210 \tTraining loss: 2.334687 \tValidation loss: 2.325680\n",
            "Epochs: 2 \tStep: 220 \tTraining loss: 2.309298 \tValidation loss: 2.290484\n",
            "Epochs: 2 \tStep: 230 \tTraining loss: 2.294806 \tValidation loss: 2.265855\n",
            "Epochs: 2 \tStep: 240 \tTraining loss: 2.249527 \tValidation loss: 2.238988\n",
            "Epochs: 2 \tStep: 250 \tTraining loss: 2.263874 \tValidation loss: 2.209068\n",
            "Epochs: 2 \tStep: 260 \tTraining loss: 2.202039 \tValidation loss: 2.173899\n",
            "Epochs: 2 \tStep: 270 \tTraining loss: 2.194218 \tValidation loss: 2.147093\n",
            "Epochs: 2 \tStep: 280 \tTraining loss: 2.167284 \tValidation loss: 2.123437\n",
            "Epochs: 2 \tStep: 290 \tTraining loss: 2.181890 \tValidation loss: 2.106787\n",
            "Epochs: 2 \tStep: 300 \tTraining loss: 2.152547 \tValidation loss: 2.083958\n",
            "Epochs: 2 \tStep: 310 \tTraining loss: 2.108178 \tValidation loss: 2.065753\n",
            "Epochs: 3 \tStep: 320 \tTraining loss: 2.065166 \tValidation loss: 2.043862\n",
            "Epochs: 3 \tStep: 330 \tTraining loss: 2.037711 \tValidation loss: 2.021435\n",
            "Epochs: 3 \tStep: 340 \tTraining loss: 2.060055 \tValidation loss: 2.006588\n",
            "Epochs: 3 \tStep: 350 \tTraining loss: 2.025235 \tValidation loss: 1.988718\n",
            "Epochs: 3 \tStep: 360 \tTraining loss: 2.041461 \tValidation loss: 1.970606\n",
            "Epochs: 3 \tStep: 370 \tTraining loss: 1.984046 \tValidation loss: 1.952269\n",
            "Epochs: 3 \tStep: 380 \tTraining loss: 2.016624 \tValidation loss: 1.935954\n",
            "Epochs: 3 \tStep: 390 \tTraining loss: 1.977937 \tValidation loss: 1.919569\n",
            "Epochs: 3 \tStep: 400 \tTraining loss: 1.960158 \tValidation loss: 1.912404\n",
            "Epochs: 3 \tStep: 410 \tTraining loss: 1.962933 \tValidation loss: 1.889828\n",
            "Epochs: 3 \tStep: 420 \tTraining loss: 1.953572 \tValidation loss: 1.871650\n",
            "Epochs: 3 \tStep: 430 \tTraining loss: 1.930400 \tValidation loss: 1.857796\n",
            "Epochs: 3 \tStep: 440 \tTraining loss: 1.905989 \tValidation loss: 1.845394\n",
            "Epochs: 3 \tStep: 450 \tTraining loss: 1.912779 \tValidation loss: 1.827343\n",
            "Epochs: 3 \tStep: 460 \tTraining loss: 1.900718 \tValidation loss: 1.816760\n",
            "Epochs: 4 \tStep: 470 \tTraining loss: 1.869178 \tValidation loss: 1.807507\n",
            "Epochs: 4 \tStep: 480 \tTraining loss: 1.834689 \tValidation loss: 1.793901\n",
            "Epochs: 4 \tStep: 490 \tTraining loss: 1.840729 \tValidation loss: 1.787730\n",
            "Epochs: 4 \tStep: 500 \tTraining loss: 1.850411 \tValidation loss: 1.771609\n",
            "Epochs: 4 \tStep: 510 \tTraining loss: 1.830667 \tValidation loss: 1.760374\n",
            "Epochs: 4 \tStep: 520 \tTraining loss: 1.812251 \tValidation loss: 1.748531\n",
            "Epochs: 4 \tStep: 530 \tTraining loss: 1.806873 \tValidation loss: 1.740050\n",
            "Epochs: 4 \tStep: 540 \tTraining loss: 1.799325 \tValidation loss: 1.726667\n",
            "Epochs: 4 \tStep: 550 \tTraining loss: 1.775585 \tValidation loss: 1.718828\n",
            "Epochs: 4 \tStep: 560 \tTraining loss: 1.790920 \tValidation loss: 1.707150\n",
            "Epochs: 4 \tStep: 570 \tTraining loss: 1.754312 \tValidation loss: 1.698739\n",
            "Epochs: 4 \tStep: 580 \tTraining loss: 1.747373 \tValidation loss: 1.691077\n",
            "Epochs: 4 \tStep: 590 \tTraining loss: 1.739916 \tValidation loss: 1.681536\n",
            "Epochs: 4 \tStep: 600 \tTraining loss: 1.759446 \tValidation loss: 1.668451\n",
            "Epochs: 4 \tStep: 610 \tTraining loss: 1.754537 \tValidation loss: 1.660934\n",
            "Epochs: 4 \tStep: 620 \tTraining loss: 1.744492 \tValidation loss: 1.656947\n",
            "Epochs: 5 \tStep: 630 \tTraining loss: 1.707670 \tValidation loss: 1.652464\n",
            "Epochs: 5 \tStep: 640 \tTraining loss: 1.691521 \tValidation loss: 1.639093\n",
            "Epochs: 5 \tStep: 650 \tTraining loss: 1.713381 \tValidation loss: 1.631910\n",
            "Epochs: 5 \tStep: 660 \tTraining loss: 1.690807 \tValidation loss: 1.624552\n",
            "Epochs: 5 \tStep: 670 \tTraining loss: 1.706458 \tValidation loss: 1.617208\n",
            "Epochs: 5 \tStep: 680 \tTraining loss: 1.656679 \tValidation loss: 1.609264\n",
            "Epochs: 5 \tStep: 690 \tTraining loss: 1.707484 \tValidation loss: 1.601962\n",
            "Epochs: 5 \tStep: 700 \tTraining loss: 1.668886 \tValidation loss: 1.595363\n",
            "Epochs: 5 \tStep: 710 \tTraining loss: 1.662799 \tValidation loss: 1.587706\n",
            "Epochs: 5 \tStep: 720 \tTraining loss: 1.676228 \tValidation loss: 1.586561\n",
            "Epochs: 5 \tStep: 730 \tTraining loss: 1.677622 \tValidation loss: 1.577319\n",
            "Epochs: 5 \tStep: 740 \tTraining loss: 1.658412 \tValidation loss: 1.568590\n",
            "Epochs: 5 \tStep: 750 \tTraining loss: 1.639467 \tValidation loss: 1.560351\n",
            "Epochs: 5 \tStep: 760 \tTraining loss: 1.654345 \tValidation loss: 1.558242\n",
            "Epochs: 5 \tStep: 770 \tTraining loss: 1.649654 \tValidation loss: 1.550653\n",
            "Epochs: 6 \tStep: 780 \tTraining loss: 1.608065 \tValidation loss: 1.545737\n",
            "Epochs: 6 \tStep: 790 \tTraining loss: 1.578992 \tValidation loss: 1.537297\n",
            "Epochs: 6 \tStep: 800 \tTraining loss: 1.613400 \tValidation loss: 1.537366\n",
            "Epochs: 6 \tStep: 810 \tTraining loss: 1.610142 \tValidation loss: 1.527691\n",
            "Epochs: 6 \tStep: 820 \tTraining loss: 1.597994 \tValidation loss: 1.522494\n",
            "Epochs: 6 \tStep: 830 \tTraining loss: 1.598419 \tValidation loss: 1.517484\n",
            "Epochs: 6 \tStep: 840 \tTraining loss: 1.596015 \tValidation loss: 1.511651\n",
            "Epochs: 6 \tStep: 850 \tTraining loss: 1.598283 \tValidation loss: 1.504787\n",
            "Epochs: 6 \tStep: 860 \tTraining loss: 1.571931 \tValidation loss: 1.502719\n",
            "Epochs: 6 \tStep: 870 \tTraining loss: 1.577389 \tValidation loss: 1.496396\n",
            "Epochs: 6 \tStep: 880 \tTraining loss: 1.552519 \tValidation loss: 1.490983\n",
            "Epochs: 6 \tStep: 890 \tTraining loss: 1.551357 \tValidation loss: 1.483664\n",
            "Epochs: 6 \tStep: 900 \tTraining loss: 1.554564 \tValidation loss: 1.482728\n",
            "Epochs: 6 \tStep: 910 \tTraining loss: 1.568741 \tValidation loss: 1.479187\n",
            "Epochs: 6 \tStep: 920 \tTraining loss: 1.581629 \tValidation loss: 1.472631\n",
            "Epochs: 6 \tStep: 930 \tTraining loss: 1.569296 \tValidation loss: 1.469630\n",
            "Epochs: 7 \tStep: 940 \tTraining loss: 1.542434 \tValidation loss: 1.462858\n",
            "Epochs: 7 \tStep: 950 \tTraining loss: 1.533020 \tValidation loss: 1.457275\n",
            "Epochs: 7 \tStep: 960 \tTraining loss: 1.552679 \tValidation loss: 1.453802\n",
            "Epochs: 7 \tStep: 970 \tTraining loss: 1.528504 \tValidation loss: 1.451559\n",
            "Epochs: 7 \tStep: 980 \tTraining loss: 1.543390 \tValidation loss: 1.448613\n",
            "Epochs: 7 \tStep: 990 \tTraining loss: 1.500817 \tValidation loss: 1.442667\n",
            "Epochs: 7 \tStep: 1000 \tTraining loss: 1.550389 \tValidation loss: 1.439102\n",
            "Epochs: 7 \tStep: 1010 \tTraining loss: 1.518065 \tValidation loss: 1.436068\n",
            "Epochs: 7 \tStep: 1020 \tTraining loss: 1.514212 \tValidation loss: 1.431141\n",
            "Epochs: 7 \tStep: 1030 \tTraining loss: 1.530672 \tValidation loss: 1.427642\n",
            "Epochs: 7 \tStep: 1040 \tTraining loss: 1.534233 \tValidation loss: 1.422636\n",
            "Epochs: 7 \tStep: 1050 \tTraining loss: 1.511950 \tValidation loss: 1.420517\n",
            "Epochs: 7 \tStep: 1060 \tTraining loss: 1.491745 \tValidation loss: 1.415916\n",
            "Epochs: 7 \tStep: 1070 \tTraining loss: 1.524078 \tValidation loss: 1.414914\n",
            "Epochs: 7 \tStep: 1080 \tTraining loss: 1.519397 \tValidation loss: 1.410593\n",
            "Epochs: 8 \tStep: 1090 \tTraining loss: 1.475386 \tValidation loss: 1.409931\n",
            "Epochs: 8 \tStep: 1100 \tTraining loss: 1.447891 \tValidation loss: 1.400496\n",
            "Epochs: 8 \tStep: 1110 \tTraining loss: 1.487906 \tValidation loss: 1.400473\n",
            "Epochs: 8 \tStep: 1120 \tTraining loss: 1.493004 \tValidation loss: 1.397517\n",
            "Epochs: 8 \tStep: 1130 \tTraining loss: 1.464031 \tValidation loss: 1.394149\n",
            "Epochs: 8 \tStep: 1140 \tTraining loss: 1.481443 \tValidation loss: 1.390354\n",
            "Epochs: 8 \tStep: 1150 \tTraining loss: 1.478687 \tValidation loss: 1.388267\n",
            "Epochs: 8 \tStep: 1160 \tTraining loss: 1.490450 \tValidation loss: 1.385046\n",
            "Epochs: 8 \tStep: 1170 \tTraining loss: 1.465783 \tValidation loss: 1.379621\n",
            "Epochs: 8 \tStep: 1180 \tTraining loss: 1.453241 \tValidation loss: 1.376430\n",
            "Epochs: 8 \tStep: 1190 \tTraining loss: 1.440367 \tValidation loss: 1.373130\n",
            "Epochs: 8 \tStep: 1200 \tTraining loss: 1.428219 \tValidation loss: 1.369828\n",
            "Epochs: 8 \tStep: 1210 \tTraining loss: 1.431668 \tValidation loss: 1.370994\n",
            "Epochs: 8 \tStep: 1220 \tTraining loss: 1.448325 \tValidation loss: 1.365779\n",
            "Epochs: 8 \tStep: 1230 \tTraining loss: 1.480279 \tValidation loss: 1.366111\n",
            "Epochs: 8 \tStep: 1240 \tTraining loss: 1.474096 \tValidation loss: 1.360790\n",
            "Epochs: 9 \tStep: 1250 \tTraining loss: 1.447803 \tValidation loss: 1.357951\n",
            "Epochs: 9 \tStep: 1260 \tTraining loss: 1.448470 \tValidation loss: 1.358070\n",
            "Epochs: 9 \tStep: 1270 \tTraining loss: 1.445418 \tValidation loss: 1.352621\n",
            "Epochs: 9 \tStep: 1280 \tTraining loss: 1.441541 \tValidation loss: 1.352909\n",
            "Epochs: 9 \tStep: 1290 \tTraining loss: 1.449593 \tValidation loss: 1.346053\n",
            "Epochs: 9 \tStep: 1300 \tTraining loss: 1.409895 \tValidation loss: 1.346637\n",
            "Epochs: 9 \tStep: 1310 \tTraining loss: 1.451882 \tValidation loss: 1.343900\n",
            "Epochs: 9 \tStep: 1320 \tTraining loss: 1.417143 \tValidation loss: 1.341441\n",
            "Epochs: 9 \tStep: 1330 \tTraining loss: 1.434787 \tValidation loss: 1.337960\n",
            "Epochs: 9 \tStep: 1340 \tTraining loss: 1.435360 \tValidation loss: 1.336447\n",
            "Epochs: 9 \tStep: 1350 \tTraining loss: 1.445469 \tValidation loss: 1.331305\n",
            "Epochs: 9 \tStep: 1360 \tTraining loss: 1.424184 \tValidation loss: 1.330397\n",
            "Epochs: 9 \tStep: 1370 \tTraining loss: 1.407182 \tValidation loss: 1.328537\n",
            "Epochs: 9 \tStep: 1380 \tTraining loss: 1.433023 \tValidation loss: 1.327649\n",
            "Epochs: 9 \tStep: 1390 \tTraining loss: 1.423718 \tValidation loss: 1.325684\n",
            "Epochs: 10 \tStep: 1400 \tTraining loss: 1.384575 \tValidation loss: 1.322124\n",
            "Epochs: 10 \tStep: 1410 \tTraining loss: 1.359445 \tValidation loss: 1.318008\n",
            "Epochs: 10 \tStep: 1420 \tTraining loss: 1.393974 \tValidation loss: 1.316529\n",
            "Epochs: 10 \tStep: 1430 \tTraining loss: 1.403158 \tValidation loss: 1.317391\n",
            "Epochs: 10 \tStep: 1440 \tTraining loss: 1.386153 \tValidation loss: 1.310433\n",
            "Epochs: 10 \tStep: 1450 \tTraining loss: 1.385949 \tValidation loss: 1.307035\n",
            "Epochs: 10 \tStep: 1460 \tTraining loss: 1.407569 \tValidation loss: 1.308458\n",
            "Epochs: 10 \tStep: 1470 \tTraining loss: 1.415428 \tValidation loss: 1.305469\n",
            "Epochs: 10 \tStep: 1480 \tTraining loss: 1.398869 \tValidation loss: 1.302848\n",
            "Epochs: 10 \tStep: 1490 \tTraining loss: 1.389378 \tValidation loss: 1.301570\n",
            "Epochs: 10 \tStep: 1500 \tTraining loss: 1.368188 \tValidation loss: 1.297799\n",
            "Epochs: 10 \tStep: 1510 \tTraining loss: 1.357541 \tValidation loss: 1.296568\n",
            "Epochs: 10 \tStep: 1520 \tTraining loss: 1.363732 \tValidation loss: 1.293954\n",
            "Epochs: 10 \tStep: 1530 \tTraining loss: 1.379942 \tValidation loss: 1.293127\n",
            "Epochs: 10 \tStep: 1540 \tTraining loss: 1.405920 \tValidation loss: 1.290180\n",
            "Epochs: 10 \tStep: 1550 \tTraining loss: 1.393425 \tValidation loss: 1.285651\n",
            "Epochs: 11 \tStep: 1560 \tTraining loss: 1.370589 \tValidation loss: 1.285183\n",
            "Epochs: 11 \tStep: 1570 \tTraining loss: 1.371063 \tValidation loss: 1.283120\n",
            "Epochs: 11 \tStep: 1580 \tTraining loss: 1.386653 \tValidation loss: 1.280855\n",
            "Epochs: 11 \tStep: 1590 \tTraining loss: 1.363747 \tValidation loss: 1.276541\n",
            "Epochs: 11 \tStep: 1600 \tTraining loss: 1.377749 \tValidation loss: 1.276701\n",
            "Epochs: 11 \tStep: 1610 \tTraining loss: 1.340534 \tValidation loss: 1.273721\n",
            "Epochs: 11 \tStep: 1620 \tTraining loss: 1.369772 \tValidation loss: 1.270848\n",
            "Epochs: 11 \tStep: 1630 \tTraining loss: 1.341890 \tValidation loss: 1.268292\n",
            "Epochs: 11 \tStep: 1640 \tTraining loss: 1.353837 \tValidation loss: 1.262035\n",
            "Epochs: 11 \tStep: 1650 \tTraining loss: 1.370558 \tValidation loss: 1.261573\n",
            "Epochs: 11 \tStep: 1660 \tTraining loss: 1.369142 \tValidation loss: 1.258413\n",
            "Epochs: 11 \tStep: 1670 \tTraining loss: 1.359869 \tValidation loss: 1.255823\n",
            "Epochs: 11 \tStep: 1680 \tTraining loss: 1.334481 \tValidation loss: 1.253799\n",
            "Epochs: 11 \tStep: 1690 \tTraining loss: 1.355146 \tValidation loss: 1.260406\n",
            "Epochs: 11 \tStep: 1700 \tTraining loss: 1.351182 \tValidation loss: 1.252232\n",
            "Epochs: 12 \tStep: 1710 \tTraining loss: 1.309280 \tValidation loss: 1.259379\n",
            "Epochs: 12 \tStep: 1720 \tTraining loss: 1.286835 \tValidation loss: 1.248439\n",
            "Epochs: 12 \tStep: 1730 \tTraining loss: 1.330629 \tValidation loss: 1.252349\n",
            "Epochs: 12 \tStep: 1740 \tTraining loss: 1.330832 \tValidation loss: 1.247495\n",
            "Epochs: 12 \tStep: 1750 \tTraining loss: 1.319836 \tValidation loss: 1.248275\n",
            "Epochs: 12 \tStep: 1760 \tTraining loss: 1.326663 \tValidation loss: 1.249595\n",
            "Epochs: 12 \tStep: 1770 \tTraining loss: 1.338106 \tValidation loss: 1.248373\n",
            "Epochs: 12 \tStep: 1780 \tTraining loss: 1.351454 \tValidation loss: 1.241459\n",
            "Epochs: 12 \tStep: 1790 \tTraining loss: 1.345020 \tValidation loss: 1.235328\n",
            "Epochs: 12 \tStep: 1800 \tTraining loss: 1.313352 \tValidation loss: 1.236122\n",
            "Epochs: 12 \tStep: 1810 \tTraining loss: 1.307252 \tValidation loss: 1.234458\n",
            "Epochs: 12 \tStep: 1820 \tTraining loss: 1.299165 \tValidation loss: 1.234831\n",
            "Epochs: 12 \tStep: 1830 \tTraining loss: 1.296867 \tValidation loss: 1.232993\n",
            "Epochs: 12 \tStep: 1840 \tTraining loss: 1.305959 \tValidation loss: 1.229471\n",
            "Epochs: 12 \tStep: 1850 \tTraining loss: 1.347596 \tValidation loss: 1.226362\n",
            "Epochs: 12 \tStep: 1860 \tTraining loss: 1.337491 \tValidation loss: 1.228759\n",
            "Epochs: 13 \tStep: 1870 \tTraining loss: 1.315969 \tValidation loss: 1.226491\n",
            "Epochs: 13 \tStep: 1880 \tTraining loss: 1.315400 \tValidation loss: 1.223253\n",
            "Epochs: 13 \tStep: 1890 \tTraining loss: 1.320863 \tValidation loss: 1.222895\n",
            "Epochs: 13 \tStep: 1900 \tTraining loss: 1.308168 \tValidation loss: 1.224598\n",
            "Epochs: 13 \tStep: 1910 \tTraining loss: 1.312552 \tValidation loss: 1.219601\n",
            "Epochs: 13 \tStep: 1920 \tTraining loss: 1.288310 \tValidation loss: 1.222776\n",
            "Epochs: 13 \tStep: 1930 \tTraining loss: 1.315172 \tValidation loss: 1.216513\n",
            "Epochs: 13 \tStep: 1940 \tTraining loss: 1.287778 \tValidation loss: 1.217458\n",
            "Epochs: 13 \tStep: 1950 \tTraining loss: 1.306757 \tValidation loss: 1.214738\n",
            "Epochs: 13 \tStep: 1960 \tTraining loss: 1.317835 \tValidation loss: 1.214175\n",
            "Epochs: 13 \tStep: 1970 \tTraining loss: 1.321783 \tValidation loss: 1.210893\n",
            "Epochs: 13 \tStep: 1980 \tTraining loss: 1.312666 \tValidation loss: 1.207736\n",
            "Epochs: 13 \tStep: 1990 \tTraining loss: 1.288662 \tValidation loss: 1.211411\n",
            "Epochs: 13 \tStep: 2000 \tTraining loss: 1.312992 \tValidation loss: 1.210218\n",
            "Epochs: 13 \tStep: 2010 \tTraining loss: 1.306910 \tValidation loss: 1.210893\n",
            "Epochs: 14 \tStep: 2020 \tTraining loss: 1.271326 \tValidation loss: 1.215533\n",
            "Epochs: 14 \tStep: 2030 \tTraining loss: 1.250195 \tValidation loss: 1.203795\n",
            "Epochs: 14 \tStep: 2040 \tTraining loss: 1.293623 \tValidation loss: 1.206379\n",
            "Epochs: 14 \tStep: 2050 \tTraining loss: 1.295046 \tValidation loss: 1.200953\n",
            "Epochs: 14 \tStep: 2060 \tTraining loss: 1.265193 \tValidation loss: 1.199106\n",
            "Epochs: 14 \tStep: 2070 \tTraining loss: 1.279228 \tValidation loss: 1.203174\n",
            "Epochs: 14 \tStep: 2080 \tTraining loss: 1.303420 \tValidation loss: 1.196745\n",
            "Epochs: 14 \tStep: 2090 \tTraining loss: 1.308265 \tValidation loss: 1.196695\n",
            "Epochs: 14 \tStep: 2100 \tTraining loss: 1.289721 \tValidation loss: 1.197185\n",
            "Epochs: 14 \tStep: 2110 \tTraining loss: 1.273946 \tValidation loss: 1.192104\n",
            "Epochs: 14 \tStep: 2120 \tTraining loss: 1.258548 \tValidation loss: 1.193493\n",
            "Epochs: 14 \tStep: 2130 \tTraining loss: 1.254829 \tValidation loss: 1.190757\n",
            "Epochs: 14 \tStep: 2140 \tTraining loss: 1.252653 \tValidation loss: 1.190928\n",
            "Epochs: 14 \tStep: 2150 \tTraining loss: 1.269815 \tValidation loss: 1.189139\n",
            "Epochs: 14 \tStep: 2160 \tTraining loss: 1.301014 \tValidation loss: 1.191293\n",
            "Epochs: 14 \tStep: 2170 \tTraining loss: 1.292787 \tValidation loss: 1.189571\n",
            "Epochs: 15 \tStep: 2180 \tTraining loss: 1.278221 \tValidation loss: 1.188923\n",
            "Epochs: 15 \tStep: 2190 \tTraining loss: 1.273968 \tValidation loss: 1.186253\n",
            "Epochs: 15 \tStep: 2200 \tTraining loss: 1.286262 \tValidation loss: 1.183964\n",
            "Epochs: 15 \tStep: 2210 \tTraining loss: 1.275079 \tValidation loss: 1.182185\n",
            "Epochs: 15 \tStep: 2220 \tTraining loss: 1.275982 \tValidation loss: 1.181830\n",
            "Epochs: 15 \tStep: 2230 \tTraining loss: 1.245260 \tValidation loss: 1.180227\n",
            "Epochs: 15 \tStep: 2240 \tTraining loss: 1.280628 \tValidation loss: 1.176562\n",
            "Epochs: 15 \tStep: 2250 \tTraining loss: 1.251509 \tValidation loss: 1.181303\n",
            "Epochs: 15 \tStep: 2260 \tTraining loss: 1.261987 \tValidation loss: 1.176734\n",
            "Epochs: 15 \tStep: 2270 \tTraining loss: 1.283299 \tValidation loss: 1.175679\n",
            "Epochs: 15 \tStep: 2280 \tTraining loss: 1.284867 \tValidation loss: 1.176078\n",
            "Epochs: 15 \tStep: 2290 \tTraining loss: 1.267654 \tValidation loss: 1.176577\n",
            "Epochs: 15 \tStep: 2300 \tTraining loss: 1.249403 \tValidation loss: 1.174713\n",
            "Epochs: 15 \tStep: 2310 \tTraining loss: 1.271655 \tValidation loss: 1.175488\n",
            "Epochs: 15 \tStep: 2320 \tTraining loss: 1.264313 \tValidation loss: 1.174656\n",
            "Epochs: 16 \tStep: 2330 \tTraining loss: 1.234534 \tValidation loss: 1.181187\n",
            "Epochs: 16 \tStep: 2340 \tTraining loss: 1.216111 \tValidation loss: 1.174776\n",
            "Epochs: 16 \tStep: 2350 \tTraining loss: 1.255460 \tValidation loss: 1.169469\n",
            "Epochs: 16 \tStep: 2360 \tTraining loss: 1.259930 \tValidation loss: 1.168865\n",
            "Epochs: 16 \tStep: 2370 \tTraining loss: 1.229670 \tValidation loss: 1.165683\n",
            "Epochs: 16 \tStep: 2380 \tTraining loss: 1.241824 \tValidation loss: 1.164948\n",
            "Epochs: 16 \tStep: 2390 \tTraining loss: 1.267669 \tValidation loss: 1.163861\n",
            "Epochs: 16 \tStep: 2400 \tTraining loss: 1.267369 \tValidation loss: 1.162024\n",
            "Epochs: 16 \tStep: 2410 \tTraining loss: 1.259838 \tValidation loss: 1.161714\n",
            "Epochs: 16 \tStep: 2420 \tTraining loss: 1.252622 \tValidation loss: 1.161621\n",
            "Epochs: 16 \tStep: 2430 \tTraining loss: 1.231635 \tValidation loss: 1.164325\n",
            "Epochs: 16 \tStep: 2440 \tTraining loss: 1.219517 \tValidation loss: 1.159410\n",
            "Epochs: 16 \tStep: 2450 \tTraining loss: 1.215532 \tValidation loss: 1.162408\n",
            "Epochs: 16 \tStep: 2460 \tTraining loss: 1.225699 \tValidation loss: 1.160721\n",
            "Epochs: 16 \tStep: 2470 \tTraining loss: 1.265203 \tValidation loss: 1.160675\n",
            "Epochs: 16 \tStep: 2480 \tTraining loss: 1.270667 \tValidation loss: 1.158936\n",
            "Epochs: 17 \tStep: 2490 \tTraining loss: 1.237880 \tValidation loss: 1.159070\n",
            "Epochs: 17 \tStep: 2500 \tTraining loss: 1.249158 \tValidation loss: 1.156133\n",
            "Epochs: 17 \tStep: 2510 \tTraining loss: 1.245163 \tValidation loss: 1.154827\n",
            "Epochs: 17 \tStep: 2520 \tTraining loss: 1.240737 \tValidation loss: 1.153712\n",
            "Epochs: 17 \tStep: 2530 \tTraining loss: 1.245232 \tValidation loss: 1.150909\n",
            "Epochs: 17 \tStep: 2540 \tTraining loss: 1.212648 \tValidation loss: 1.151356\n",
            "Epochs: 17 \tStep: 2550 \tTraining loss: 1.239331 \tValidation loss: 1.150801\n",
            "Epochs: 17 \tStep: 2560 \tTraining loss: 1.222034 \tValidation loss: 1.148487\n",
            "Epochs: 17 \tStep: 2570 \tTraining loss: 1.229108 \tValidation loss: 1.148414\n",
            "Epochs: 17 \tStep: 2580 \tTraining loss: 1.249101 \tValidation loss: 1.147149\n",
            "Epochs: 17 \tStep: 2590 \tTraining loss: 1.252727 \tValidation loss: 1.146913\n",
            "Epochs: 17 \tStep: 2600 \tTraining loss: 1.241474 \tValidation loss: 1.146062\n",
            "Epochs: 17 \tStep: 2610 \tTraining loss: 1.224843 \tValidation loss: 1.146779\n",
            "Epochs: 17 \tStep: 2620 \tTraining loss: 1.248822 \tValidation loss: 1.143431\n",
            "Epochs: 17 \tStep: 2630 \tTraining loss: 1.232523 \tValidation loss: 1.144022\n",
            "Epochs: 18 \tStep: 2640 \tTraining loss: 1.199272 \tValidation loss: 1.150112\n",
            "Epochs: 18 \tStep: 2650 \tTraining loss: 1.183038 \tValidation loss: 1.146963\n",
            "Epochs: 18 \tStep: 2660 \tTraining loss: 1.221940 \tValidation loss: 1.142941\n",
            "Epochs: 18 \tStep: 2670 \tTraining loss: 1.234110 \tValidation loss: 1.140680\n",
            "Epochs: 18 \tStep: 2680 \tTraining loss: 1.206621 \tValidation loss: 1.136789\n",
            "Epochs: 18 \tStep: 2690 \tTraining loss: 1.218878 \tValidation loss: 1.141140\n",
            "Epochs: 18 \tStep: 2700 \tTraining loss: 1.244832 \tValidation loss: 1.137582\n",
            "Epochs: 18 \tStep: 2710 \tTraining loss: 1.238030 \tValidation loss: 1.136345\n",
            "Epochs: 18 \tStep: 2720 \tTraining loss: 1.236074 \tValidation loss: 1.134059\n",
            "Epochs: 18 \tStep: 2730 \tTraining loss: 1.221347 \tValidation loss: 1.134136\n",
            "Epochs: 18 \tStep: 2740 \tTraining loss: 1.199115 \tValidation loss: 1.134359\n",
            "Epochs: 18 \tStep: 2750 \tTraining loss: 1.190590 \tValidation loss: 1.136569\n",
            "Epochs: 18 \tStep: 2760 \tTraining loss: 1.193921 \tValidation loss: 1.135260\n",
            "Epochs: 18 \tStep: 2770 \tTraining loss: 1.212336 \tValidation loss: 1.134177\n",
            "Epochs: 18 \tStep: 2780 \tTraining loss: 1.242941 \tValidation loss: 1.136601\n",
            "Epochs: 18 \tStep: 2790 \tTraining loss: 1.244198 \tValidation loss: 1.133085\n",
            "Epochs: 19 \tStep: 2800 \tTraining loss: 1.207838 \tValidation loss: 1.133668\n",
            "Epochs: 19 \tStep: 2810 \tTraining loss: 1.215210 \tValidation loss: 1.133157\n",
            "Epochs: 19 \tStep: 2820 \tTraining loss: 1.220844 \tValidation loss: 1.130040\n",
            "Epochs: 19 \tStep: 2830 \tTraining loss: 1.211617 \tValidation loss: 1.129202\n",
            "Epochs: 19 \tStep: 2840 \tTraining loss: 1.223541 \tValidation loss: 1.126673\n",
            "Epochs: 19 \tStep: 2850 \tTraining loss: 1.189638 \tValidation loss: 1.126947\n",
            "Epochs: 19 \tStep: 2860 \tTraining loss: 1.225735 \tValidation loss: 1.126969\n",
            "Epochs: 19 \tStep: 2870 \tTraining loss: 1.188551 \tValidation loss: 1.124207\n",
            "Epochs: 19 \tStep: 2880 \tTraining loss: 1.199654 \tValidation loss: 1.123565\n",
            "Epochs: 19 \tStep: 2890 \tTraining loss: 1.223968 \tValidation loss: 1.122230\n",
            "Epochs: 19 \tStep: 2900 \tTraining loss: 1.223506 \tValidation loss: 1.123510\n",
            "Epochs: 19 \tStep: 2910 \tTraining loss: 1.215038 \tValidation loss: 1.122816\n",
            "Epochs: 19 \tStep: 2920 \tTraining loss: 1.200771 \tValidation loss: 1.121388\n",
            "Epochs: 19 \tStep: 2930 \tTraining loss: 1.221093 \tValidation loss: 1.121328\n",
            "Epochs: 19 \tStep: 2940 \tTraining loss: 1.207453 \tValidation loss: 1.119899\n",
            "Epochs: 20 \tStep: 2950 \tTraining loss: 1.179203 \tValidation loss: 1.123821\n",
            "Epochs: 20 \tStep: 2960 \tTraining loss: 1.160420 \tValidation loss: 1.125135\n",
            "Epochs: 20 \tStep: 2970 \tTraining loss: 1.200187 \tValidation loss: 1.121622\n",
            "Epochs: 20 \tStep: 2980 \tTraining loss: 1.208330 \tValidation loss: 1.118791\n",
            "Epochs: 20 \tStep: 2990 \tTraining loss: 1.187321 \tValidation loss: 1.119650\n",
            "Epochs: 20 \tStep: 3000 \tTraining loss: 1.196680 \tValidation loss: 1.116683\n",
            "Epochs: 20 \tStep: 3010 \tTraining loss: 1.213759 \tValidation loss: 1.116201\n",
            "Epochs: 20 \tStep: 3020 \tTraining loss: 1.214494 \tValidation loss: 1.115374\n",
            "Epochs: 20 \tStep: 3030 \tTraining loss: 1.205357 \tValidation loss: 1.112511\n",
            "Epochs: 20 \tStep: 3040 \tTraining loss: 1.197625 \tValidation loss: 1.113699\n",
            "Epochs: 20 \tStep: 3050 \tTraining loss: 1.177383 \tValidation loss: 1.113341\n",
            "Epochs: 20 \tStep: 3060 \tTraining loss: 1.161062 \tValidation loss: 1.111309\n",
            "Epochs: 20 \tStep: 3070 \tTraining loss: 1.171640 \tValidation loss: 1.113225\n",
            "Epochs: 20 \tStep: 3080 \tTraining loss: 1.181556 \tValidation loss: 1.113658\n",
            "Epochs: 20 \tStep: 3090 \tTraining loss: 1.213982 \tValidation loss: 1.114512\n",
            "Epochs: 20 \tStep: 3100 \tTraining loss: 1.232443 \tValidation loss: 1.112831\n",
            "Epochs: 21 \tStep: 3110 \tTraining loss: 1.196392 \tValidation loss: 1.113191\n",
            "Epochs: 21 \tStep: 3120 \tTraining loss: 1.194679 \tValidation loss: 1.112124\n",
            "Epochs: 21 \tStep: 3130 \tTraining loss: 1.203053 \tValidation loss: 1.108656\n",
            "Epochs: 21 \tStep: 3140 \tTraining loss: 1.193110 \tValidation loss: 1.109236\n",
            "Epochs: 21 \tStep: 3150 \tTraining loss: 1.203039 \tValidation loss: 1.109858\n",
            "Epochs: 21 \tStep: 3160 \tTraining loss: 1.169810 \tValidation loss: 1.105898\n",
            "Epochs: 21 \tStep: 3170 \tTraining loss: 1.204540 \tValidation loss: 1.108975\n",
            "Epochs: 21 \tStep: 3180 \tTraining loss: 1.170927 \tValidation loss: 1.104441\n",
            "Epochs: 21 \tStep: 3190 \tTraining loss: 1.185230 \tValidation loss: 1.105913\n",
            "Epochs: 21 \tStep: 3200 \tTraining loss: 1.195747 \tValidation loss: 1.101525\n",
            "Epochs: 21 \tStep: 3210 \tTraining loss: 1.205626 \tValidation loss: 1.102864\n",
            "Epochs: 21 \tStep: 3220 \tTraining loss: 1.196007 \tValidation loss: 1.102632\n",
            "Epochs: 21 \tStep: 3230 \tTraining loss: 1.175605 \tValidation loss: 1.102034\n",
            "Epochs: 21 \tStep: 3240 \tTraining loss: 1.198039 \tValidation loss: 1.100251\n",
            "Epochs: 21 \tStep: 3250 \tTraining loss: 1.183656 \tValidation loss: 1.099364\n",
            "Epochs: 22 \tStep: 3260 \tTraining loss: 1.150301 \tValidation loss: 1.104653\n",
            "Epochs: 22 \tStep: 3270 \tTraining loss: 1.143453 \tValidation loss: 1.105309\n",
            "Epochs: 22 \tStep: 3280 \tTraining loss: 1.177071 \tValidation loss: 1.102096\n",
            "Epochs: 22 \tStep: 3290 \tTraining loss: 1.185509 \tValidation loss: 1.100103\n",
            "Epochs: 22 \tStep: 3300 \tTraining loss: 1.159991 \tValidation loss: 1.097455\n",
            "Epochs: 22 \tStep: 3310 \tTraining loss: 1.181424 \tValidation loss: 1.100183\n",
            "Epochs: 22 \tStep: 3320 \tTraining loss: 1.199988 \tValidation loss: 1.095494\n",
            "Epochs: 22 \tStep: 3330 \tTraining loss: 1.194465 \tValidation loss: 1.095286\n",
            "Epochs: 22 \tStep: 3340 \tTraining loss: 1.182860 \tValidation loss: 1.093877\n",
            "Epochs: 22 \tStep: 3350 \tTraining loss: 1.172158 \tValidation loss: 1.095054\n",
            "Epochs: 22 \tStep: 3360 \tTraining loss: 1.159988 \tValidation loss: 1.093912\n",
            "Epochs: 22 \tStep: 3370 \tTraining loss: 1.150203 \tValidation loss: 1.092316\n",
            "Epochs: 22 \tStep: 3380 \tTraining loss: 1.154768 \tValidation loss: 1.097754\n",
            "Epochs: 22 \tStep: 3390 \tTraining loss: 1.165576 \tValidation loss: 1.092382\n",
            "Epochs: 22 \tStep: 3400 \tTraining loss: 1.199921 \tValidation loss: 1.095097\n",
            "Epochs: 22 \tStep: 3410 \tTraining loss: 1.209054 \tValidation loss: 1.093268\n",
            "Epochs: 23 \tStep: 3420 \tTraining loss: 1.181129 \tValidation loss: 1.094495\n",
            "Epochs: 23 \tStep: 3430 \tTraining loss: 1.187617 \tValidation loss: 1.094619\n",
            "Epochs: 23 \tStep: 3440 \tTraining loss: 1.187748 \tValidation loss: 1.088700\n",
            "Epochs: 23 \tStep: 3450 \tTraining loss: 1.174285 \tValidation loss: 1.090916\n",
            "Epochs: 23 \tStep: 3460 \tTraining loss: 1.187415 \tValidation loss: 1.086788\n",
            "Epochs: 23 \tStep: 3470 \tTraining loss: 1.140458 \tValidation loss: 1.087714\n",
            "Epochs: 23 \tStep: 3480 \tTraining loss: 1.175766 \tValidation loss: 1.087518\n",
            "Epochs: 23 \tStep: 3490 \tTraining loss: 1.150504 \tValidation loss: 1.087236\n",
            "Epochs: 23 \tStep: 3500 \tTraining loss: 1.172517 \tValidation loss: 1.086222\n",
            "Epochs: 23 \tStep: 3510 \tTraining loss: 1.184040 \tValidation loss: 1.083284\n",
            "Epochs: 23 \tStep: 3520 \tTraining loss: 1.181883 \tValidation loss: 1.084101\n",
            "Epochs: 23 \tStep: 3530 \tTraining loss: 1.178053 \tValidation loss: 1.083451\n",
            "Epochs: 23 \tStep: 3540 \tTraining loss: 1.157073 \tValidation loss: 1.083517\n",
            "Epochs: 23 \tStep: 3550 \tTraining loss: 1.183202 \tValidation loss: 1.081188\n",
            "Epochs: 23 \tStep: 3560 \tTraining loss: 1.172279 \tValidation loss: 1.081053\n",
            "Epochs: 24 \tStep: 3570 \tTraining loss: 1.148299 \tValidation loss: 1.089631\n",
            "Epochs: 24 \tStep: 3580 \tTraining loss: 1.125409 \tValidation loss: 1.086682\n",
            "Epochs: 24 \tStep: 3590 \tTraining loss: 1.169231 \tValidation loss: 1.086142\n",
            "Epochs: 24 \tStep: 3600 \tTraining loss: 1.175367 \tValidation loss: 1.081303\n",
            "Epochs: 24 \tStep: 3610 \tTraining loss: 1.143310 \tValidation loss: 1.080667\n",
            "Epochs: 24 \tStep: 3620 \tTraining loss: 1.161032 \tValidation loss: 1.080577\n",
            "Epochs: 24 \tStep: 3630 \tTraining loss: 1.184187 \tValidation loss: 1.080071\n",
            "Epochs: 24 \tStep: 3640 \tTraining loss: 1.180963 \tValidation loss: 1.078831\n",
            "Epochs: 24 \tStep: 3650 \tTraining loss: 1.170104 \tValidation loss: 1.078137\n",
            "Epochs: 24 \tStep: 3660 \tTraining loss: 1.160271 \tValidation loss: 1.077120\n",
            "Epochs: 24 \tStep: 3670 \tTraining loss: 1.147186 \tValidation loss: 1.076182\n",
            "Epochs: 24 \tStep: 3680 \tTraining loss: 1.135874 \tValidation loss: 1.076230\n",
            "Epochs: 24 \tStep: 3690 \tTraining loss: 1.128379 \tValidation loss: 1.075422\n",
            "Epochs: 24 \tStep: 3700 \tTraining loss: 1.142580 \tValidation loss: 1.077416\n",
            "Epochs: 24 \tStep: 3710 \tTraining loss: 1.180434 \tValidation loss: 1.080788\n",
            "Epochs: 24 \tStep: 3720 \tTraining loss: 1.192211 \tValidation loss: 1.077195\n",
            "Epochs: 25 \tStep: 3730 \tTraining loss: 1.160013 \tValidation loss: 1.079519\n",
            "Epochs: 25 \tStep: 3740 \tTraining loss: 1.165374 \tValidation loss: 1.076126\n",
            "Epochs: 25 \tStep: 3750 \tTraining loss: 1.171414 \tValidation loss: 1.072636\n",
            "Epochs: 25 \tStep: 3760 \tTraining loss: 1.157113 \tValidation loss: 1.073633\n",
            "Epochs: 25 \tStep: 3770 \tTraining loss: 1.157751 \tValidation loss: 1.072718\n",
            "Epochs: 25 \tStep: 3780 \tTraining loss: 1.132922 \tValidation loss: 1.070746\n",
            "Epochs: 25 \tStep: 3790 \tTraining loss: 1.161066 \tValidation loss: 1.069261\n",
            "Epochs: 25 \tStep: 3800 \tTraining loss: 1.146768 \tValidation loss: 1.069533\n",
            "Epochs: 25 \tStep: 3810 \tTraining loss: 1.157322 \tValidation loss: 1.069030\n",
            "Epochs: 25 \tStep: 3820 \tTraining loss: 1.164810 \tValidation loss: 1.068420\n",
            "Epochs: 25 \tStep: 3830 \tTraining loss: 1.174845 \tValidation loss: 1.068210\n",
            "Epochs: 25 \tStep: 3840 \tTraining loss: 1.161530 \tValidation loss: 1.066217\n",
            "Epochs: 25 \tStep: 3850 \tTraining loss: 1.139405 \tValidation loss: 1.068235\n",
            "Epochs: 25 \tStep: 3860 \tTraining loss: 1.162711 \tValidation loss: 1.063869\n",
            "Epochs: 25 \tStep: 3870 \tTraining loss: 1.154034 \tValidation loss: 1.065059\n",
            "Epochs: 26 \tStep: 3880 \tTraining loss: 1.125519 \tValidation loss: 1.071119\n",
            "Epochs: 26 \tStep: 3890 \tTraining loss: 1.111054 \tValidation loss: 1.070075\n",
            "Epochs: 26 \tStep: 3900 \tTraining loss: 1.155621 \tValidation loss: 1.067801\n",
            "Epochs: 26 \tStep: 3910 \tTraining loss: 1.154855 \tValidation loss: 1.064238\n",
            "Epochs: 26 \tStep: 3920 \tTraining loss: 1.130420 \tValidation loss: 1.063019\n",
            "Epochs: 26 \tStep: 3930 \tTraining loss: 1.150851 \tValidation loss: 1.064499\n",
            "Epochs: 26 \tStep: 3940 \tTraining loss: 1.166907 \tValidation loss: 1.062310\n",
            "Epochs: 26 \tStep: 3950 \tTraining loss: 1.167030 \tValidation loss: 1.061177\n",
            "Epochs: 26 \tStep: 3960 \tTraining loss: 1.147814 \tValidation loss: 1.061728\n",
            "Epochs: 26 \tStep: 3970 \tTraining loss: 1.158805 \tValidation loss: 1.063177\n",
            "Epochs: 26 \tStep: 3980 \tTraining loss: 1.118997 \tValidation loss: 1.063622\n",
            "Epochs: 26 \tStep: 3990 \tTraining loss: 1.113762 \tValidation loss: 1.059533\n",
            "Epochs: 26 \tStep: 4000 \tTraining loss: 1.121732 \tValidation loss: 1.058828\n",
            "Epochs: 26 \tStep: 4010 \tTraining loss: 1.135735 \tValidation loss: 1.059194\n",
            "Epochs: 26 \tStep: 4020 \tTraining loss: 1.173563 \tValidation loss: 1.062822\n",
            "Epochs: 26 \tStep: 4030 \tTraining loss: 1.177755 \tValidation loss: 1.058434\n",
            "Epochs: 27 \tStep: 4040 \tTraining loss: 1.157889 \tValidation loss: 1.064366\n",
            "Epochs: 27 \tStep: 4050 \tTraining loss: 1.140294 \tValidation loss: 1.061134\n",
            "Epochs: 27 \tStep: 4060 \tTraining loss: 1.157262 \tValidation loss: 1.057476\n",
            "Epochs: 27 \tStep: 4070 \tTraining loss: 1.148322 \tValidation loss: 1.057168\n",
            "Epochs: 27 \tStep: 4080 \tTraining loss: 1.150909 \tValidation loss: 1.054677\n",
            "Epochs: 27 \tStep: 4090 \tTraining loss: 1.114947 \tValidation loss: 1.056425\n",
            "Epochs: 27 \tStep: 4100 \tTraining loss: 1.145999 \tValidation loss: 1.055563\n",
            "Epochs: 27 \tStep: 4110 \tTraining loss: 1.126621 \tValidation loss: 1.054339\n",
            "Epochs: 27 \tStep: 4120 \tTraining loss: 1.147176 \tValidation loss: 1.053035\n",
            "Epochs: 27 \tStep: 4130 \tTraining loss: 1.158284 \tValidation loss: 1.051406\n",
            "Epochs: 27 \tStep: 4140 \tTraining loss: 1.157555 \tValidation loss: 1.051830\n",
            "Epochs: 27 \tStep: 4150 \tTraining loss: 1.154735 \tValidation loss: 1.052698\n",
            "Epochs: 27 \tStep: 4160 \tTraining loss: 1.120417 \tValidation loss: 1.052890\n",
            "Epochs: 27 \tStep: 4170 \tTraining loss: 1.161447 \tValidation loss: 1.050457\n",
            "Epochs: 27 \tStep: 4180 \tTraining loss: 1.142790 \tValidation loss: 1.051702\n",
            "Epochs: 28 \tStep: 4190 \tTraining loss: 1.112240 \tValidation loss: 1.058556\n",
            "Epochs: 28 \tStep: 4200 \tTraining loss: 1.095268 \tValidation loss: 1.057980\n",
            "Epochs: 28 \tStep: 4210 \tTraining loss: 1.145767 \tValidation loss: 1.053013\n",
            "Epochs: 28 \tStep: 4220 \tTraining loss: 1.137944 \tValidation loss: 1.048341\n",
            "Epochs: 28 \tStep: 4230 \tTraining loss: 1.116380 \tValidation loss: 1.048647\n",
            "Epochs: 28 \tStep: 4240 \tTraining loss: 1.137191 \tValidation loss: 1.052648\n",
            "Epochs: 28 \tStep: 4250 \tTraining loss: 1.151490 \tValidation loss: 1.049044\n",
            "Epochs: 28 \tStep: 4260 \tTraining loss: 1.148893 \tValidation loss: 1.048722\n",
            "Epochs: 28 \tStep: 4270 \tTraining loss: 1.135055 \tValidation loss: 1.049781\n",
            "Epochs: 28 \tStep: 4280 \tTraining loss: 1.129195 \tValidation loss: 1.050810\n",
            "Epochs: 28 \tStep: 4290 \tTraining loss: 1.121915 \tValidation loss: 1.048055\n",
            "Epochs: 28 \tStep: 4300 \tTraining loss: 1.105038 \tValidation loss: 1.045366\n",
            "Epochs: 28 \tStep: 4310 \tTraining loss: 1.107427 \tValidation loss: 1.045756\n",
            "Epochs: 28 \tStep: 4320 \tTraining loss: 1.114984 \tValidation loss: 1.046265\n",
            "Epochs: 28 \tStep: 4330 \tTraining loss: 1.159750 \tValidation loss: 1.050433\n",
            "Epochs: 28 \tStep: 4340 \tTraining loss: 1.172927 \tValidation loss: 1.046988\n",
            "Epochs: 29 \tStep: 4350 \tTraining loss: 1.135050 \tValidation loss: 1.049579\n",
            "Epochs: 29 \tStep: 4360 \tTraining loss: 1.135704 \tValidation loss: 1.044572\n",
            "Epochs: 29 \tStep: 4370 \tTraining loss: 1.142263 \tValidation loss: 1.048205\n",
            "Epochs: 29 \tStep: 4380 \tTraining loss: 1.140869 \tValidation loss: 1.046090\n",
            "Epochs: 29 \tStep: 4390 \tTraining loss: 1.139681 \tValidation loss: 1.045743\n",
            "Epochs: 29 \tStep: 4400 \tTraining loss: 1.111399 \tValidation loss: 1.048127\n",
            "Epochs: 29 \tStep: 4410 \tTraining loss: 1.140618 \tValidation loss: 1.044244\n",
            "Epochs: 29 \tStep: 4420 \tTraining loss: 1.112890 \tValidation loss: 1.040193\n",
            "Epochs: 29 \tStep: 4430 \tTraining loss: 1.131755 \tValidation loss: 1.040953\n",
            "Epochs: 29 \tStep: 4440 \tTraining loss: 1.137631 \tValidation loss: 1.040461\n",
            "Epochs: 29 \tStep: 4450 \tTraining loss: 1.140604 \tValidation loss: 1.039107\n",
            "Epochs: 29 \tStep: 4460 \tTraining loss: 1.132383 \tValidation loss: 1.038851\n",
            "Epochs: 29 \tStep: 4470 \tTraining loss: 1.112448 \tValidation loss: 1.039561\n",
            "Epochs: 29 \tStep: 4480 \tTraining loss: 1.144896 \tValidation loss: 1.038426\n",
            "Epochs: 29 \tStep: 4490 \tTraining loss: 1.132088 \tValidation loss: 1.038167\n",
            "Epochs: 30 \tStep: 4500 \tTraining loss: 1.107769 \tValidation loss: 1.043405\n",
            "Epochs: 30 \tStep: 4510 \tTraining loss: 1.088043 \tValidation loss: 1.039658\n",
            "Epochs: 30 \tStep: 4520 \tTraining loss: 1.129040 \tValidation loss: 1.041327\n",
            "Epochs: 30 \tStep: 4530 \tTraining loss: 1.131060 \tValidation loss: 1.036930\n",
            "Epochs: 30 \tStep: 4540 \tTraining loss: 1.109843 \tValidation loss: 1.037088\n",
            "Epochs: 30 \tStep: 4550 \tTraining loss: 1.119897 \tValidation loss: 1.036856\n",
            "Epochs: 30 \tStep: 4560 \tTraining loss: 1.144984 \tValidation loss: 1.035158\n",
            "Epochs: 30 \tStep: 4570 \tTraining loss: 1.145091 \tValidation loss: 1.038447\n",
            "Epochs: 30 \tStep: 4580 \tTraining loss: 1.121727 \tValidation loss: 1.031895\n",
            "Epochs: 30 \tStep: 4590 \tTraining loss: 1.124476 \tValidation loss: 1.032325\n",
            "Epochs: 30 \tStep: 4600 \tTraining loss: 1.115835 \tValidation loss: 1.032897\n",
            "Epochs: 30 \tStep: 4610 \tTraining loss: 1.089090 \tValidation loss: 1.032924\n",
            "Epochs: 30 \tStep: 4620 \tTraining loss: 1.094527 \tValidation loss: 1.032576\n",
            "Epochs: 30 \tStep: 4630 \tTraining loss: 1.114773 \tValidation loss: 1.033332\n",
            "Epochs: 30 \tStep: 4640 \tTraining loss: 1.136905 \tValidation loss: 1.035197\n",
            "Epochs: 30 \tStep: 4650 \tTraining loss: 1.159223 \tValidation loss: 1.032161\n",
            "Epochs: 31 \tStep: 4660 \tTraining loss: 1.132991 \tValidation loss: 1.038047\n",
            "Epochs: 31 \tStep: 4670 \tTraining loss: 1.127814 \tValidation loss: 1.033943\n",
            "Epochs: 31 \tStep: 4680 \tTraining loss: 1.129742 \tValidation loss: 1.034878\n",
            "Epochs: 31 \tStep: 4690 \tTraining loss: 1.114140 \tValidation loss: 1.032325\n",
            "Epochs: 31 \tStep: 4700 \tTraining loss: 1.120566 \tValidation loss: 1.030001\n",
            "Epochs: 31 \tStep: 4710 \tTraining loss: 1.099096 \tValidation loss: 1.035666\n",
            "Epochs: 31 \tStep: 4720 \tTraining loss: 1.127154 \tValidation loss: 1.029202\n",
            "Epochs: 31 \tStep: 4730 \tTraining loss: 1.092136 \tValidation loss: 1.027156\n",
            "Epochs: 31 \tStep: 4740 \tTraining loss: 1.123122 \tValidation loss: 1.030316\n",
            "Epochs: 31 \tStep: 4750 \tTraining loss: 1.128957 \tValidation loss: 1.026456\n",
            "Epochs: 31 \tStep: 4760 \tTraining loss: 1.133999 \tValidation loss: 1.025024\n",
            "Epochs: 31 \tStep: 4770 \tTraining loss: 1.127030 \tValidation loss: 1.026386\n",
            "Epochs: 31 \tStep: 4780 \tTraining loss: 1.097497 \tValidation loss: 1.025455\n",
            "Epochs: 31 \tStep: 4790 \tTraining loss: 1.128833 \tValidation loss: 1.024361\n",
            "Epochs: 31 \tStep: 4800 \tTraining loss: 1.113487 \tValidation loss: 1.025021\n",
            "Epochs: 32 \tStep: 4810 \tTraining loss: 1.094043 \tValidation loss: 1.031089\n",
            "Epochs: 32 \tStep: 4820 \tTraining loss: 1.081013 \tValidation loss: 1.028253\n",
            "Epochs: 32 \tStep: 4830 \tTraining loss: 1.116610 \tValidation loss: 1.028398\n",
            "Epochs: 32 \tStep: 4840 \tTraining loss: 1.110027 \tValidation loss: 1.026443\n",
            "Epochs: 32 \tStep: 4850 \tTraining loss: 1.089514 \tValidation loss: 1.024820\n",
            "Epochs: 32 \tStep: 4860 \tTraining loss: 1.106706 \tValidation loss: 1.023343\n",
            "Epochs: 32 \tStep: 4870 \tTraining loss: 1.139511 \tValidation loss: 1.024240\n",
            "Epochs: 32 \tStep: 4880 \tTraining loss: 1.127249 \tValidation loss: 1.021583\n",
            "Epochs: 32 \tStep: 4890 \tTraining loss: 1.115430 \tValidation loss: 1.019469\n",
            "Epochs: 32 \tStep: 4900 \tTraining loss: 1.113783 \tValidation loss: 1.021610\n",
            "Epochs: 32 \tStep: 4910 \tTraining loss: 1.093470 \tValidation loss: 1.020929\n",
            "Epochs: 32 \tStep: 4920 \tTraining loss: 1.082200 \tValidation loss: 1.021595\n",
            "Epochs: 32 \tStep: 4930 \tTraining loss: 1.085796 \tValidation loss: 1.020482\n",
            "Epochs: 32 \tStep: 4940 \tTraining loss: 1.101694 \tValidation loss: 1.020821\n",
            "Epochs: 32 \tStep: 4950 \tTraining loss: 1.138498 \tValidation loss: 1.023744\n",
            "Epochs: 32 \tStep: 4960 \tTraining loss: 1.148456 \tValidation loss: 1.020587\n",
            "Epochs: 33 \tStep: 4970 \tTraining loss: 1.121136 \tValidation loss: 1.021855\n",
            "Epochs: 33 \tStep: 4980 \tTraining loss: 1.110381 \tValidation loss: 1.023505\n",
            "Epochs: 33 \tStep: 4990 \tTraining loss: 1.119396 \tValidation loss: 1.024281\n",
            "Epochs: 33 \tStep: 5000 \tTraining loss: 1.111543 \tValidation loss: 1.019444\n",
            "Epochs: 33 \tStep: 5010 \tTraining loss: 1.102659 \tValidation loss: 1.018494\n",
            "Epochs: 33 \tStep: 5020 \tTraining loss: 1.086628 \tValidation loss: 1.019408\n",
            "Epochs: 33 \tStep: 5030 \tTraining loss: 1.112705 \tValidation loss: 1.018755\n",
            "Epochs: 33 \tStep: 5040 \tTraining loss: 1.092655 \tValidation loss: 1.016862\n",
            "Epochs: 33 \tStep: 5050 \tTraining loss: 1.102917 \tValidation loss: 1.015427\n",
            "Epochs: 33 \tStep: 5060 \tTraining loss: 1.116186 \tValidation loss: 1.013855\n",
            "Epochs: 33 \tStep: 5070 \tTraining loss: 1.117942 \tValidation loss: 1.015130\n",
            "Epochs: 33 \tStep: 5080 \tTraining loss: 1.111244 \tValidation loss: 1.017297\n",
            "Epochs: 33 \tStep: 5090 \tTraining loss: 1.103666 \tValidation loss: 1.015110\n",
            "Epochs: 33 \tStep: 5100 \tTraining loss: 1.119061 \tValidation loss: 1.013941\n",
            "Epochs: 33 \tStep: 5110 \tTraining loss: 1.108132 \tValidation loss: 1.015433\n",
            "Epochs: 34 \tStep: 5120 \tTraining loss: 1.077491 \tValidation loss: 1.020410\n",
            "Epochs: 34 \tStep: 5130 \tTraining loss: 1.071795 \tValidation loss: 1.018224\n",
            "Epochs: 34 \tStep: 5140 \tTraining loss: 1.106619 \tValidation loss: 1.017765\n",
            "Epochs: 34 \tStep: 5150 \tTraining loss: 1.104385 \tValidation loss: 1.014199\n",
            "Epochs: 34 \tStep: 5160 \tTraining loss: 1.078667 \tValidation loss: 1.013902\n",
            "Epochs: 34 \tStep: 5170 \tTraining loss: 1.104497 \tValidation loss: 1.014865\n",
            "Epochs: 34 \tStep: 5180 \tTraining loss: 1.116749 \tValidation loss: 1.012414\n",
            "Epochs: 34 \tStep: 5190 \tTraining loss: 1.119975 \tValidation loss: 1.014064\n",
            "Epochs: 34 \tStep: 5200 \tTraining loss: 1.105826 \tValidation loss: 1.009120\n",
            "Epochs: 34 \tStep: 5210 \tTraining loss: 1.101601 \tValidation loss: 1.008363\n",
            "Epochs: 34 \tStep: 5220 \tTraining loss: 1.081245 \tValidation loss: 1.008959\n",
            "Epochs: 34 \tStep: 5230 \tTraining loss: 1.064858 \tValidation loss: 1.008689\n",
            "Epochs: 34 \tStep: 5240 \tTraining loss: 1.069278 \tValidation loss: 1.010145\n",
            "Epochs: 34 \tStep: 5250 \tTraining loss: 1.088814 \tValidation loss: 1.009064\n",
            "Epochs: 34 \tStep: 5260 \tTraining loss: 1.119952 \tValidation loss: 1.010948\n",
            "Epochs: 34 \tStep: 5270 \tTraining loss: 1.135873 \tValidation loss: 1.009297\n",
            "Epochs: 35 \tStep: 5280 \tTraining loss: 1.112546 \tValidation loss: 1.011395\n",
            "Epochs: 35 \tStep: 5290 \tTraining loss: 1.091660 \tValidation loss: 1.010823\n",
            "Epochs: 35 \tStep: 5300 \tTraining loss: 1.105339 \tValidation loss: 1.011720\n",
            "Epochs: 35 \tStep: 5310 \tTraining loss: 1.110731 \tValidation loss: 1.010773\n",
            "Epochs: 35 \tStep: 5320 \tTraining loss: 1.092359 \tValidation loss: 1.008461\n",
            "Epochs: 35 \tStep: 5330 \tTraining loss: 1.076155 \tValidation loss: 1.008916\n",
            "Epochs: 35 \tStep: 5340 \tTraining loss: 1.104379 \tValidation loss: 1.007083\n",
            "Epochs: 35 \tStep: 5350 \tTraining loss: 1.083298 \tValidation loss: 1.005006\n",
            "Epochs: 35 \tStep: 5360 \tTraining loss: 1.094043 \tValidation loss: 1.004574\n",
            "Epochs: 35 \tStep: 5370 \tTraining loss: 1.109929 \tValidation loss: 1.005589\n",
            "Epochs: 35 \tStep: 5380 \tTraining loss: 1.109181 \tValidation loss: 1.003995\n",
            "Epochs: 35 \tStep: 5390 \tTraining loss: 1.105490 \tValidation loss: 1.004778\n",
            "Epochs: 35 \tStep: 5400 \tTraining loss: 1.081454 \tValidation loss: 1.005046\n",
            "Epochs: 35 \tStep: 5410 \tTraining loss: 1.114569 \tValidation loss: 1.002573\n",
            "Epochs: 35 \tStep: 5420 \tTraining loss: 1.102580 \tValidation loss: 1.003438\n",
            "Epochs: 36 \tStep: 5430 \tTraining loss: 1.070825 \tValidation loss: 1.010652\n",
            "Epochs: 36 \tStep: 5440 \tTraining loss: 1.062221 \tValidation loss: 1.005582\n",
            "Epochs: 36 \tStep: 5450 \tTraining loss: 1.091976 \tValidation loss: 1.009468\n",
            "Epochs: 36 \tStep: 5460 \tTraining loss: 1.096666 \tValidation loss: 1.005040\n",
            "Epochs: 36 \tStep: 5470 \tTraining loss: 1.072875 \tValidation loss: 1.002948\n",
            "Epochs: 36 \tStep: 5480 \tTraining loss: 1.087683 \tValidation loss: 1.002748\n",
            "Epochs: 36 \tStep: 5490 \tTraining loss: 1.104444 \tValidation loss: 1.006530\n",
            "Epochs: 36 \tStep: 5500 \tTraining loss: 1.114252 \tValidation loss: 1.006374\n",
            "Epochs: 36 \tStep: 5510 \tTraining loss: 1.102448 \tValidation loss: 1.002030\n",
            "Epochs: 36 \tStep: 5520 \tTraining loss: 1.090144 \tValidation loss: 1.001402\n",
            "Epochs: 36 \tStep: 5530 \tTraining loss: 1.079052 \tValidation loss: 1.000208\n",
            "Epochs: 36 \tStep: 5540 \tTraining loss: 1.064992 \tValidation loss: 1.001390\n",
            "Epochs: 36 \tStep: 5550 \tTraining loss: 1.066093 \tValidation loss: 0.999722\n",
            "Epochs: 36 \tStep: 5560 \tTraining loss: 1.074928 \tValidation loss: 1.000044\n",
            "Epochs: 36 \tStep: 5570 \tTraining loss: 1.118156 \tValidation loss: 1.001728\n",
            "Epochs: 36 \tStep: 5580 \tTraining loss: 1.122373 \tValidation loss: 0.999174\n",
            "Epochs: 37 \tStep: 5590 \tTraining loss: 1.101481 \tValidation loss: 0.998783\n",
            "Epochs: 37 \tStep: 5600 \tTraining loss: 1.094984 \tValidation loss: 1.001514\n",
            "Epochs: 37 \tStep: 5610 \tTraining loss: 1.099744 \tValidation loss: 1.002047\n",
            "Epochs: 37 \tStep: 5620 \tTraining loss: 1.093613 \tValidation loss: 1.002159\n",
            "Epochs: 37 \tStep: 5630 \tTraining loss: 1.094085 \tValidation loss: 0.997422\n",
            "Epochs: 37 \tStep: 5640 \tTraining loss: 1.062713 \tValidation loss: 0.998576\n",
            "Epochs: 37 \tStep: 5650 \tTraining loss: 1.093185 \tValidation loss: 0.996647\n",
            "Epochs: 37 \tStep: 5660 \tTraining loss: 1.078608 \tValidation loss: 0.995295\n",
            "Epochs: 37 \tStep: 5670 \tTraining loss: 1.092308 \tValidation loss: 0.996773\n",
            "Epochs: 37 \tStep: 5680 \tTraining loss: 1.105326 \tValidation loss: 0.994315\n",
            "Epochs: 37 \tStep: 5690 \tTraining loss: 1.093158 \tValidation loss: 0.994943\n",
            "Epochs: 37 \tStep: 5700 \tTraining loss: 1.086429 \tValidation loss: 0.996190\n",
            "Epochs: 37 \tStep: 5710 \tTraining loss: 1.073057 \tValidation loss: 0.994238\n",
            "Epochs: 37 \tStep: 5720 \tTraining loss: 1.099701 \tValidation loss: 0.993576\n",
            "Epochs: 37 \tStep: 5730 \tTraining loss: 1.087327 \tValidation loss: 0.993835\n",
            "Epochs: 38 \tStep: 5740 \tTraining loss: 1.063895 \tValidation loss: 0.996626\n",
            "Epochs: 38 \tStep: 5750 \tTraining loss: 1.050102 \tValidation loss: 0.993870\n",
            "Epochs: 38 \tStep: 5760 \tTraining loss: 1.090794 \tValidation loss: 0.999024\n",
            "Epochs: 38 \tStep: 5770 \tTraining loss: 1.091318 \tValidation loss: 0.998262\n",
            "Epochs: 38 \tStep: 5780 \tTraining loss: 1.060098 \tValidation loss: 0.993384\n",
            "Epochs: 38 \tStep: 5790 \tTraining loss: 1.083763 \tValidation loss: 0.992300\n",
            "Epochs: 38 \tStep: 5800 \tTraining loss: 1.099957 \tValidation loss: 0.994098\n",
            "Epochs: 38 \tStep: 5810 \tTraining loss: 1.100588 \tValidation loss: 0.992768\n",
            "Epochs: 38 \tStep: 5820 \tTraining loss: 1.089309 \tValidation loss: 0.990454\n",
            "Epochs: 38 \tStep: 5830 \tTraining loss: 1.079086 \tValidation loss: 0.993397\n",
            "Epochs: 38 \tStep: 5840 \tTraining loss: 1.070666 \tValidation loss: 0.989736\n",
            "Epochs: 38 \tStep: 5850 \tTraining loss: 1.052177 \tValidation loss: 0.991532\n",
            "Epochs: 38 \tStep: 5860 \tTraining loss: 1.053923 \tValidation loss: 0.992246\n",
            "Epochs: 38 \tStep: 5870 \tTraining loss: 1.073691 \tValidation loss: 0.990677\n",
            "Epochs: 38 \tStep: 5880 \tTraining loss: 1.108840 \tValidation loss: 0.991704\n",
            "Epochs: 38 \tStep: 5890 \tTraining loss: 1.113797 \tValidation loss: 0.990344\n",
            "Epochs: 39 \tStep: 5900 \tTraining loss: 1.088630 \tValidation loss: 0.991841\n",
            "Epochs: 39 \tStep: 5910 \tTraining loss: 1.086976 \tValidation loss: 0.992816\n",
            "Epochs: 39 \tStep: 5920 \tTraining loss: 1.078710 \tValidation loss: 0.991856\n",
            "Epochs: 39 \tStep: 5930 \tTraining loss: 1.078635 \tValidation loss: 0.991749\n",
            "Epochs: 39 \tStep: 5940 \tTraining loss: 1.079685 \tValidation loss: 0.986707\n",
            "Epochs: 39 \tStep: 5950 \tTraining loss: 1.061284 \tValidation loss: 0.991615\n",
            "Epochs: 39 \tStep: 5960 \tTraining loss: 1.088451 \tValidation loss: 0.988227\n",
            "Epochs: 39 \tStep: 5970 \tTraining loss: 1.062791 \tValidation loss: 0.988120\n",
            "Epochs: 39 \tStep: 5980 \tTraining loss: 1.086066 \tValidation loss: 0.988216\n",
            "Epochs: 39 \tStep: 5990 \tTraining loss: 1.086692 \tValidation loss: 0.984432\n",
            "Epochs: 39 \tStep: 6000 \tTraining loss: 1.091954 \tValidation loss: 0.983914\n",
            "Epochs: 39 \tStep: 6010 \tTraining loss: 1.075767 \tValidation loss: 0.983929\n",
            "Epochs: 39 \tStep: 6020 \tTraining loss: 1.063614 \tValidation loss: 0.984673\n",
            "Epochs: 39 \tStep: 6030 \tTraining loss: 1.085711 \tValidation loss: 0.984783\n",
            "Epochs: 39 \tStep: 6040 \tTraining loss: 1.076276 \tValidation loss: 0.987423\n",
            "Epochs: 40 \tStep: 6050 \tTraining loss: 1.056744 \tValidation loss: 0.991661\n",
            "Epochs: 40 \tStep: 6060 \tTraining loss: 1.043462 \tValidation loss: 0.990309\n",
            "Epochs: 40 \tStep: 6070 \tTraining loss: 1.080995 \tValidation loss: 0.991138\n",
            "Epochs: 40 \tStep: 6080 \tTraining loss: 1.080730 \tValidation loss: 0.989896\n",
            "Epochs: 40 \tStep: 6090 \tTraining loss: 1.054358 \tValidation loss: 0.985388\n",
            "Epochs: 40 \tStep: 6100 \tTraining loss: 1.082713 \tValidation loss: 0.984239\n",
            "Epochs: 40 \tStep: 6110 \tTraining loss: 1.095500 \tValidation loss: 0.987325\n",
            "Epochs: 40 \tStep: 6120 \tTraining loss: 1.085128 \tValidation loss: 0.985511\n",
            "Epochs: 40 \tStep: 6130 \tTraining loss: 1.085530 \tValidation loss: 0.981196\n",
            "Epochs: 40 \tStep: 6140 \tTraining loss: 1.070939 \tValidation loss: 0.982051\n",
            "Epochs: 40 \tStep: 6150 \tTraining loss: 1.065789 \tValidation loss: 0.981789\n",
            "Epochs: 40 \tStep: 6160 \tTraining loss: 1.039315 \tValidation loss: 0.981281\n",
            "Epochs: 40 \tStep: 6170 \tTraining loss: 1.047527 \tValidation loss: 0.980914\n",
            "Epochs: 40 \tStep: 6180 \tTraining loss: 1.065296 \tValidation loss: 0.979000\n",
            "Epochs: 40 \tStep: 6190 \tTraining loss: 1.100323 \tValidation loss: 0.984281\n",
            "Epochs: 40 \tStep: 6200 \tTraining loss: 1.107279 \tValidation loss: 0.980427\n",
            "Epochs: 41 \tStep: 6210 \tTraining loss: 1.086459 \tValidation loss: 0.982313\n",
            "Epochs: 41 \tStep: 6220 \tTraining loss: 1.080900 \tValidation loss: 0.980009\n",
            "Epochs: 41 \tStep: 6230 \tTraining loss: 1.081639 \tValidation loss: 0.982433\n",
            "Epochs: 41 \tStep: 6240 \tTraining loss: 1.081205 \tValidation loss: 0.985513\n",
            "Epochs: 41 \tStep: 6250 \tTraining loss: 1.072420 \tValidation loss: 0.979411\n",
            "Epochs: 41 \tStep: 6260 \tTraining loss: 1.060481 \tValidation loss: 0.981543\n",
            "Epochs: 41 \tStep: 6270 \tTraining loss: 1.085028 \tValidation loss: 0.982206\n",
            "Epochs: 41 \tStep: 6280 \tTraining loss: 1.056306 \tValidation loss: 0.978243\n",
            "Epochs: 41 \tStep: 6290 \tTraining loss: 1.067168 \tValidation loss: 0.981133\n",
            "Epochs: 41 \tStep: 6300 \tTraining loss: 1.084015 \tValidation loss: 0.975548\n",
            "Epochs: 41 \tStep: 6310 \tTraining loss: 1.073907 \tValidation loss: 0.977872\n",
            "Epochs: 41 \tStep: 6320 \tTraining loss: 1.079048 \tValidation loss: 0.976112\n",
            "Epochs: 41 \tStep: 6330 \tTraining loss: 1.050905 \tValidation loss: 0.976341\n",
            "Epochs: 41 \tStep: 6340 \tTraining loss: 1.090407 \tValidation loss: 0.974151\n",
            "Epochs: 41 \tStep: 6350 \tTraining loss: 1.074528 \tValidation loss: 0.976773\n",
            "Epochs: 42 \tStep: 6360 \tTraining loss: 1.041969 \tValidation loss: 0.982240\n",
            "Epochs: 42 \tStep: 6370 \tTraining loss: 1.039965 \tValidation loss: 0.978695\n",
            "Epochs: 42 \tStep: 6380 \tTraining loss: 1.072187 \tValidation loss: 0.979241\n",
            "Epochs: 42 \tStep: 6390 \tTraining loss: 1.077585 \tValidation loss: 0.980945\n",
            "Epochs: 42 \tStep: 6400 \tTraining loss: 1.045188 \tValidation loss: 0.974960\n",
            "Epochs: 42 \tStep: 6410 \tTraining loss: 1.075171 \tValidation loss: 0.977474\n",
            "Epochs: 42 \tStep: 6420 \tTraining loss: 1.078748 \tValidation loss: 0.977522\n",
            "Epochs: 42 \tStep: 6430 \tTraining loss: 1.087452 \tValidation loss: 0.975563\n",
            "Epochs: 42 \tStep: 6440 \tTraining loss: 1.067067 \tValidation loss: 0.974279\n",
            "Epochs: 42 \tStep: 6450 \tTraining loss: 1.053803 \tValidation loss: 0.973875\n",
            "Epochs: 42 \tStep: 6460 \tTraining loss: 1.060504 \tValidation loss: 0.972131\n",
            "Epochs: 42 \tStep: 6470 \tTraining loss: 1.038470 \tValidation loss: 0.971083\n",
            "Epochs: 42 \tStep: 6480 \tTraining loss: 1.048099 \tValidation loss: 0.973320\n",
            "Epochs: 42 \tStep: 6490 \tTraining loss: 1.052694 \tValidation loss: 0.971430\n",
            "Epochs: 42 \tStep: 6500 \tTraining loss: 1.088784 \tValidation loss: 0.974224\n",
            "Epochs: 42 \tStep: 6510 \tTraining loss: 1.102884 \tValidation loss: 0.972576\n",
            "Epochs: 43 \tStep: 6520 \tTraining loss: 1.069659 \tValidation loss: 0.974677\n",
            "Epochs: 43 \tStep: 6530 \tTraining loss: 1.074468 \tValidation loss: 0.973137\n",
            "Epochs: 43 \tStep: 6540 \tTraining loss: 1.071118 \tValidation loss: 0.973001\n",
            "Epochs: 43 \tStep: 6550 \tTraining loss: 1.072981 \tValidation loss: 0.977151\n",
            "Epochs: 43 \tStep: 6560 \tTraining loss: 1.064737 \tValidation loss: 0.969496\n",
            "Epochs: 43 \tStep: 6570 \tTraining loss: 1.045533 \tValidation loss: 0.972342\n",
            "Epochs: 43 \tStep: 6580 \tTraining loss: 1.064897 \tValidation loss: 0.971399\n",
            "Epochs: 43 \tStep: 6590 \tTraining loss: 1.045172 \tValidation loss: 0.968313\n",
            "Epochs: 43 \tStep: 6600 \tTraining loss: 1.069822 \tValidation loss: 0.968519\n",
            "Epochs: 43 \tStep: 6610 \tTraining loss: 1.068595 \tValidation loss: 0.968236\n",
            "Epochs: 43 \tStep: 6620 \tTraining loss: 1.070018 \tValidation loss: 0.968583\n",
            "Epochs: 43 \tStep: 6630 \tTraining loss: 1.066566 \tValidation loss: 0.966990\n",
            "Epochs: 43 \tStep: 6640 \tTraining loss: 1.044657 \tValidation loss: 0.968181\n",
            "Epochs: 43 \tStep: 6650 \tTraining loss: 1.071744 \tValidation loss: 0.965868\n",
            "Epochs: 43 \tStep: 6660 \tTraining loss: 1.071475 \tValidation loss: 0.968646\n",
            "Epochs: 44 \tStep: 6670 \tTraining loss: 1.028458 \tValidation loss: 0.972378\n",
            "Epochs: 44 \tStep: 6680 \tTraining loss: 1.020642 \tValidation loss: 0.968241\n",
            "Epochs: 44 \tStep: 6690 \tTraining loss: 1.059788 \tValidation loss: 0.973103\n",
            "Epochs: 44 \tStep: 6700 \tTraining loss: 1.068406 \tValidation loss: 0.972134\n",
            "Epochs: 44 \tStep: 6710 \tTraining loss: 1.038580 \tValidation loss: 0.967823\n",
            "Epochs: 44 \tStep: 6720 \tTraining loss: 1.057842 \tValidation loss: 0.966239\n",
            "Epochs: 44 \tStep: 6730 \tTraining loss: 1.072359 \tValidation loss: 0.966619\n",
            "Epochs: 44 \tStep: 6740 \tTraining loss: 1.063904 \tValidation loss: 0.966252\n",
            "Epochs: 44 \tStep: 6750 \tTraining loss: 1.072270 \tValidation loss: 0.967004\n",
            "Epochs: 44 \tStep: 6760 \tTraining loss: 1.057202 \tValidation loss: 0.966267\n",
            "Epochs: 44 \tStep: 6770 \tTraining loss: 1.045413 \tValidation loss: 0.964105\n",
            "Epochs: 44 \tStep: 6780 \tTraining loss: 1.028704 \tValidation loss: 0.962987\n",
            "Epochs: 44 \tStep: 6790 \tTraining loss: 1.040185 \tValidation loss: 0.964597\n",
            "Epochs: 44 \tStep: 6800 \tTraining loss: 1.050012 \tValidation loss: 0.962631\n",
            "Epochs: 44 \tStep: 6810 \tTraining loss: 1.086683 \tValidation loss: 0.967124\n",
            "Epochs: 44 \tStep: 6820 \tTraining loss: 1.095733 \tValidation loss: 0.963675\n",
            "Epochs: 45 \tStep: 6830 \tTraining loss: 1.073032 \tValidation loss: 0.971013\n",
            "Epochs: 45 \tStep: 6840 \tTraining loss: 1.064460 \tValidation loss: 0.972761\n",
            "Epochs: 45 \tStep: 6850 \tTraining loss: 1.062823 \tValidation loss: 0.969034\n",
            "Epochs: 45 \tStep: 6860 \tTraining loss: 1.056137 \tValidation loss: 0.971745\n",
            "Epochs: 45 \tStep: 6870 \tTraining loss: 1.075170 \tValidation loss: 0.967368\n",
            "Epochs: 45 \tStep: 6880 \tTraining loss: 1.043397 \tValidation loss: 0.968064\n",
            "Epochs: 45 \tStep: 6890 \tTraining loss: 1.065860 \tValidation loss: 0.967226\n",
            "Epochs: 45 \tStep: 6900 \tTraining loss: 1.044547 \tValidation loss: 0.964457\n",
            "Epochs: 45 \tStep: 6910 \tTraining loss: 1.067820 \tValidation loss: 0.964688\n",
            "Epochs: 45 \tStep: 6920 \tTraining loss: 1.064603 \tValidation loss: 0.962917\n",
            "Epochs: 45 \tStep: 6930 \tTraining loss: 1.059277 \tValidation loss: 0.962246\n",
            "Epochs: 45 \tStep: 6940 \tTraining loss: 1.056244 \tValidation loss: 0.964422\n",
            "Epochs: 45 \tStep: 6950 \tTraining loss: 1.039550 \tValidation loss: 0.961901\n",
            "Epochs: 45 \tStep: 6960 \tTraining loss: 1.064345 \tValidation loss: 0.961353\n",
            "Epochs: 45 \tStep: 6970 \tTraining loss: 1.051715 \tValidation loss: 0.964211\n",
            "Epochs: 46 \tStep: 6980 \tTraining loss: 1.038352 \tValidation loss: 0.970071\n",
            "Epochs: 46 \tStep: 6990 \tTraining loss: 1.029388 \tValidation loss: 0.964484\n",
            "Epochs: 46 \tStep: 7000 \tTraining loss: 1.056921 \tValidation loss: 0.965751\n",
            "Epochs: 46 \tStep: 7010 \tTraining loss: 1.062074 \tValidation loss: 0.961402\n",
            "Epochs: 46 \tStep: 7020 \tTraining loss: 1.034976 \tValidation loss: 0.959856\n",
            "Epochs: 46 \tStep: 7030 \tTraining loss: 1.062444 \tValidation loss: 0.960360\n",
            "Epochs: 46 \tStep: 7040 \tTraining loss: 1.069675 \tValidation loss: 0.959421\n",
            "Epochs: 46 \tStep: 7050 \tTraining loss: 1.067363 \tValidation loss: 0.959484\n",
            "Epochs: 46 \tStep: 7060 \tTraining loss: 1.061401 \tValidation loss: 0.955349\n",
            "Epochs: 46 \tStep: 7070 \tTraining loss: 1.048271 \tValidation loss: 0.956947\n",
            "Epochs: 46 \tStep: 7080 \tTraining loss: 1.038582 \tValidation loss: 0.955107\n",
            "Epochs: 46 \tStep: 7090 \tTraining loss: 1.014363 \tValidation loss: 0.954712\n",
            "Epochs: 46 \tStep: 7100 \tTraining loss: 1.014646 \tValidation loss: 0.955344\n",
            "Epochs: 46 \tStep: 7110 \tTraining loss: 1.044722 \tValidation loss: 0.954246\n",
            "Epochs: 46 \tStep: 7120 \tTraining loss: 1.075448 \tValidation loss: 0.957440\n",
            "Epochs: 46 \tStep: 7130 \tTraining loss: 1.086730 \tValidation loss: 0.954564\n",
            "Epochs: 47 \tStep: 7140 \tTraining loss: 1.055649 \tValidation loss: 0.956261\n",
            "Epochs: 47 \tStep: 7150 \tTraining loss: 1.053981 \tValidation loss: 0.959968\n",
            "Epochs: 47 \tStep: 7160 \tTraining loss: 1.054983 \tValidation loss: 0.959227\n",
            "Epochs: 47 \tStep: 7170 \tTraining loss: 1.049641 \tValidation loss: 0.957367\n",
            "Epochs: 47 \tStep: 7180 \tTraining loss: 1.062972 \tValidation loss: 0.957375\n",
            "Epochs: 47 \tStep: 7190 \tTraining loss: 1.029968 \tValidation loss: 0.957969\n",
            "Epochs: 47 \tStep: 7200 \tTraining loss: 1.061243 \tValidation loss: 0.956069\n",
            "Epochs: 47 \tStep: 7210 \tTraining loss: 1.032123 \tValidation loss: 0.954889\n",
            "Epochs: 47 \tStep: 7220 \tTraining loss: 1.045036 \tValidation loss: 0.954978\n",
            "Epochs: 47 \tStep: 7230 \tTraining loss: 1.059903 \tValidation loss: 0.953306\n",
            "Epochs: 47 \tStep: 7240 \tTraining loss: 1.053681 \tValidation loss: 0.952326\n",
            "Epochs: 47 \tStep: 7250 \tTraining loss: 1.041913 \tValidation loss: 0.952406\n",
            "Epochs: 47 \tStep: 7260 \tTraining loss: 1.034473 \tValidation loss: 0.951005\n",
            "Epochs: 47 \tStep: 7270 \tTraining loss: 1.062782 \tValidation loss: 0.952842\n",
            "Epochs: 47 \tStep: 7280 \tTraining loss: 1.054517 \tValidation loss: 0.956262\n",
            "Epochs: 48 \tStep: 7290 \tTraining loss: 1.026850 \tValidation loss: 0.960748\n",
            "Epochs: 48 \tStep: 7300 \tTraining loss: 1.015907 \tValidation loss: 0.954549\n",
            "Epochs: 48 \tStep: 7310 \tTraining loss: 1.053109 \tValidation loss: 0.954193\n",
            "Epochs: 48 \tStep: 7320 \tTraining loss: 1.057134 \tValidation loss: 0.955423\n",
            "Epochs: 48 \tStep: 7330 \tTraining loss: 1.033196 \tValidation loss: 0.951855\n",
            "Epochs: 48 \tStep: 7340 \tTraining loss: 1.048604 \tValidation loss: 0.956637\n",
            "Epochs: 48 \tStep: 7350 \tTraining loss: 1.063748 \tValidation loss: 0.954263\n",
            "Epochs: 48 \tStep: 7360 \tTraining loss: 1.053327 \tValidation loss: 0.950789\n",
            "Epochs: 48 \tStep: 7370 \tTraining loss: 1.055725 \tValidation loss: 0.949951\n",
            "Epochs: 48 \tStep: 7380 \tTraining loss: 1.048323 \tValidation loss: 0.949996\n",
            "Epochs: 48 \tStep: 7390 \tTraining loss: 1.034109 \tValidation loss: 0.950015\n",
            "Epochs: 48 \tStep: 7400 \tTraining loss: 1.016043 \tValidation loss: 0.947287\n",
            "Epochs: 48 \tStep: 7410 \tTraining loss: 1.018821 \tValidation loss: 0.949307\n",
            "Epochs: 48 \tStep: 7420 \tTraining loss: 1.034764 \tValidation loss: 0.946348\n",
            "Epochs: 48 \tStep: 7430 \tTraining loss: 1.064610 \tValidation loss: 0.950419\n",
            "Epochs: 48 \tStep: 7440 \tTraining loss: 1.079909 \tValidation loss: 0.951457\n",
            "Epochs: 49 \tStep: 7450 \tTraining loss: 1.063359 \tValidation loss: 0.953877\n",
            "Epochs: 49 \tStep: 7460 \tTraining loss: 1.038892 \tValidation loss: 0.951023\n",
            "Epochs: 49 \tStep: 7470 \tTraining loss: 1.046009 \tValidation loss: 0.953256\n",
            "Epochs: 49 \tStep: 7480 \tTraining loss: 1.046184 \tValidation loss: 0.950815\n",
            "Epochs: 49 \tStep: 7490 \tTraining loss: 1.052885 \tValidation loss: 0.949108\n",
            "Epochs: 49 \tStep: 7500 \tTraining loss: 1.026776 \tValidation loss: 0.949878\n",
            "Epochs: 49 \tStep: 7510 \tTraining loss: 1.046202 \tValidation loss: 0.946951\n",
            "Epochs: 49 \tStep: 7520 \tTraining loss: 1.026807 \tValidation loss: 0.950859\n",
            "Epochs: 49 \tStep: 7530 \tTraining loss: 1.039139 \tValidation loss: 0.949548\n",
            "Epochs: 49 \tStep: 7540 \tTraining loss: 1.051095 \tValidation loss: 0.944597\n",
            "Epochs: 49 \tStep: 7550 \tTraining loss: 1.042124 \tValidation loss: 0.946198\n",
            "Epochs: 49 \tStep: 7560 \tTraining loss: 1.040305 \tValidation loss: 0.945669\n",
            "Epochs: 49 \tStep: 7570 \tTraining loss: 1.025767 \tValidation loss: 0.944170\n",
            "Epochs: 49 \tStep: 7580 \tTraining loss: 1.051181 \tValidation loss: 0.943973\n",
            "Epochs: 49 \tStep: 7590 \tTraining loss: 1.039755 \tValidation loss: 0.945528\n",
            "Epochs: 50 \tStep: 7600 \tTraining loss: 1.015262 \tValidation loss: 0.951055\n",
            "Epochs: 50 \tStep: 7610 \tTraining loss: 1.009069 \tValidation loss: 0.943434\n",
            "Epochs: 50 \tStep: 7620 \tTraining loss: 1.048951 \tValidation loss: 0.945959\n",
            "Epochs: 50 \tStep: 7630 \tTraining loss: 1.044706 \tValidation loss: 0.948295\n",
            "Epochs: 50 \tStep: 7640 \tTraining loss: 1.023418 \tValidation loss: 0.945826\n",
            "Epochs: 50 \tStep: 7650 \tTraining loss: 1.039450 \tValidation loss: 0.946109\n",
            "Epochs: 50 \tStep: 7660 \tTraining loss: 1.051227 \tValidation loss: 0.946514\n",
            "Epochs: 50 \tStep: 7670 \tTraining loss: 1.055107 \tValidation loss: 0.942780\n",
            "Epochs: 50 \tStep: 7680 \tTraining loss: 1.043781 \tValidation loss: 0.942934\n",
            "Epochs: 50 \tStep: 7690 \tTraining loss: 1.038878 \tValidation loss: 0.942196\n",
            "Epochs: 50 \tStep: 7700 \tTraining loss: 1.033358 \tValidation loss: 0.943223\n",
            "Epochs: 50 \tStep: 7710 \tTraining loss: 1.012668 \tValidation loss: 0.939898\n",
            "Epochs: 50 \tStep: 7720 \tTraining loss: 1.016721 \tValidation loss: 0.940691\n",
            "Epochs: 50 \tStep: 7730 \tTraining loss: 1.031471 \tValidation loss: 0.942903\n",
            "Epochs: 50 \tStep: 7740 \tTraining loss: 1.059158 \tValidation loss: 0.943890\n",
            "Epochs: 50 \tStep: 7750 \tTraining loss: 1.073137 \tValidation loss: 0.943471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, char, h=None, top_k=None):\n",
        "\n",
        "  x = np.array([[model.char_to_int[char]]])\n",
        "  x = one_hot_encode(x, len(model.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  inputs = inputs.to(device)\n",
        "\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  p = F.softmax(out, dim=1).data \n",
        "\n",
        "  if train_on_gpu:\n",
        "    p = p.cpu()\n",
        "\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(model.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p = p/p.sum())\n",
        "  return model.int_to_char[char], h"
      ],
      "metadata": {
        "id": "ZDPcIBLy50WH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime=\"The\", top_k=None):\n",
        "  model.to(device)\n",
        "  \n",
        "  model.eval()\n",
        "  \n",
        "  chars = [ch for ch in prime]\n",
        "  h = model.initialize_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "  return \"\".join(chars)"
      ],
      "metadata": {
        "id": "K0cSQ5KL50Y5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 2000, prime=\"Anna\", top_k=10))"
      ],
      "metadata": {
        "id": "oUBVQmOYO6is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa45307-ea3e-445b-ace0-88c55687150a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna (what she had gone, and he, finishing his\n",
            "difficulty. Only the way of those people had flown over, said himself\n",
            "in the same still more often.\n",
            "\n",
            "\"No, I meant,\" she said to him, a steppe--obviously changing to her.\n",
            "\"But I suppose I'm one man say it's approaching for such the shriek of\n",
            "article.... No, I'm going to the peasants), but I, supposing\n",
            "that it's all there to me,\" he said.\n",
            "\n",
            "He listened to his wound an offer to sound something in the rest of\n",
            "her cain over the position. \"Something ill was said.\n",
            "\n",
            "\"Here's the reading solitude. What would he has? This is!... I've\n",
            "liked to come home!\" Kitty shouted to him. This glance\n",
            "with a gained humble watch was a letter with a low wife. He walked\n",
            "away with him.\n",
            "\n",
            "\"Yes, yes,\" she went on, never being sank having from bringing his\n",
            "gun in the reflection and looking at the saboves of the\n",
            "carriage and white and smile that had smiled business, and saw\n",
            "down on the doorway when he caught sight of his spring article.\n",
            "\n",
            "\"Yes, I shall be the pure, and all don't make out any neaser.\"\n",
            "\n",
            "Levin could shook something.\n",
            "\n",
            "\"Of course I don't feel the consequence so important. Would you\n",
            "distinguish what is it?\"\n",
            "\n",
            "\"No; yes, I do anything,\" and thought that a continual person in\n",
            "the meaning of the dinner and pursuits on her carriage, some special side\n",
            "and spring lightly obvious and showly atting the sufferings and ten\n",
            "o'clock in the direction his carriage, when the beginning of the front\n",
            "of the bundance of the countess of the door, with his love some\n",
            "foot and the sarrantemanes. After corrie with his office in their\n",
            "public mals, when he had called all the passion of sense, which the\n",
            "doctor- the time, as an answer, and trying not to remember\n",
            "whether to the family sight of the first doctor had come up.\n",
            "\n",
            "With a bart look over the door that she was going by the strempth that\n",
            "began to go and countess from his fresh smile, this world had not been\n",
            "instisutively speaking of she did not console her, and when his\n",
            "eyes opening his face when he said, then for her head \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"char_rnn.model\"\n",
        "\n",
        "checkpoint = {\"n_hidden\": model.n_hidden,\n",
        "              \"n_layers\": model.n_layers,\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"tokens\": model.chars}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "1kHHTVxN50TF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"char_rnn.model\", \"rb\") as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "loaded = RNN(checkpoint['tokens'], n_hidden=checkpoint[\"n_hidden\"], n_layers=checkpoint[\"n_layers\"])\n",
        "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
        "print(sample(loaded, size = 2000, prime=\"And Levin said\", top_k=10))"
      ],
      "metadata": {
        "id": "z4zZJCN3PFHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13689ba0-07cd-4501-d4c9-48345c6c7b60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And Levin said to\n",
            "Vronsky, who had changed, trying to prove theretime than ever. The tempte\n",
            "of the peasants with poscible, horse-gasted, as wondering stations.\n",
            "\n",
            "\"I know this, I'll do, that's the sound to anybody.\"\n",
            "\n",
            "The meaning of the subject.\n",
            "\n",
            "\"But that's only this subject.\"\n",
            "\n",
            "\"Anna Arkadyevna,\" Kitty answered, with a lawyer's eyes sitting,\n",
            "held it to the door.\n",
            "\n",
            "\n",
            "\n",
            "Chapter 18\n",
            "\n",
            "\n",
            "And the conversation was not as a fascinating day to stem serious prechous\n",
            "court, of words were so much as a day before. He tried to think about\n",
            "the people who did not recognize his lips, and he remembered the\n",
            "most plight-defeneed matters that she could not love, and went forward to\n",
            "him. In the lest minutes as to the baby all in right, in all\n",
            "reposts, he felt now with a warm conduct of his own thoughts, the\n",
            "fearful children, that she could not sleep for the counting house, but\n",
            "a decidering, which stronger his movements, and atcompressed their\n",
            "dress, and of their fearful painting his brother adoining them,\n",
            "watched for her office which were talking of line to comprehend\n",
            "them all--though she might be in love as something and anxiety. But\n",
            "at that doubt to her whole horror was already beginning her hands, and,\n",
            "telling her that, humiliating for a gentleman, of the sisters' today,\n",
            "had long therefore another and falling brother. And in spite of\n",
            "the duspision of the plination of the province would have a weakness\n",
            "boots, of his live, without wearing especially absolutely blissful\n",
            "connection and simple, he had driven, or when he was as he think\n",
            "out again with them: \"And how is it you don't know,\" he turned to\n",
            "his wife.\n",
            "\n",
            "\"And not the position of your shooting, see ne erger married attention?\"\n",
            "\n",
            "And Lizaveta Petrovna began sonting in the munning cofted, and the plowly\n",
            "smile and the stands crystally, he was she with summer, had come, but\n",
            "wousting still more, and with his brows own horses, he came upon Levin.\n",
            "\n",
            "Alexey Alexandrovitch about her hisbily stured feeling in the figure of\n",
            "all his conversation or production--of his o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwbIx2D6lLRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}