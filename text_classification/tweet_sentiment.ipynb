{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNJxg2mJO3SvB539+C+lKWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/text_classification/tweet_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wdRbkMDSiRb"
      },
      "outputs": [],
      "source": [
        "# upload kaggle API key from your local machine\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a kaggle dir, copy the API key to it\n",
        "# and make sure the file in only readable by yourself (chmod 600)\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "gxQgJbjUTCcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use API command to download the dataset\n",
        "!kaggle datasets download -d kazanova/sentiment140"
      ],
      "metadata": {
        "id": "HdPSfBYTTChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the dataset\n",
        "!unzip -qq sentiment140.zip"
      ],
      "metadata": {
        "id": "VQd-xK2-TClP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.9.1\n",
        "!pip install torch==1.8.1\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "2bkVwHJfrplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Cr6GwcgCT4Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print(\"CUDA is not available\")\n",
        "else:\n",
        "  print(\"CUDA is available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "s7bA1gDH7rbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n",
        "                       encoding='latin-1', header=None)"
      ],
      "metadata": {
        "id": "W4MnwKUkqM1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.head(3)"
      ],
      "metadata": {
        "id": "w_fXvz1u1C5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df[0].value_counts()"
      ],
      "metadata": {
        "id": "QfwWqhHX0iKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the dataset"
      ],
      "metadata": {
        "id": "aPrY3TDm178g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a column of type category from the label column\n",
        "tweets_df[\"sentiment_cat\"] = tweets_df[0].astype('category')\n",
        "# encode category column as numerical info in another column (sentiment)\n",
        "tweets_df[\"sentiment\"] = tweets_df[\"sentiment_cat\"].cat.codes\n",
        "# save the modified csv back to dist\n",
        "tweets_df.to_csv(\"train-processed.csv\", header=None, index=None)   "
      ],
      "metadata": {
        "id": "BdhlgOkY0iNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.head(3)"
      ],
      "metadata": {
        "id": "Uh87HVymofAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.tail(3)"
      ],
      "metadata": {
        "id": "y1qGjIZLo5la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL = data.LabelField() \n",
        "TWEET = data.Field('spacy', tokenizer_language='en_core_web_sm', lower=True)\n",
        "\n",
        "# carete a list that maps the fields onto the list of rows that are in the tweets dataframe\n",
        "fields = [('score',None), ('id',None), ('date',None), ('query',None),\n",
        "          ('name',None), ('tweet', TWEET), ('category',None), ('label',LABEL)]"
      ],
      "metadata": {
        "id": "twRgSSFp0iQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = data.dataset.TabularDataset(\n",
        "        path=\"train-processed.csv\", \n",
        "        format=\"CSV\", \n",
        "        fields=fields,\n",
        "        skip_header=False)"
      ],
      "metadata": {
        "id": "TT0q_W3z0iTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into train, test, and validation sets\n",
        "(train_data, test_data, valid_data) = tweet_data.split(split_ratio=[0.8, 0.1, 0.1],\n",
        "                                            stratified=True, strata_field='label')\n",
        "\n",
        "print(\"Num of training data: \", len(train_data))\n",
        "print(\"Num of test data: \", len(test_data))\n",
        "print(\"Num of validation_data: \", len(valid_data))"
      ],
      "metadata": {
        "id": "y-o5cvVb7frr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# limit the vocabulary in the training data\n",
        "vocab_size = 20000\n",
        "TWEET.build_vocab(train_data, max_size = vocab_size)\n",
        "LABEL.build_vocab(train_data)\n",
        "# most common words in the vocabulary\n",
        "TWEET.vocab.freqs.most_common(10)"
      ],
      "metadata": {
        "id": "hKH1TSCV7fvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define data loader\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = 32,\n",
        "    device = device,\n",
        "    sort_key = lambda x: len(x.tweet),\n",
        "    sort_within_batch = False)"
      ],
      "metadata": {
        "id": "L1iEQXNd7fx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a model"
      ],
      "metadata": {
        "id": "n2zHcyB30nqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, num_embeddings, embedding_dim, hidden_dim, output_dim, num_layers):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, seq):\n",
        "    output, (hidden,_) = self.encoder(self.embedding(seq))\n",
        "    preds = self.fc(hidden.squeeze(0))\n",
        "    return preds"
      ],
      "metadata": {
        "id": "bkOOC5Zq0iV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_embeddings = vocab_size+2\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "num_layers = 1\n",
        "\n",
        "model = Net(num_embeddings, embedding_dim, hidden_dim, output_dim, num_layers)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "5RU9iOB2ynSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a loss function and an optimizer"
      ],
      "metadata": {
        "id": "uwAEx19F0XAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "lr = 0.01\n",
        "params = model.parameters()\n",
        "optimizer = optim.Adam(params, lr=lr)"
      ],
      "metadata": {
        "id": "L7Qj39-5jILD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "4IfomB8I0bCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
        "\n",
        "  train_loss, valid_loss = 0.0, 0.0\n",
        "\n",
        "  model.train()\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    optimizer.zero_grad()\n",
        "    predict = model(batch.tweet)\n",
        "    loss = criterion(predict, batch.label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.data.item() * batch.tweet.size(0)\n",
        "  train_loss /= len(train_iterator)\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(valid_iterator):\n",
        "      predict = model(batch.tweet)\n",
        "      loss = criterion(predict, batch.label)\n",
        "      valid_loss += loss.data.item() * batch.tweet.size(0)\n",
        "  valid_loss /= len(valid_iterator)\n",
        "\n",
        "  return train_loss, valid_loss"
      ],
      "metadata": {
        "id": "o0xaoxhX7wvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "  train_loss, valid_loss = train(num_epochs, model, optimizer, criterion, train_iterator, valid_iterator) \n",
        "\n",
        "  print('Epoch: {} | Training Loss: {:.2f} | Validation Loss: {:.2f}'.format(epoch, train_loss, valid_loss))     \n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)  "
      ],
      "metadata": {
        "id": "Mx3e5Svc7zQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "0cKzd8__2H_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_tweet(tweet):\n",
        "  \"\"\" classify_tweet emulate the processing pipeline that happens internally\n",
        "  and make the required prediction on the output of that pipeline \"\"\"\n",
        "  \n",
        "  categories = {0: \"Negative\", 1:\"Positive\"}\n",
        "  processed = TWEET.process([TWEET.preprocess(tweet)])\n",
        "  processed = processed.to(device)\n",
        "  model.eval()\n",
        "  return categories[model(processed).argmax().item()]"
      ],
      "metadata": {
        "id": "5qlY0IVu72F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet1 = \"Watching this movie is just waste of time\"\n",
        "classify_tweet(tweet1)"
      ],
      "metadata": {
        "id": "dg8eIwz04V3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet2 = \"This movie is one of my favorite movies\"\n",
        "classify_tweet(tweet2)"
      ],
      "metadata": {
        "id": "YqCP_W5VLhtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}