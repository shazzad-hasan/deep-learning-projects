{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam_vs_ham.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/text_classification/spam_vs_ham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g64cuofepDK"
      },
      "outputs": [],
      "source": [
        "# upload kaggle API key from your local machine\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a kaggle dir, copy the API key to it\n",
        "# and make sure the file in only readable by yourself (chmod 600)\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TaUiYMasgQbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use API command to download the dataset\n",
        "!kaggle datasets download -d uciml/sms-spam-collection-dataset"
      ],
      "metadata": {
        "id": "Jx-dUPRygZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncompress the dataset\n",
        "!unzip -qq sms-spam-collection-dataset.zip"
      ],
      "metadata": {
        "id": "-rAuXQS4hEcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.9.1\n",
        "!pip install torch==1.8.1"
      ],
      "metadata": {
        "id": "maa4X1xUkhHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "0xFnve8DgZXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print(\"CUDA is not available\")\n",
        "else:\n",
        "  print(\"CUDA is available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "_WfMujNVOEfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Visualize the data"
      ],
      "metadata": {
        "id": "eEg_KFzLAPOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data from text file\n",
        "sms_df = pd.read_csv(\"/content/spam.csv\", encoding=\"latin-1\")\n",
        "\n",
        "sms_df.head()"
      ],
      "metadata": {
        "id": "-zTCORd0gZar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data pre-processing"
      ],
      "metadata": {
        "id": "1U8umQ9NAi_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop 3 unnamed columns\n",
        "sms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
        "# rename the columns and v2\n",
        "sms_df = sms_df.rename(index = str, columns = {\"v1\": \"labels\", \"v2\": \"text\"})\n",
        "\n",
        "sms_df.head()"
      ],
      "metadata": {
        "id": "CIu5VD5vgZdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "train_data, test_data = train_test_split(sms_df, test_size = 0.2, random_state = 42)\n",
        "# reset index\n",
        "train_data.reset_index(drop=True)\n",
        "test_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "dk4cwJoPiOp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num of training data :\", len(train_data))\n",
        "print(\"Num of test data: \", len(test_data))"
      ],
      "metadata": {
        "id": "zJfcP7M9iFIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the training and test data to csv files\n",
        "train_data.to_csv(\"train.csv\", index=False)\n",
        "test_data.to_csv(\"test.csv\", index=False)"
      ],
      "metadata": {
        "id": "ROFtlvnujWni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "5DJqMEOhj3Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "WnFYwD6gkEqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify how texts and labels should be processed\n",
        "TEXT = data.Field(tokenize = word_tokenize)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "metadata": {
        "id": "9PM3ROa3wvCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the filds objects\n",
        "fields = [(\"labels\", LABEL), (\"text\", TEXT)]"
      ],
      "metadata": {
        "id": "0DjM0CIaxNxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify row data for the data fields\n",
        "train_data, test_data = data.dataset.TabularDataset.splits(path = '/content',\n",
        "                                 train = \"train.csv\",\n",
        "                                 test = \"test.csv\",\n",
        "                                 format = \"csv\",\n",
        "                                 skip_header = True,\n",
        "                                 fields = fields)"
      ],
      "metadata": {
        "id": "RLX4LfWXxfOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Num of training data: {len(train_data)}')\n",
        "print(f'Number of testing data: {len(test_data)}')"
      ],
      "metadata": {
        "id": "ooSxWeavZdVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0].__dict__.keys()"
      ],
      "metadata": {
        "id": "SR4NSCt4ZtY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0].text"
      ],
      "metadata": {
        "id": "Fde7RxmfZ33X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0].labels"
      ],
      "metadata": {
        "id": "cLmEvbbiaC93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out all attributes associated with training\n",
        "print(vars(train_data.examples[5]))"
      ],
      "metadata": {
        "id": "wKB99ADnaIcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# limit the vocabulary in the training data\n",
        "vocab_size = 10000\n",
        "\n",
        "# numericalize \n",
        "TEXT.build_vocab(train_data, max_size = vocab_size)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "metadata": {
        "id": "slaJjuUlaQXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "id": "d0wkHYeKa8I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# most common 20 words\n",
        "TEXT.vocab.freqs.most_common(20)"
      ],
      "metadata": {
        "id": "AOIWKB-kbG-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indices to tokens\n",
        "TEXT.vocab.itos[:10]"
      ],
      "metadata": {
        "id": "foMpA2aIbygi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric representation of individual string\n",
        "print(LABEL.vocab.stoi)"
      ],
      "metadata": {
        "id": "K1oCus6_ghxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define an iterator that batches examples of similar lengths together\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "        (train_data, test_data),\n",
        "        batch_size = batch_size,\n",
        "        device = device,\n",
        "        sort_key = lambda x: len(x.text),\n",
        "        sort_within_batch = False)"
      ],
      "metadata": {
        "id": "KydvssJcgrDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Model"
      ],
      "metadata": {
        "id": "Jfb7KU5eAv8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text)\n",
        "    output, hidden = self.rnn(embedded)\n",
        "    hidden_1D = hidden.squeeze(0)\n",
        "    preds = self.fc(hidden_1D)\n",
        "    return preds\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text)\n",
        "    output, (hidden,_) = self.lstm(embedded)\n",
        "    hidden_1D = hidden.squeeze(0)\n",
        "    preds = self.fc(hidden_1D)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "Y0tZlwQVhQVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_embeddings  = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "num_layers = 1\n",
        "\n",
        "RNN_model = RNN(num_embeddings , embedding_dim, hidden_dim, output_dim)\n",
        "RNN_model.to(device)\n",
        "\n",
        "LSTM_model = LSTM(num_embeddings, embedding_dim, hidden_dim, output_dim, num_layers)\n",
        "LSTM_model.to(device)"
      ],
      "metadata": {
        "id": "wskZsctGw8t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "KC9yq1vdBA-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):  \n",
        "\n",
        "  train_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    optimizer.zero_grad()\n",
        "    predict = model(batch.text)\n",
        "    targets = (batch.labels).unsqueeze(1)\n",
        "    loss = criterion(predict, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  train_loss /= len(train_iterator)\n",
        "\n",
        "  return train_loss"
      ],
      "metadata": {
        "id": "sYVpLT1sBEaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(RNN_model.parameters(), lr = 1e-6)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = train(RNN_model, train_iterator, optimizer, criterion)\n",
        "\n",
        "  print('Epoch: {} | Training Loss: {:.2f}'.format(epoch, train_loss)) \n",
        "\n",
        "  train_losses.append(train_loss)"
      ],
      "metadata": {
        "id": "XEzYVjbZx-L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(RNN_model.parameters(), lr = 1e-6)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = train(LSTM_model, train_iterator, optimizer, criterion)\n",
        "\n",
        "  print('Epoch: {} | Training Loss: {:.2f}'.format(epoch, train_loss)) \n",
        "\n",
        "  train_losses.append(train_loss)"
      ],
      "metadata": {
        "id": "39S7zFX8yQ5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "PxqsO6LABHMj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUiqVvMwyQ82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cV5BfOjdyQ_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}