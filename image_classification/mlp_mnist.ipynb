{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_mnist.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPcP9D077/vBV1I5y3elwRG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/practice-deep-learning-with-pytorch/blob/main/image_classification/mlp_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will train an MLP to classify handwritten digits using [MNIST](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology) dataset. The MNIST dataset is a set of 60,000 training images and 10,000 test images, assembled by the National Institute of Standards and Technology in the 1980s. This is considered as the \"Hello World\" of deep learning.\n",
        "\n",
        "The process will be broken down into the following steps:\n",
        "\n",
        "1. Load and visualize the dataset\n",
        "2. Define a neural network\n",
        "3. Define a Loss function and optimizer\n",
        "4. Train the model on the training dataset\n",
        "5. Evaluate the performance of the trained model on the test dataset\n"
      ],
      "metadata": {
        "id": "y1AGs2iJDqBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "i0rnHiAf5mjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "  print(\"CUDA is available\")\n",
        "else:\n",
        "  print(\"CUDA is not available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "0yauFfqSn35o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the general configeration\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    torch.set_deterministic(True)\n",
        "\n",
        "random_seed = 125\n",
        "set_all_seeds(random_seed)\n",
        "set_deterministic()"
      ],
      "metadata": {
        "id": "ldUAQNVWCtiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and visualize the dataset"
      ],
      "metadata": {
        "id": "ZlmpNR1w5iap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# obtain training indices for creating a validation dataset\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "# choose percentage of training data for validation\n",
        "valid_size = 0.2\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "## define dataloader parameters\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# prepare train, test and validation data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "# image classes in the dataset\n",
        "classes = ['0','1','2','3','4','5', '6','7','8','9']\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "uCyUdGzv6M7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out some data stats\n",
        "\n",
        "print(\"Number of training images: \", len(train_data))\n",
        "print(\"Number of test images: \", len(test_data))\n",
        "\n",
        "for inputs, targets in train_loader:  \n",
        "    print('Image batch dimensions:', inputs.shape)\n",
        "    print('Image label dimensions:', targets.shape)\n",
        "    print('Class labels of 10 examples:', targets[:10])\n",
        "    break"
      ],
      "metadata": {
        "id": "f7mxzelj_54I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualize a batch of training data\n",
        "\n",
        "def imshow(img):\n",
        "  plt.imshow(np.squeeze(img), cmap='gray')\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "inputs, targets = dataiter.next()\n",
        "inputs = inputs.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(inputs[idx])\n",
        "    ax.set_title(str(targets[idx].item()))"
      ],
      "metadata": {
        "id": "IwsvDRoh-IDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a neural network"
      ],
      "metadata": {
        "id": "RHY2Re4I5nAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(28*28, 512) \n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512,512)\n",
        "    self.fc4 = nn.Linear(512, 10)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28*28)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "  \n",
        "model = Net()\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "0WEiVcgO5pqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# display model\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "id": "7ckpXapgZdvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Loss function and optimizer"
      ],
      "metadata": {
        "id": "_5wintQZPejx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss\n",
        "criterion = nn.CrossEntropyLoss() # categorical cross-entropy loss\n",
        "\n",
        "# specify optimizer\n",
        "params = model.parameters()\n",
        "optimizer = optim.Adam(params, lr=0.001) # SGD with momentum\n",
        "# learning rate scheduler\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.01)"
      ],
      "metadata": {
        "id": "dzkgQg5KAMOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model on the training dataset"
      ],
      "metadata": {
        "id": "aLDQzqNL5qE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "num_epochs = 20\n",
        "\n",
        "# track training and validation losses\n",
        "train_loss, valid_loss = [], []\n",
        "\n",
        "# initialize trackers for min validation loss\n",
        "min_valid_loss = np.inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  running_train_loss = 0.0\n",
        "  running_valid_loss = 0.0\n",
        "\n",
        "  #--------- train the model -----------\n",
        "  # set model to training mode\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, data in enumerate(train_loader):\n",
        "    # get the inputs; data is a list of [inputs, targets]\n",
        "    inputs, targets = data\n",
        "    # move tensor to the right device\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass\n",
        "    outputs = model(inputs)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(outputs, targets)\n",
        "    # backward loss\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # update training loss\n",
        "    running_train_loss += loss.item()\n",
        "  \n",
        "  # -------- validate the model ---------\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(valid_loader):\n",
        "      # move tensor to the right device\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      # forward pass\n",
        "      outputs = model(inputs)\n",
        "      # calculate the batch loss\n",
        "      loss = criterion(outputs, targets)\n",
        "      # update validation loss\n",
        "      running_valid_loss += loss.item()\n",
        "\n",
        "  # update the learning rate\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # calculate average loss over an epoch\n",
        "  running_train_loss = running_train_loss / len(train_loader)\n",
        "  running_valid_loss = running_valid_loss / len(valid_loader)\n",
        "\n",
        "  train_loss.append(running_train_loss)\n",
        "  valid_loss.append(running_valid_loss)\n",
        "\n",
        "  print(\"Epoch: {} \\tTraining loss: {:.6f} \\tValidation loss: {:.6f}\".format(epoch+1, running_train_loss, running_valid_loss))\n",
        "\n",
        "  # save model if validation loss has decreased\n",
        "  if running_valid_loss <= min_valid_loss:\n",
        "    print(\"Validation loss decreased ({:.6f} --> {:.6f}). Saving model ... \".format(\n",
        "        min_valid_loss, running_valid_loss\n",
        "    ))\n",
        "\n",
        "    torch.save(model.state_dict(), 'mlp_mnist_model.pt')\n",
        "    min_valid_loss = running_valid_loss\n",
        "\n",
        "print(\"Finished training!\")"
      ],
      "metadata": {
        "id": "JWsY1aZX5sPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and validation loss for each epoch\n",
        "epochs = range(1, num_epochs+1)\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, valid_loss, 'b', label='Validation loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p402mTEVm1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the performance of the trained model on the test dataset"
      ],
      "metadata": {
        "id": "DPtXaPWx5sjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model with the lowest validation loss\n",
        "model.load_state_dict(torch.load('mlp_mnist_model.pt'))"
      ],
      "metadata": {
        "id": "x2FWz81RCmza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# track test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = [0 for i in range(10)]\n",
        "class_total = [0 for i in range(10)]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    # forward pass\n",
        "    outputs = model(inputs)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(outputs, targets)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    # compare predictions to true labels\n",
        "    correct_tensor = predictions.eq(targets.data.view_as(predictions))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each class\n",
        "    for i in range(len(targets)):\n",
        "      label = targets.data[i]\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "print(\"Test loss (overall): {:6f}\\n\".format(test_loss))\n",
        "\n",
        "# print test accuracy for each classes\n",
        "for i in range(len(classes)):\n",
        "  if class_total[i] > 0:\n",
        "    accuracy = (100 * class_correct[i]) / class_total[i]\n",
        "    print(f'Test accuracy of {classes[i]:10s}: {accuracy:.1f} % ({np.sum(class_correct[i])}/{np.sum(class_total[i])})')\n",
        "\n",
        "# overall test accuracy\n",
        "test_acc = 100 * np.sum(class_correct) / np.sum(class_total)\n",
        "print(\"\\nTest accuracy (overall): %2d%% (%2d/%2d)\" % ( \n",
        "      test_acc, np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "vZtB6axZaK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "inputs, targets = dataiter.next()\n",
        "inputs.numpy()\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# get sample outputs\n",
        "outputs = model(inputs)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(outputs, 1)\n",
        "predictions = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(inputs[idx] if not train_on_gpu else inputs[idx].cpu())\n",
        "    ax.set_title(\"{} ({})\".format(classes[predictions[idx]], classes[targets[idx]]),\n",
        "                 color=(\"green\" if predictions[idx]==targets[idx].item() else \"red\"))"
      ],
      "metadata": {
        "id": "PvQnny39PzR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qf6uUPnLSYV0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}